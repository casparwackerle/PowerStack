% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

\chapter{Architecture and Design} % Main chapter title
\label{Chapter2}

\section{Overview of Test Environment}

The test environment consists of a Kubernetes cluster deployed on three bare-metal servers housed in a university datacenter. The three servers are identical in hardware specifications and connected through both a private network and the university network. The setup allows complete remote management and ensures direct communication between the servers for Kubernetes workloads. Below is a detailed description of the hardware and network topology. A diagram illustrating the architecture and network setup is provided in figure~\ref{fig:physical_and_network_infra}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/physical_and_network_infra.png}
    \decoRule
    \caption[Physical Infrastructure Diagram]{Physical Infrastructure Diagram}
    \label{fig:physical_and_network_infra}
    \end{figure}

\subsection{Hardware and Network}

\subsubsection{Bare-Metal Servers}

The cluster is built using three identical Lenovo ThinkSystem SR530 servers, each equipped with the following hardware:

\begin{itemize}
\item CPU: 1x Intel(R) Xeon(R) Bronze 3104 @ 1.70GHz, 6 cores.
\item Memory: 4x 16GB DDR4 DIMMs, totaling 64GB of RAM per server.
\item Storage:
\begin{itemize}
\item 2x 32GB M.2 SATA SSD for the operating system boot drive.
\item 1x 240GB 6Gbps SATA 2.5" SSD for persistent storage.
\item 3x 10TB 7.2K RPM 12Gbps SAS 3.5" HDDs for bulk storage.
\end{itemize}
\item Power Supply: Dual redundant power supplies.
\item Cooling: 4 out of 6 possible fans installed.
\item Firmware:
\begin{itemize}
\item BMC Version: 8.88 (Build ID: CDI3A4A)
\item UEFI Version: 3.42 (Build ID: TEE180J)
\item LXPM Version: 2.08 (Build ID: PDL142H)
\end{itemize}
\end{itemize}

The servers are equipped with Lenovo XClarity Controller (BMC) for remote management. Each server can be accessed via its BMC IP address for out-of-band management and monitoring.

\subsubsection{Network Topology}

The servers are connected using two distinct networks:

\begin{itemize}
\item Private Network: Each server has a private IP address (192.168.0.104–192.168.0.106), allowing direct, high-speed communication between nodes. This reduces the load on the university network and improves Kubernetes workload performance.
\item University Network: Public-facing IP addresses (160.85.30.104–160.85.30.106) allow access within the university network, with external access enabled via VPN.
\end{itemize}

\textbf{Note}: Detailed switch and gateway configurations are managed by the university IT department and are beyond the scope of this document.
%\subsection{Ubuntu Linux for bare-metal Kubernetes}
%\subsubsection{OS version and Kernel features}
%\subsubsection{Kernel tuning for performance monitoring}

\section{Key Technologies}

\subsection{Ubuntu}
Ubuntu was chosen as the operating system for this project primarily due to the author's familiarity with it. Additionally, it was already installed on the servers when they were received, which saved time and reduced setup complexity. While there are other Linux distributions specifically designed for Kubernetes, using a familiar distribution ensured smoother initial configuration and operation.

\subsection{Bare-Metal K3s}
Installing Kubernetes directly on bare-metal servers (without using a hypervisor or virtual machines) was a fundamental decision to ensure direct access to hardware-level data. This approach allows Kubernetes to interact with the underlying hardware more effectively, which is critical for accurate energy consumption monitoring.

K3s was chosen for several reasons:
\begin{itemize}
\item It is lightweight, making it suitable even for weaker servers, while potentially also lowering energy consumption.
\item Despite its lightweight nature, it remains fully compatible with stock Kubernetes, ensuring that standard Kubernetes resources and configurations can be used without modification.
\item K3s is optimized for ARM architectures, making it ideal for deployment on devices like Raspberry Pis in a homelab environment.
\item The author had prior experience with K3s and Rancher, which contributed to a faster and smoother deployment.
\end{itemize}

\subsection{Ansible, Helm, Kubectl}
For automation, Ansible and Helm were selected. Helm and Kubectl were an obvious choice due to their widespread use in Kubernetes for managing and deploying applications.

Ansible was chosen for its flexibility and ease of use in managing server configurations and automating repetitive tasks across multiple nodes. Additionally, Ansible's agentless architecture simplifies the management of bare-metal servers by requiring only SSH access and Python installed on the target machines.

\subsection{Kube-Prometheus Stack}
The Kube-Prometheus stack was chosen because it is the de-facto standard for monitoring in Kubernetes environments. This project has reached a high level of maturity, offering robust features and a wide range of integrations. Installation and configuration using Helm are straightforward, and the abundance of available resources makes troubleshooting easier.

\subsubsection{Prometheus}
Prometheus was selected as the primary monitoring tool because it is the standard in the Kubernetes ecosystem. Despite its advantages, Prometheus has some downsides: it can introduce significant overhead, and it is not suitable for monitoring low-second or sub-second intervals due to typical scrape intervals being longer. However, for container orchestration, where longer container lifetimes are expected, this limitation is acceptable.

\subsubsection{Grafana}
Grafana was chosen for its ability to provide excellent, customizable visualizations of metrics collected by Prometheus. It enables easy interpretation of complex data through dashboards and visual aids, making it a valuable addition to the monitoring stack.

\subsubsection{AlertManager}
AlertManager is included in the Kube-Prometheus stack and is used to handle alerts generated by Prometheus. While it was not utilized in this project, its inclusion is welcomed for potential future use in managing alerts and notifications in a production environment.

\subsection{KEPLER}

\subsubsection{Purpose of KEPLER}
KEPLER, or Kubernetes-based Efficient Power Level Exporter, is a promising project focused on measuring energy consumption in Kubernetes environments. It provides detailed power consumption metrics at the process, container, and pod levels, addressing the growing need for energy-efficient cloud computing.

With cloud providers and enterprises under increasing pressure to improve energy efficiency and meet regulatory requirements, KEPLER offers a practical solution. By enabling detailed real-time measurement of power usage, it bridges the gap between high-level infrastructure metrics and workload-specific energy consumption data. This capability makes KEPLER a valuable tool in advancing energy-efficient Kubernetes clusters.
\subsubsection{Overview of KEPLER Metrics}

KEPLER collects and exports a wide range of metrics related to energy consumption and resource utilization. The key metrics include:
\begin{itemize}
\item \textbf{kepler\textunderscore container\textunderscore joules\textunderscore total:} Total energy consumption of a container, aggregated from CPU, DRAM, and other components.
\item \textbf{kepler\textunderscore container\textunderscore core\textunderscore joules\textunderscore total:} Energy consumed by CPU cores used by a container.
\item \textbf{kepler\textunderscore container\textunderscore dram\textunderscore joules\textunderscore total:} Energy consumed by the DRAM utilized by a container.
\item \textbf{kepler\textunderscore container\textunderscore uncore\textunderscore joules\textunderscore total:} Energy consumed by uncore components such as last-level cache and memory controllers.
\item \textbf{kepler\textunderscore container\textunderscore gpu\textunderscore joules\textunderscore total:} Energy consumed by GPUs assigned to a container (currently supports NVIDIA GPUs).
\item \textbf{kepler\textunderscore node\textunderscore package\textunderscore joules\textunderscore total:} Total energy consumed by the CPU package on a node.
\item \textbf{kepler\textunderscore node\textunderscore platform\textunderscore joules\textunderscore total:} Total energy consumption of the entire node, including idle and dynamic power.
\end{itemize}
These metrics are collected using various APIs and kernel features, including Intel Running Average Power Limit (RAPL), Advanced Configuration and Power Interface (ACPI), and eBPF for tracing resource utilization. KEPLER aggregates these metrics at both the container and node levels, making them available for visualization and analysis through Prometheus and Grafana.

\subsubsection{Limitations of KEPLER}

Despite its potential, KEPLER has some limitations:
\begin{itemize}
\item Active Development: KEPLER is still in active development, meaning its features and APIs may change over time. Additionally, the documentation is currently limited, and there are few community resources available for troubleshooting.
\item Complexity: As a large and complex project, adapting KEPLER beyond basic configuration requires a deep understanding of its architecture. Implementing custom changes or enhancements can be challenging without significant expertise.
\end{itemize}

While KEPLER may not be perfect, it is currently the most promising approach to addressing the challenge of measuring energy consumption in Kubernetes environments. Consequently, a large focus of this thesis will be on evaluating KEPLER's capabilities and identifying areas for improvement.


\section{Architecture and design}
\subsection{Kubernetes Cluster Design}
\subsubsection{Cluster nodes and Roles}
\subsubsection{Networking between nodes}
\subsection{Monitoring Architecture}
\subsubsection{Prometheus data flow}
\subsubsection{Grafana Dashboards}

\section{KEPLER Architecture and Metrics collection}
\subsection{KEPLER components}
\subsubsection{KEPLER DaemonSet deployment}
\subsubsection{KEPLER Exporter and its interactions with the node}
\subsubsection{Other KEPLER features}
\subsection{Data Collection Methods}
\subsubsection{eBPF Probes}
\subsubsection{Performance Counters}
\subsubsection{Power Metrics from Intel RAPL}
\subsection{Metrics produced by KEPLER}
\subsubsection{Container-level metrics}
\subsubsection{Node-level metrics}
\subsubsection{Process-level metrics}


\section{Repository Strucure}
\subsection{Git main repository and submodules}
\subsection{Tools}