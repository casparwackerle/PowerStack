@misc{PowerStack,
  author       = {Caspar Wackerle},
  title        = {PowerStack: Automated Kubernetes Deployment for Energy Efficiency Analysis},
  year         = {2025},
  url          = {https://github.com/casparwackerle/PowerStack},
  note         = {GitHub repository}
}

@misc{iea2025energyai,
  author       = {International Energy Agency},
  title        = {Energy and AI},
  year         = {2025},
  publisher    = {IEA},
  address      = {Paris},
  url          = {https://www.iea.org/reports/energy-and-ai},
  note         = {Licence: CC BY 4.0}
}

@online{tomshardware2023mooreslaw,
  author       = {Ryan Smith},
  title        = {Intel’s CEO Says Moore’s Law Is Slowing to a Three-Year Cadence — But It’s Not Dead Yet},
  year         = {2023},
  url          = {https://www.tomshardware.com/tech-industry/semiconductors/intels-ceo-says-moores-law-is-slowing-to-a-three-year-cadence-but-its-not-dead-yet},
  note         = {Accessed: 2025-04-14},
  publisher    = {Tom's Hardware}
}

@online{cartesian2013dennard,
  author       = {Martin Keegan},
  title        = {The End of Dennard Scaling},
  year         = {2013},
  url          = {https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/},
  note         = {Accessed: 2025-04-14},
  publisher    = {The Cartesian Product}
}

@online{uptime2023pue,
  author       = {Uptime Institute},
  title        = {Global PUEs – Are They Going Anywhere?},
  year         = {2023},
  url          = {https://journal.uptimeinstitute.com/global-pues-are-they-going-anywhere/},
  note         = {Accessed: 2025-04-14},
  publisher    = {Uptime Institute Journal}
}

@article{masanet2020,
  author = {Eric Masanet  and Arman Shehabi  and Nuoa Lei  and Sarah Smith  and Jonathan Koomey},
  title = {Recalibrating global data center energy-use estimates},
  journal = {Science},
  volume = {367},
  number = {6481},
  pages = {984-986},
  year = {2020},
  doi = {10.1126/science.aba3758},
  URL = {https://www.science.org/doi/abs/10.1126/science.aba3758},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.aba3758},
  abstract = {Growth in energy use has slowed owing to efficiency gains that smart policies can help maintain in the near term Data centers represent the information backbone of an increasingly digitalized world. Demand for their services has been rising rapidly (1), and data-intensive technologies such as artificial intelligence, smart and connected energy systems, distributed manufacturing systems, and autonomous vehicles promise to increase demand further (2). Given that data centers are energy-intensive enterprises, estimated to account for around 1\% of worldwide electricity use, these trends have clear implications for global energy demand and must be analyzed rigorously. Several oft-cited yet simplistic analyses claim that the energy used by the world's data centers has doubled over the past decade and that their energy use will triple or even quadruple within the next decade (3–5). Such estimates contribute to a conventional wisdom (5, 6) that as demand for data center services rises rapidly, so too must their global energy use. But such extrapolations based on recent service demand growth indicators overlook strong countervailing energy efficiency trends that have occurred in parallel (see the first figure). Here, we integrate new data from different sources that have emerged recently and suggest more modest growth in global data center energy use (see the second figure). This provides policy-makers and energy analysts a recalibrated understanding of global data center energy use, its drivers, and near-term efficiency potential.}
}
@article{Potdar2020,
  title = {Performance {{Evaluation}} of {{Docker Container}} and {{Virtual Machine}}},
  author = {Potdar, Amit M. and G, Narayan D. and Kengond, Shivaraj and Mulla, Mohammed Moin},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {171},
  pages = {1419--1428},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.04.152},
  abstract = {Server virtualization is a technological innovation broadly used in IT enterprises. Virtualization provides a platform to run different services of operating systems on the cloud. It facilitates to build multiple virtual machines on a single basic physical machine either in the form of hypervisors or containers. To host many microservice applications, the emergent technology has introduced a model which consists of different operations performed by smaller individual deployed services. Thus, the demand for low-overhead virtualization technique is rapidly developing. There are many lightweight virtualization technologies; docker is one among them, which is an open-source platform. This technology allows developers and system admins to build, create, and run applications using docker engine. This paper provides the performance evaluation of Docker containers and virtual machines using standard benchmark tools such as Sysbench, Phoronix, and Apache benchmark, which include CPU performance, Memory throughput, Storage read/write performance, load test, and operation speed measurement.},
  keywords = {Benchmark tools,Docker Container,Virtual Machine,Virtualization}
}
@inproceedings{Morabito2015,
  title = {Power {{Consumption}} of {{Virtualization Technologies}}: {{An Empirical Investigation}}},
  shorttitle = {Power {{Consumption}} of {{Virtualization Technologies}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 8th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  author = {Morabito, Roberto},
  year = {2015},
  month = dec,
  pages = {522--527},
  doi = {10.1109/UCC.2015.93},
  urldate = {2025-05-21},
  abstract = {Virtualization is growing rapidly as a result of the increasing number of alternative solutions in this area, and of the wide range of application field. Until now, hypervisor-based virtualization has been the de facto solution to perform server virtualization. Recently, container-based virtualization -- an alternative to hypervisors -- has gained more attention because of lightweight characteristics, attracting cloud providers that have already made use of it to deliver their services. However, a gap in the existing research on containers exists in the area of power consumption. This paper presents the results of a performance comparison in terms of power consumption of four different virtualization technologies: KVM and Xen, which are based on hypervisor virtualization, Docker and LXC which are based on container virtualization. The aim of this empirical investigation, carried out by means of a testbed, is to understand how these technologies react to particular workloads. Our initial results show how, despite of the number of virtual entities running, both kinds of virtualization alternatives behave similarly in idle state and in CPU/Memory stress test. Contrarily, the results on network performance show differences between the two technologies.},
  keywords = {Cloud Computing,container,Containers,Docker,Hardware,hypervisor,KVM,LXC,Operating systems,Performance,power consumption,Power demand,Servers,Virtual machine monitors,virtualization,Virtualization,Xen}
}
@misc{projectexigence_rapl,
  author       = {{Project Exigence}},
  title        = {{Running Average Power Limit (RAPL)}},
  howpublished = {\url{https://projectexigence.eu/green-ict-digest/running-average-power-limit-rapl/}},
  note         = {Accessed April 2025},
  year         = {n.d.}
}
@article{raffin2024dissecting,
  title = {Dissecting the {{Software-Based Measurement}} of {{CPU Energy Consumption}}: {{A Comparative Analysis}}},
  shorttitle = {Dissecting the {{Software-Based Measurement}} of {{CPU Energy Consumption}}},
  author = {Raffin, Guillaume and Trystram, Denis},
  year = {2025},
  month = jan,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {36},
  number = {1},
  pages = {96--107},
  issn = {1558-2183},
  doi = {10.1109/TPDS.2024.3492336},
  urldate = {2025-04-02},
  abstract = {Information and Communications Technologies (ICT) are an increasingly important contributor to the environmental crisis. Computer scientists need tools for measuring the footprint of the code they produce and for optimizing it. Running Average Power Limit (RAPL) is a low-level interface designed by Intel that provides a measure of the energy consumption of a CPU (and more) without the need for additional hardware. Since 2017, it is available on most x86 processors, including AMD processors. More and more people are using RAPL for energy measurement, mostly like a black box without deep knowledge of its behavior. Unfortunately, this causes mistakes when implementing measurement tools. In this article, we propose to come back to the basic mechanisms that allow to use RAPL measurements and present a critical analysis of their operations. In addition to long-established mechanisms, we explore the suitability of the recent eBPF technology (formerly and abbreviation for extended Berkeley Packet Filter) for working with RAPL. We release an implementation in Rust that avoids the pitfalls we detected in existing tools, improving correctness, timing accuracy and performance, with desirable properties for monitoring and profiling parallel applications. We provide an experimental study with multiple benchmarks and processor models to evaluate the efficiency of the various mechanisms and their impact on parallel software. We show that no mechanism provides a significant performance advantage over the others. However, they differ significantly in terms of ease-of-use and resiliency. We believe that this work will help the community to develop correct, resilient and lightweight measurement tools.},
  keywords = {Central Processing Unit,Climate change,Closed box,Energy consumption,energy efficiency,Energy efficiency,Environmental monitoring,Global warming,performance analysis,Performance analysis,RAPL library (Running Average Power Limit),software measurement,Software measurement,Weather forecasting}
}
@inproceedings{schone2024energy,
  title = {Energy {{Efficiency Features}} of the {{Intel Alder Lake Architecture}}},
  booktitle = {Proceedings of the 15th {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Sch{\"o}ne, Robert and Velten, Markus and Hackenberg, Daniel and Ilsche, Thomas},
  year = {2024},
  month = may,
  pages = {95--106},
  publisher = {ACM},
  address = {London United Kingdom},
  doi = {10.1145/3629526.3645040},
  urldate = {2025-04-07},
  abstract = {Intel's first heterogeneous processor, Alder Lake, combines two different core architectures from the Core and Atom families: Golden Cove and Gracemont, respectively. While the heterogeneity of this chip can improve performance and energy efficiency, it also increases the complexity of scheduling decisions and power saving mechanisms. In this paper, we analyze performance and energy characteristics of an Alder Lake system and describe effects of power saving mechanisms. We evaluate the factors that influence the time required to switch core and uncore frequencies and waking cores from idle states. In addition, we assess the efficiency of the two core architectures across various workloads. We show that in states with low power consumption, RAPL energy measurements are inaccurate, and actual (externally measured) power consumption also exhibits peculiar patterns. Through experiments, we also examine the newly introduced user space idle states, and the novel telemetry capability. This information can be used by other researchers to design efficient software and further experiments, and explain measured performance on heterogeneous Intel processors.},
  isbn = {979-8-4007-0444-4},
  langid = {english},
  keywords = {notion}
}
@online{amd_energy,
  author       = {AMD},
  title        = {amd\_energy: AMD Energy Driver},
  year         = {2023},
  url          = {https://github.com/amd/amd_energy},
  note         = {Accessed: 2025-04-28}
}
@inproceedings{schone2021energy,
  title = {Energy {{Efficiency Aspects}} of the {{AMD Zen}} 2 {{Architecture}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Sch{\"o}ne, Robert and Ilsche, Thomas and Bielert, Mario and Velten, Markus and Schmidl, Markus and Hackenberg, Daniel},
  year = {2021},
  month = sep,
  pages = {562--571},
  issn = {2168-9253},
  doi = {10.1109/Cluster48925.2021.00087},
  urldate = {2025-04-28},
  abstract = {In High Performance Computing, systems are evaluated based on their computational throughput. However, performance in contemporary server processors is primarily limited by power and thermal constraints. Ensuring operation within a given power envelope requires a wide range of sophisticated control mechanisms. While some of these are handled transparently by hardware control loops, others are controlled by the operating system. A lack of publicly disclosed implementation details further complicates this topic. However, understanding these mechanisms is a prerequisite for any effort to exploit the full computing capability and to minimize the energy consumption of today's server systems. This paper highlights the various energy efficiency aspects of the AMD Zen 2 microarchitecture to facilitate system understanding and optimization. Key findings include qualitative and quantitative descriptions regarding core frequency transition delays, workload-based frequency limitations, effects of I/O die P-states on memory performance as well as discussion on the built-in power monitoring capabilities and its limitations. Moreover, we present specifics and caveats of idle states, wakeup times as well as the impact of idling and inactive hardware threads and cores on the performance of active resources such as other cores.},
  keywords = {AMD,C-State,DVFS,energy efficiency,Energy efficiency,Epyc Rome,Hardware,High performance computing,Instruction sets,Microarchitecture,Operating systems,performance,power saving,RAPL,Throughput,Zen 2}
}
@inproceedings{hackenberg2013power,
  title = {Power Measurement Techniques on Standard Compute Nodes: {{A}} Quantitative Comparison},
  shorttitle = {Power Measurement Techniques on Standard Compute Nodes},
  booktitle = {2013 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Hackenberg, Daniel and Ilsche, Thomas and Sch{\"o}ne, Robert and Molka, Daniel and Schmidt, Maik and Nagel, Wolfgang E.},
  year = {2013},
  month = apr,
  pages = {194--204},
  doi = {10.1109/ISPASS.2013.6557170},
  urldate = {2025-04-28},
  abstract = {Energy efficiency is of steadily growing importance in virtually all areas from mobile to high performance computing. Therefore, lots of research projects focus on this topic and strongly rely on power measurements from their test platforms. The need for finer grained measurement data-both in terms of temporal and spatial resolution (component breakdown)-often collides with very rudimentary measurement setups that rely e.g., on non-professional power meters, IMPI based platform data or model-based interfaces such as RAPL or APM. This paper presents an in-depth study of several different AC and DC measurement methodologies as well as model approaches on test systems with the latest processor generations from both Intel and AMD. We analyze most important aspects such as signal quality, time resolution, accuracy, and measurement overhead and use a calibrated, professional power analyzer as our reference.},
  keywords = {Accuracy,Energy measurement,Instruments,Power demand,Power measurement,Sockets,Temperature measurement}
}
@inproceedings{hackenberg2015energy,
  title = {An {{Energy Efficiency Feature Survey}} of the {{Intel Haswell Processor}}},
  booktitle = {2015 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshop}}},
  author = {Hackenberg, Daniel and Sch{\"o}ne, Robert and Ilsche, Thomas and Molka, Daniel and Schuchart, Joseph and Geyer, Robin},
  year = {2015},
  month = may,
  pages = {896--904},
  doi = {10.1109/IPDPSW.2015.70},
  urldate = {2025-04-28},
  abstract = {The recently introduced Intel Xeon E5-1600 v3 and E5-2600 v3 series processors -- codenamed Haswell-EP -- implement major changes compared to their predecessors. Among these changes are integrated voltage regulators that enable individual voltages and frequencies for every core. In this paper we analyze a number of consequences of this development that are of utmost importance for energy efficiency optimization strategies such as dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT). This includes the enhanced RAPL implementation and its improved accuracy as it moves from modeling to actual measurement. Another fundamental change is that every clock speed above AVX frequency -- including nominal frequency -- is opportunistic and unreliable, which vastly decreases performance predictability with potential effects on scalability. Moreover, we characterize significantly changed p-state transition behavior, and determine crucial memory performance data.}
}
@article{khan2018rapl,
  title = {{{RAPL}} in {{Action}}: {{Experiences}} in {{Using RAPL}} for {{Power Measurements}}},
  shorttitle = {{{RAPL}} in {{Action}}},
  author = {Khan, Kashif Nizam and Hirki, Mikael and Niemi, Tapio and Nurminen, Jukka K. and Ou, Zhonghong},
  year = {2018},
  month = mar,
  journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  volume = {3},
  number = {2},
  pages = {9:1--9:26},
  issn = {2376-3639},
  doi = {10.1145/3177754},
  urldate = {2025-04-07},
  abstract = {To improve energy efficiency and comply with the power budgets, it is important to be able to measure the power consumption of cloud computing servers. Intel's Running Average Power Limit (RAPL) interface is a powerful tool for this purpose. RAPL provides power limiting features and accurate energy readings for CPUs and DRAM, which are easily accessible through different interfaces on large distributed computing systems. Since its introduction, RAPL has been used extensively in power measurement and modeling. However, the advantages and disadvantages of RAPL have not been well investigated yet. To fill this gap, we conduct a series of experiments to disclose the underlying strengths and weaknesses of the RAPL interface by using both customized microbenchmarks and three well-known application level benchmarks: Stream, Stress-ng, and ParFullCMS. Moreover, to make the analysis as realistic as possible, we leverage two production-level power measurement datasets from the Taito, a supercomputing cluster of the Finnish Center of Scientific Computing and also replicate our experiments on Amazon EC2. Our results illustrate different aspects of RAPL and document the findings through comprehensive analysis. Our observations reveal that RAPL readings are highly correlated with plug power, promisingly accurate enough, and have negligible performance overhead. Experimental results suggest RAPL can be a very useful tool to measure and monitor the energy consumption of servers without deploying any complex power meters. We also show that there are still some open issues, such as driver support, non-atomicity of register updates, and unpredictable timings that might weaken the usability of RAPL in certain scenarios. For such scenarios, we pinpoint solutions and workarounds.}
}
@article{servat2016detailed,
  title = {Detailed and Simultaneous Power and Performance Analysis - {{Servat}} - 2016 - {{Concurrency}} and {{Computation}}: {{Practice}} and {{Experience}} - {{Wiley Online Library}}},
  urldate = {2025-05-21},
  howpublished = {https://onlinelibrary.wiley.com/doi/full/10.1002/cpe.3188}
}
@inproceedings{lipp2021platypus,
  title = {{{PLATYPUS}}: {{Software-based Power Side-Channel Attacks}} on X86},
  shorttitle = {{{PLATYPUS}}},
  booktitle = {2021 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Lipp, Moritz and Kogler, Andreas and Oswald, David and Schwarz, Michael and Easdon, Catherine and Canella, Claudio and Gruss, Daniel},
  year = {2021},
  month = may,
  pages = {355--371},
  issn = {2375-1207},
  doi = {10.1109/SP40001.2021.00063},
  urldate = {2025-05-21},
  abstract = {Power side-channel attacks exploit variations in power consumption to extract secrets from a device, e.g., cryptographic keys. Prior attacks typically required physical access to the target device and specialized equipment such as probes and a high-resolution oscilloscope.In this paper, we present PLATYPUS attacks, which are novel software-based power side-channel attacks on Intel server, desktop, and laptop CPUs. We exploit unprivileged access to the Intel Running Average Power Limit (RAPL) interface that exposes values directly correlated with power consumption, forming a low-resolution side channel.We show that with sufficient statistical evaluation, we can observe variations in power consumption, which distinguish different instructions and different Hamming weights of operands and memory loads. This enables us to not only monitor the control flow of applications but also to infer data and extract cryptographic keys. We demonstrate how an unprivileged attacker can leak AES-NI keys from Intel SGX and the Linux kernel, break kernel address-space layout randomization (KASLR), infer secret instruction streams, and establish a timing-independent covert channel. We also present a privileged attack on mbed TLS, utilizing precise execution control to recover RSA keys from an SGX enclave. We discuss countermeasures and show that mitigating these attacks in a privileged context is not trivial.},
  keywords = {Energy consumption,Portable computers,Power demand,Privacy,Servers,Side-channel attacks,Thermal management}
}
@techreport{intel2023,
  author       = {{Intel Corporation}},
  title        = {{Intel\textsuperscript{\textregistered} 64 and IA-32 Architectures Software Developer’s Manual Volume 4: Model-Specific Registers}},
  institution  = {{Intel Corporation}},
  number       = {335592-081US},
  year         = {2023},
  month        = sep,
  url          = {https://cdrdv2.intel.com/v1/dl/getContent/671098},
  note         = {Accessed 2025-04-28}
}
@misc{kennes2023measuring,
  title = {Measuring {{IT Carbon Footprint}}: {{What}} Is the {{Current Status Actually}}?},
  shorttitle = {Measuring {{IT Carbon Footprint}}},
  author = {Kennes, Tom},
  year = {2023},
  month = jun,
  number = {arXiv:2306.10049},
  eprint = {2306.10049},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.10049},
  urldate = {2025-04-23},
  abstract = {Despite the new Corporate Sustainability Reporting Directive from the European Union, which presses large enterprises to be more transparent about their GHG emissions, and though large technology- or advisory firms might peddle otherwise, there are plenty of challenges ahead when it comes to measuring GHG emissions from IT activities in the first place. This paper categories those challenges into 4 categories, and explains the current status, shortcomings and potential future research directions. These categories are: measuring software energy consumption, server overhead energy consumption, Energy Mix and emissions from embodied carbon. Next to that, various non-profit and open-source initiatives are introduced as well as a mathematical framework, based on CPU consumption, that can act as a rule-of-thumb for quick and effortless assessments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Software Engineering}
}
@inproceedings{paniego2018analysis,
  title = {Analysis of {{RAPL Energy Prediction Accuracy}} in a {{Matrix Multiplication Application}} on {{Shared Memory}}},
  booktitle = {Computer {{Science}} -- {{CACIC}} 2017},
  author = {Paniego, Juan Manuel and Gallo, Silvana and Pi Puig, Mart{\'i}n and Chichizola, Franco and De Giusti, Laura and Balladini, Javier},
  editor = {De Giusti, Armando Eduardo},
  year = {2018},
  pages = {37--46},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-75214-3_4},
  abstract = {In recent years, energy consumption has emerged as one of the biggest issues in the development of HPC applications. The traditional approach of parallel and distributed computing has changed its perspective from looking for greater computational efficiency to an approach that balances performance with energy consumption. As a consequence, different metrics and measurement mechanisms have been implemented to achieve this balance. The objective of this article focuses on monitoring and analyzing energy consumption for a given application through physical measurements and a software interface based on hardware counters. A comparison of the energy values gathered by Intel RAPL versus physical measurements obtained through the processor power source is presented. These measurements are applied during the execution of a classic matrix multiplication application. Our results show that, for the application being considered, the average power required by the processor has an error of up to 22\% versus the values predicted by RAPL.},
  isbn = {978-3-319-75214-3},
  langid = {english},
  keywords = {Energy consumption,Hardware counters,Perf,Power,Prediction,RAPL}
}
@online{scaphandre_documentation,
  author       = {Hubblo-org},
  title        = {Scaphandre Documentation},
  year         = {2024},
  url          = {https://github.com/hubblo-org/scaphandre-documentation},
  note         = {Accessed: 2025-04-28}
}
@online{joularjx,
  author       = {JoularJX Contributors},
  title        = {JoularJX: Energy profiling agent for Java applications},
  year         = {2023},
  url          = {https://github.com/joular/joularjx},
  note         = {Accessed: 2025-04-28}
}
@online{kepler_energy,
  author       = {Meta Platforms, Inc.},
  title        = {Kepler: Kubernetes-based power and energy estimation framework},
  year         = {2023},
  url          = {https://github.com/sustainable-computing-io/kepler},
  note         = {Accessed: 2025-04-28}
}
@online{aipowermeter,
  author       = {GreenAI-UPPA},
  title        = {AI PowerMeter: A Tool to Estimate the Energy Consumption of AI Workloads},
  year         = {2023},
  url          = {https://greenai-uppa.github.io/AIPowerMeter/},
  note         = {Accessed: 2025-04-28}
}
@online{codecarbon,
  author       = {MLCO2},
  title        = {CodeCarbon: Track emissions from your computing},
  year         = {2023},
  url          = {https://github.com/mlco2/codecarbon},
  note         = {Accessed: 2025-04-28}
}
@online{powertop,
  author       = {Intel Corporation},
  title        = {PowerTOP: Linux tool to diagnose issues with power consumption and power management},
  year         = {2023},
  url          = {https://github.com/fenrus75/powertop},
  note         = {Accessed: 2025-04-28}
}
@online{greencodingdocs,
  author       = {Tarara, Arne},
  title        = {Green Coding Documentation},
  year         = {2023},
  url          = {https://github.com/green-coding-solutions/green-metrics-tool},
  note         = {Accessed: 2025-04-28}
}
@article{fieni2024powerapi,
  title = {{{PowerAPI}}: {{A Python}} Framework for Building Software-Defined Power Meters},
  shorttitle = {{{PowerAPI}}},
  author = {Fieni, Guillaume and Acero, Daniel Romero and Rust, Pierre and Rouvoy, Romain},
  year = {2024},
  month = jun,
  journal = {Journal of Open Source Software},
  volume = {9},
  number = {98},
  pages = {6670},
  publisher = {Open Journals},
  doi = {10.21105/joss.06670},
  urldate = {2025-06-21},
  abstract = {Software that we use daily for accessing digital services from connected devices has a negative impact on the environment as it consumes energy. These digital services, hosted by physical machines around the world, also contribute to planetary pollution. Unfortunately, providers of these online services mostly focus on hardware efficiency to reduce the environmental impact without considering the software they host. For this reason, we propose PowerAPI (G. Fieni, Romero, et al., 2024), a software-defined solution that delivers real-time estimations of software power consumption to spot opportunities to reduce it and therefore to limit their impact on the planet beyond hardware improvements.},
  keywords = {Energy,Framework,Measurement,Power,Toolkit}
}

@software{OpenAI_ChatGPT_2025,
  author    = {OpenAI},
  year      = {2025},
  title     = {{ChatGPT} (Version 4o)},
  url       = {https://chat.openai.com},
  note      = {Used for document generation and formatting}
}

@misc{copilot2025,
  author       = {{GitHub}},
  title        = {{GitHub Copilot}},
  year         = {2025},
  howpublished = {\url{https://github.com/features/copilot}},
  note         = {Visual Studio Code extension for AI-powered code completion},
}
@article{long2022review,
  title = {A Review of Energy Efficiency Evaluation Technologies in Cloud Data Centers},
  author = {Long, Saiqin and Li, Yuan and Huang, Jinna and Li, Zhetao and Li, Yanchun},
  year = {2022},
  month = apr,
  journal = {Energy and Buildings},
  volume = {260},
  pages = {111848},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2022.111848},
  urldate = {2025-04-20},
  abstract = {The energy consumption by data centers is expanding in tandem with the rapid rise of the digital economy. Data centers, as high-energy-consumption organizations, have garnered extensive attention from society in order to accomplish energy conservation and emission reduction. As a result, improving the energy efficiency of cloud data centers has become a major topic of research. Researchers are working hard to develop practical energy efficiency evaluation methodologies and metrics in order to attain this goal. This article summarizes data center energy efficiency evaluation methods, classifies existing energy efficiency evaluation metrics, examines the current state and challenges of data center energy efficiency evaluation, and makes recommendations for improving energy efficiency evaluation technology to assist cloud operators, decision-makers, and researchers in developing appropriate energy efficiency evaluation strategies. We give data center researchers a better grasp of energy efficiency evaluation and encourage them to combine theory and practice in energy efficiency evaluation and utilize more advanced metrics to assess data center energy efficiency. This is a critical step in the quest for the most advanced green technology, as well as a significant step toward reaching sustainable development goals.},
  keywords = {Data center,Energy consumption,Energy efficiency,Evaluation,Metrics,notion}
}

@article{jin2020review,
  title = {A Review of Power Consumption Models of Servers in Data Centers},
  author = {Jin, Chaoqiang and Bai, Xuelian and Yang, Chao and Mao, Wangxin and Xu, Xin},
  year = {2020},
  month = may,
  journal = {Applied Energy},
  volume = {265},
  pages = {114806},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2020.114806},
  urldate = {2025-03-16},
  abstract = {This study provides an overview of power consumption models of servers in data centers. The server is the basic unit of both power and heat flow paths; therefore, its power consumption model can be used for both energy management and thermal management. Investigations of server power trends were carried out according to the data from the Standard Performance Evaluation Corporation (SPEC). It is found that a heavier workload can be handled without consuming more energy, and the difference between the peak power and idle power of the servers is not consistent from generation to generation. Furthermore, the existing power consumption models are categorized as additive models, baseline power~+~active power (BA) models, and other models based on calculation formula and other factors. Specifically, there are four forms of BA models: linear regression models, power function models, non-linear models and polynomial models. Besides, these models have been compared in terms of accuracy. It can be found that the polynomial model and the linear regression model perform better in terms of accuracy. Additionally, the model applications are summarized. Considering server architecture upgrades and technological innovation, the establishment of the new model and its application scenarios are discussed. Moreover, in-depth and accurate power consumption models must be extensively researched and applied to effectively improve data centers, including information technology (IT) equipment and cooling equipment, in terms of overall energy performance.},
  keywords = {Data center,Energy performance,notion,Power consumption model,Server}
}
@inproceedings{tropgen202416,
  title = {16 {{Years}} of {{SPEC Power}}: {{An Analysis}} of X86 {{Energy Efficiency Trends}}},
  shorttitle = {16 {{Years}} of {{SPEC Power}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Cluster Computing Workshops}} ({{CLUSTER Workshops}})},
  author = {Tr{\"o}pgen, Hannes and Sch{\"o}ne, Robert and Ilsche, Thomas and Hackenberg, Daniel},
  year = {2024},
  month = sep,
  pages = {76--80},
  doi = {10.1109/CLUSTERWorkshops61563.2024.00020},
  urldate = {2025-04-20},
  abstract = {The SPEC Power benchmark offers valuable insights into the energy efficiency of server systems, allowing comparisons across various hardware and software configurations. Benchmark results are publicly available for hundreds of systems from different vendors, published since 2007. We leverage this data to perform an analysis of trends in x86 server systems, focusing on power consumption, energy efficiency, energy proportionality and idle power consumption. Through this analysis, we aim to provide a clearer understanding of how server energy efficiency has evolved and the factors influencing these changes.},
  keywords = {Benchmark testing,Computer architecture,Computer performance,Conferences,Energy efficiency,Focusing,Hardware,High performance computing,Market research,Performance analysis,Power demand,Processor energy efficiency,Servers,Software}
}
@article{lin2020taxonomy,
  title = {A {{Taxonomy}} and {{Survey}} of {{Power Models}} and {{Power Modeling}} for {{Cloud Servers}}},
  author = {Lin, Weiwei and Shi, Fang and Wu, Wentai and Li, Keqin and Wu, Guangxin and Mohammed, Al-Alas},
  year = {2020},
  month = sep,
  journal = {ACM Comput. Surv.},
  volume = {53},
  number = {5},
  pages = {100:1--100:41},
  issn = {0360-0300},
  doi = {10.1145/3406208},
  urldate = {2025-04-20},
  abstract = {Due to the increasing demand of cloud resources, the ever-increasing number and scale of cloud data centers make their massive power consumption a prominent issue today. Evidence reveals that the behaviors of cloud servers make the major impact on data centers' power consumption. Although extensive research can be found in this context, a systematic review of the models and modeling methods for the entire hierarchy (from underlying hardware components to the upper-layer applications) of the cloud server is still missing, which is supposed to cover the relevant studies on physical and virtual cloud server instances, server components, and cloud applications. In this article, we summarize a broad range of relevant studies from three perspectives: power data acquisition, power models, and power modeling methods for cloud servers (including bare-metal, virtual machine (VM), and container instances). We present a comprehensive taxonomy on the collection methods of server-level power data, the existing mainstream power models at multiple levels from hardware to software and application, and commonly used methods for modeling power consumption including classical regression analysis and emerging methods like reinforcement learning. Throughout the work, we introduce a variety of models and methods, illustrating their implementation, usability, and applicability while discussing the limitations of existing approaches and possible ways of improvement. Apart from reviewing existing studies on server power models and modeling methods, we further figure out several open challenges and possible research directions, such as the study on modeling the power consumption of lightweight virtual units like unikernel and the necessity of further explorations toward empowering server power estimation/prediction with machine learning. As power monitoring is drawing increasing attention from cloud service providers (CSPs), this survey provides useful guidelines on server power modeling and can be inspiring for further research on energy-efficient data centers.},
  keywords = {notion}
}
@inproceedings{song2013unified,
  title = {Unified Performance and Power Modeling of Scientific Workloads},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Energy Efficient Supercomputing}}},
  author = {Song, Shuaiwen Leon and Barker, Kevin and Kerbyson, Darren},
  year = {2013},
  month = nov,
  series = {{{E2SC}} '13},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2536430.2536435},
  urldate = {2025-05-21},
  abstract = {It is expected that scientific applications executing on future large-scale HPC must be optimized not only in terms of performance, but also in terms of power consumption. As power and energy become increasingly constrained resources, researchers and developers must have access to tools that will allow for accurate prediction of both performance and power consumption. Reasoning about performance and power consumption in concert will be critical for achieving maximum utilization of limited resources on future HPC systems. To this end, we present a unified performance and power model for the Nek-Bone mini-application developed as part of the DOE's CESAR Exascale Co-Design Center. Our models consider the impact of computation, point-to-point communication, and collective communication individually and quantitatively predict their impact on both performance and energy efficiency. Further, these models are demonstrated to be accurate on currently available HPC system architectures. In this paper, we present our modeling methodology and performance and power models for the Nek-Bone mini-application. We present validation results that indicate the accuracy of these models.},
  isbn = {978-1-4503-2504-2}
}
@article{fan2007power,
  title = {Power Provisioning for a Warehouse-Sized Computer},
  author = {Fan, Xiaobo and Weber, Wolf-Dietrich and Barroso, Luiz Andre},
  year = {2007},
  month = jun,
  journal = {SIGARCH Comput. Archit. News},
  volume = {35},
  number = {2},
  pages = {13--23},
  issn = {0163-5964},
  doi = {10.1145/1273440.1250665},
  urldate = {2025-05-21},
  abstract = {Large-scale Internet services require a computing infrastructure that can beappropriately described as a warehouse-sized computing system. The cost ofbuilding datacenter facilities capable of delivering a given power capacity tosuch a computer can rival the recurring energy consumption costs themselves.Therefore, there are strong economic incentives to operate facilities as closeas possible to maximum capacity, so that the non-recurring facility costs canbe best amortized. That is difficult to achieve in practice because ofuncertainties in equipment power ratings and because power consumption tends tovary significantly with the actual computing activity. Effective powerprovisioning strategies are needed to determine how much computing equipmentcan be safely and efficiently hosted within a given power budget.In this paper we present the aggregate power usage characteristics of largecollections of servers (up to 15 thousand) for different classes ofapplications over a period of approximately six months. Those observationsallow us to evaluate opportunities for maximizing the use of the deployed powercapacity of datacenters, and assess the risks of over-subscribing it. We findthat even in well-tuned applications there is a noticeable gap (7 - 16\%)between achieved and theoretical aggregate peak power usage at the clusterlevel (thousands of servers). The gap grows to almost 40\% in wholedatacenters. This headroom can be used to deploy additional compute equipmentwithin the same power budget with minimal risk of exceeding it. We use ourmodeling framework to estimate the potential of power management schemes toreduce peak power and energy usage. We find that the opportunities for powerand energy savings are significant, but greater at the cluster-level (thousandsof servers) than at the rack-level (tens). Finally we argue that systems needto be power efficient across the activity range, and not only at peakperformance levels.}
}
@inproceedings{hsu2011power,
  title = {Power Signature Analysis of the {{SPECpower}}\_ssj2008 Benchmark},
  booktitle = {({{IEEE ISPASS}}) {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}}},
  author = {Hsu, Chung-Hsing and Poole, Stephen W.},
  year = {2011},
  month = apr,
  pages = {227--236},
  doi = {10.1109/ISPASS.2011.5762739},
  urldate = {2025-05-21},
  abstract = {As the power consumption of a server system becomes a mainstream concern in enterprise environments, understanding the system's power behavior at varying utilization levels provides us a key to select appropriate energy-efficiency optimizations. In this work, we present an in-depth analysis of 177 SPECpower\_ssj2008 results published between 2007-2010 to understand the changes of server's power behavior over time. In particular, we identified simple nonlinear functions appropriate for modeling the power behavior of today's, aggressively power-managed, machines. We consider this work as an important first step towards developing capability for power signature analysis of a high-end computer system.},
  keywords = {Accuracy,energy proportionality,Linearity,Power demand,power modeling,Program processors,Servers,Shape}
}
@inproceedings{desrochers2016validation,
  title = {A {{Validation}} of {{DRAM RAPL Power Measurements}}},
  booktitle = {Proceedings of the {{Second International Symposium}} on {{Memory Systems}}},
  author = {Desrochers, Spencer and Paradis, Chad and Weaver, Vincent M.},
  year = {2016},
  month = oct,
  series = {{{MEMSYS}} '16},
  pages = {455--470},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2989081.2989088},
  urldate = {2025-05-21},
  abstract = {Recent Intel processors support the Running Average Power Level (RAPL) interface, which among other things provides estimated energy measurements for the CPUs, integrated GPU, and DRAM. These measurements are easily accessible by the user, and can be gathered by a wide variety of tools, including the Linux perf\_event interface. This allows unprecedented easy access to energy information when designing and optimizing energy-aware code.While greatly useful, on most systems these RAPL measurements are estimated values, generated on the fly by an on-chip energy model. The values are not documented well, and the results (especially the DRAM results) have undergone only limited validation.We validate the DRAM RAPL results on both desktop and server Haswell machines, with multiple types of DDR3 and DDR4 memory. We instrument the hardware to gather actual power measurements and compare them to the RAPL values returned via Linux perf\_event. We describe the many challenges encountered when instrumenting systems for detailed power measurement.We find that the RAPL results match overall energy and power trends, usually by a constant power offset. The results match best when the DRAM is being heavily utilized, but do not match as well in cases where the system is idle, or when an integrated GPU is using the memory.We also verify that Haswell server machines produce more accurate results, as they include actual power measurements gathered through the integrated voltage regulator.}
}
@inproceedings{kavanagh2016accuracy,
  title = {Accuracy of {{Energy Model Calibration}} with {{IPMI}}},
  booktitle = {2016 {{IEEE}} 9th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Kavanagh, Richard and Armstrong, Django and Djemame, Karim},
  year = {2016},
  month = jun,
  pages = {648--655},
  issn = {2159-6190},
  doi = {10.1109/CLOUD.2016.0091},
  urldate = {2025-04-23},
  abstract = {Energy consumption in Cloud computing is a significant issue and affects aspects such as the cost of energy, cooling in the data center and the environmental impact of cloud data centers. Monitoring and prediction provides the groundwork for improving the energy efficiency of data centers. This monitoring however is required to be fast and efficient without unnecessary overhead. It is also required to scale to the size of a data center where measurement through directly attached Watt meters is unrealistic. This therefore requires models that translate resource utilisation into the power consumed by a physical host. These models require calibrating and are hence subject to error. We discuss the causes of error within these models, focusingupon the use of IPMI in order to gather this data. We make recommendations on ways to mitigate this error without overly complicating the underlying model. The final result of these models is a Watt meter emulator that can provide values for power consumption from hosts in the data center, with an average error of 0.20W.},
  keywords = {accuracy,calibration,Calibration,Cloud,Data center,Data models,Energy Model,IPMI,Mathematical model,Monitoring,Power demand,Power measurement,Sensors}
}
@article{kavanagh2019rapid,
  title = {Rapid and Accurate Energy Models through Calibration with {{IPMI}} and {{RAPL}}},
  author = {Kavanagh, Richard and Djemame, Karim},
  year = {2019},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {31},
  number = {13},
  pages = {e5124},
  issn = {1532-0634},
  doi = {10.1002/cpe.5124},
  urldate = {2025-04-23},
  abstract = {Energy consumption in Cloud and High Performance Computing platforms is a significant issue and affects aspects such as the cost of energy and the cooling of the data center. Host level monitoring and prediction provides the groundwork for improving energy efficiency through the placement of workloads. Monitoring must be fast and efficient without unnecessary overhead, to enable scalability. This precludes the use of Watt meters attached per host, requiring alternative approaches such as integrated measurements and models. IPMI and RAPL are subject to error and partial measurement, which may be mitigated. Models allow for prediction and more responsive measures of power consumption, but require calibrating. The causes of calibration error are discussed, along with mitigation strategies, without overly complicating the underlying model. An outcome is a Watt meter emulator that provides hosts level power measurement along with estimated power consumption for a given workload, with an average error of 0.20W.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {calibration,energy,energy model,IPMI,power,RAPL}
}
@inproceedings{agusti2024powerheat,
  title = {{{PowerHeat}}: {{A}} Non-Intrusive Approach for Estimating the Power Consumption of Bare Metal Water-Cooled Servers},
  shorttitle = {{{PowerHeat}}},
  booktitle = {2024 {{IEEE International Conferences}} on {{Internet}} of {{Things}} ({{iThings}}) and {{IEEE Green Computing}} \& {{Communications}} ({{GreenCom}}) and {{IEEE Cyber}}, {{Physical}} \& {{Social Computing}} ({{CPSCom}}) and {{IEEE Smart Data}} ({{SmartData}}) and {{IEEE Congress}} on {{Cybermatics}}},
  author = {Agusti, Maxime and Caron, Eddy and Fichel, Benjamin and Lef{\`e}vre, Laurent and Nicol, Olivier and Orgerie, Anne-C{\'e}cile},
  year = {2024},
  month = aug,
  pages = {415--421},
  issn = {2836-3701},
  doi = {10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00083},
  urldate = {2025-05-21},
  abstract = {Numerous cloud providers offer physical servers for rental in bare metal paradigm. This mode gives customers total control over hardware resources, but limits cloud providers' visibility of their usage. Accurately measuring server energy consumption in this context represents a major challenge, as installing physical energy meters is both costly and complex. Existing energy models are generally based on system usage data, which is incompatible with the general privacy policies of bare-metal server contracts. To deal with these problems, it is imperative to develop new approaches for estimating the energy consumption of these servers. This paper presents an original non-intrusive method for estimating the energy consumption of a server cooled by direct-chip liquid-cooling, based on the coolant temperature and the processor temperature obtained via IPMI. Our approach is evaluated on an experiment carried out on 19 bare metal servers of a production infrastructure equipped with physical wattmeters.},
  keywords = {bare metal server,cloud computing,Cloud computing,Computational modeling,data center,Data models,direct-to-chip water cooling,Energy consumption,Metals,Power demand,power model,Production,Servers,Temperature sensors,Wattmeters}
}
@inproceedings{white2020monitoring,
  title = {Monitoring and {{Analysis}} of {{Power Consumption}} on {{HPC Clusters}} Using {{XDMoD}}},
  booktitle = {Practice and {{Experience}} in {{Advanced Research Computing}}},
  author = {White, Joseph P. and Innus, Martins and Deleon, Robert L. and Jones, Matthew D. and Furlani, Thomas R.},
  year = {2020},
  month = jul,
  pages = {112--119},
  publisher = {ACM},
  address = {Portland OR USA},
  doi = {10.1145/3311790.3396624},
  urldate = {2025-04-23},
  abstract = {As part of the NSF funded XMS project we are developing tools and techniques for the audit and analysis of HPC infrastructure. This includes a suite of tools for the analysis of HPC jobs based on performance metrics collected from compute nodes. Although it may not be salient to the user, the energy consumption of an HPC system is an important part of the cost of maintenance and contributes a substantial fraction of the cost of calculations done with the system. We added support for energy usage analysis to the open-source XDMoD tool chain. This allows HPC centers to provide information directly to HPC stakeholders about the power consumption. This includes providing end users with energy usage information about their jobs as well as providing data to allow HPC center staff to analyze how the energy usage of the system is related to other system parameters. We explain how energy metrics were added to XDMoD and describe the issues we overcame in instrumenting a 1400 node academic HPC cluster. We present an analysis of 14 months of data collected on real jobs on the cluster. We performed a machine learning analysis of the data and show how energy usage is related to other system performance metrics.},
  isbn = {978-1-4503-6689-2},
  langid = {english}
}
@inproceedings{wang2019empirical,
  title = {An {{Empirical Study}} of {{Power Characterization Approaches}} for {{Servers}}},
  booktitle = {{{ENERGY}} 2019 - {{The Ninth International Conference}} on {{Smart Grids}}, {{Green Communications}} and {{IT Energy-aware Technologies}}},
  author = {Wang, Yewan and N{\"o}rtersh{\"a}user, David and Masson, St{\'e}phane Le and Menaud, Jean-Marc},
  year = {2019},
  month = jun,
  pages = {1},
  urldate = {2025-04-23},
  abstract = {Data centers are energy-hungry facilities. Emerging studies have proposed energy-aware solutions for reducing the power consumption of data centers. Power consumption characterization of servers is an essential part to realize power-aware adaption strategies. Traditional methods adopt accuracy and secure direct measurements by using physical instruments such as wattmeters. Recently, watt-meter free solutions are adopted widely as an economical replacement. These solutions provide power consumption information by making use of self-resources without additional instruments. There are two commonly adopted solutions: 1) standard specifications that provide interface with integrated sensors, such as Intelligent Platform Management Interface (IPMI) and Redfish; 2) Power models based on system activity related indicators. The energy-aware scheduling decisions are made based on the power values obtained, but few works give information about the correctness of the power values while discussing the results or drawing conclusions. In this study, we try to fill up this missing part by evaluating some commonly used, economical ways in obtaining power values. We compare and discuss the reliability, advantages and limitations for the CPU-utilization based power models. The findings highlight the challenges in realizing accurate and reliable power models. We also evaluate the reliability of IPMI and RedFish, in order to give references in choosing appropriate power characterization solutions.},
  langid = {english}
}
@misc{thomas-krenn-redfish,
  author = {{Thomas-Krenn.AG}},
  title = {Redfish - Thomas-Krenn-Wiki},
  year = {n.d.},
  url = {https://www.thomas-krenn.com/de/wiki/Redfish},
  note = {Accessed: April 27, 2025}
}
@inproceedings{jay2023experimental,
  title = {An Experimental Comparison of Software-Based Power Meters: Focus on {{CPU}} and {{GPU}}},
  shorttitle = {An Experimental Comparison of Software-Based Power Meters},
  booktitle = {2023 {{IEEE}}/{{ACM}} 23rd {{International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGrid}})},
  author = {Jay, Mathilde and Ostapenco, Vladimir and Lefevre, Laurent and Trystram, Denis and Orgerie, Anne-C{\'e}cile and Fichel, Benjamin},
  year = {2023},
  month = may,
  pages = {106--118},
  doi = {10.1109/CCGrid57682.2023.00020},
  urldate = {2025-04-21},
  abstract = {The global energy demand for digital activities is constantly growing. Computing nodes and cloud services are at the heart of these activities. Understanding their energy consumption is an important step towards reducing it. On one hand, physical power meters are very accurate in measuring energy but they are expensive, difficult to deploy on a large scale, and are not able to provide measurements at the service level. On the other hand, power models and vendor-specific internal interfaces are already available or can be implemented on existing systems. Plenty of tools, called software-based power meters, have been developed around the concepts of power models and internal interfaces, in order to report the power consumption at levels ranging from the whole computing node to applications and services. However, we have found that it can be difficult to choose the right tool for a specific need. In this work, we qualitatively and experimentally compare several software-based power meters able to deal with CPU or GPU-based infrastructures. For this purpose, we evaluate them against high-precision physical power meters while executing various intensive workloads. We extend this empirical study to highlight the strengths and limitations of each software-based power meter.},
  keywords = {Cloud computing,Energy consumption,Energy measurement,Experimental comparison,Machine learning algorithms,Meters,Nonvolatile memory,Power demand,Power measurement,Software evaluation}
}
@manual{intel-sdm,
  title        = {Intel\textsuperscript{\textregistered} 64 and IA-32 Architectures Software Developer’s Manual},
  author       = {{Intel Corporation}},
  year         = {2024},
  note         = {Volume 3B, Chapter 16.10: Plaform Specific Power Management Support. Available online: \url{https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html}},
  institution  = {Intel Corporation}
}
@inproceedings{fieni2020smartwatts,
  title = {{{SmartWatts}}: {{Self-Calibrating Software-Defined Power Meter}} for {{Containers}}},
  shorttitle = {{{SmartWatts}}},
  booktitle = {2020 20th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGRID}})},
  author = {Fieni, Guillaume and Rouvoy, Romain and Seinturier, Lionel},
  year = {2020},
  month = may,
  pages = {479--488},
  doi = {10.1109/CCGrid49817.2020.00-45},
  urldate = {2025-05-21},
  abstract = {Fine-grained power monitoring of software activities becomes unavoidable to maximize the power usage efficiency of data centers. In particular, achieving an optimal scheduling of containers requires the deployment of software-defined power meters to go beyond the granularity of hardware power monitoring sensors, such as Power Distribution Units (PDU) or Intel's Running Average Power Limit (RAPL), to deliver power estimations of activities at the granularity of software containers. However, the definition of the underlying power models that estimate the power consumption remains a long and fragile process that is tightly coupled to the host machine.To overcome these limitations, this paper introduces SmartWatts: a lightweight power monitoring system that adopts online calibration to automatically adjust the CPU and DRAM power models in order to maximize the accuracy of runtime power estimations of containers. Unlike state-of-the-art techniques, SmartWatts does not require any a priori training phase or hardware equipment to configure the power models and can therefore be deployed on a wide range of machines including the latest power optimizations, at no cost.},
  keywords = {Containers,Energy,Hardware,Meters,Monitoring,Power demand,Power measurement,Power model,Random access memory}
}
@article{hahnel2012measuring,
  title = {Measuring Energy Consumption for Short Code Paths Using {{RAPL}}},
  author = {H{\"a}hnel, Marcus and D{\"o}bel, Bj{\"o}rn and V{\"o}lp, Marcus and H{\"a}rtig, Hermann},
  year = {2012},
  month = jan,
  journal = {SIGMETRICS Perform. Eval. Rev.},
  volume = {40},
  number = {3},
  pages = {13--17},
  issn = {0163-5999},
  doi = {10.1145/2425248.2425252},
  urldate = {2025-05-21},
  abstract = {Measuring the energy consumption of software components is a major building block for generating models that allow for energy-aware scheduling, accounting and budgeting. Current measurement techniques focus on coarse-grained measurements of application or system events. However, fine grain adjustments in particular in the operating-system kernel and in application-level servers require power profiles at the level of a single software function. Until recently, this appeared to be impossible due to the lacking fine grain resolution and high costs of measurement equipment. In this paper we report on our experience in using the Running Average Power Limit (RAPL) energy sensors available in recent Intel CPUs for measuring energy consumption of short code paths. We investigate the granularity at which RAPL measurements can be performed and discuss practical obstacles that occur when performing these measurements on complex modern CPUs. Furthermore, we demonstrate how to use the RAPL infrastructure to characterize the energy costs for decoding video slices.}
}
@article{fahad2019comparative,
  title = {A {{Comparative Study}} of {{Methods}} for {{Measurement}} of {{Energy}} of {{Computing}}},
  author = {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi Reddy and Lastovetsky, Alexey},
  year = {2019},
  month = jan,
  journal = {Energies},
  volume = {12},
  number = {11},
  pages = {2204},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en12112204},
  urldate = {2025-04-23},
  abstract = {Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consumption during an application execution is key to application-level energy minimization techniques. There are three popular approaches to providing it: (a) System-level physical measurements using external power meters; (b) Measurements using on-chip power sensors and (c) Energy predictive models. In this work, we present a comprehensive study comparing the accuracy of state-of-the-art on-chip power sensors and energy predictive models against system-level physical measurements using external power meters, which we consider to be the ground truth. We show that the average error of the dynamic energy profiles obtained using on-chip power sensors can be as high as 73\% and the maximum reaches 300\% for two scientific applications, matrix-matrix multiplication and 2D fast Fourier transform for a wide range of problem sizes. The applications are executed on three modern Intel multicore CPUs, two Nvidia GPUs and an Intel Xeon Phi accelerator. The average error of the energy predictive models employing performance monitoring counters (PMCs) as predictor variables can be as high as 32\% and the maximum reaches 100\% for a diverse set of seventeen benchmarks executed on two Intel multicore CPUs (one Haswell and the other Skylake). We also demonstrate that using inaccurate energy measurements provided by on-chip sensors for dynamic energy optimization can result in significant energy losses up to 84\%. We show that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on-chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy. Finally, we present the lessons learned, our recommendations for the use of on-chip sensors and energy predictive models and future directions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {energy efficiency,energy predictive models,GPU,multicore CPU,NVML,performance monitoring counters,power aensors,power meters,RAPL,Xeon Phi}
}
@inproceedings{alt2024experimental,
  title = {An {{Experimental Setup}} to {{Evaluate RAPL Energy Counters}} for {{Heterogeneous Memory}}},
  booktitle = {Proceedings of the 15th {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Alt, Lukas and Kozhokanova, Anara and Ilsche, Thomas and Terboven, Christian and Mueller, Matthias S.},
  year = {2024},
  month = may,
  series = {{{ICPE}} '24},
  pages = {71--82},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3629526.3645052},
  urldate = {2025-04-02},
  abstract = {Power consumption of the main memory in modern heterogeneous high-performance computing (HPC) constitutes a significant part of the total power consumption of a node. This motivates energy-efficient solutions targeting the memory domain as well. Practitioners need reliable energy measurement techniques for analyzing energy and power consumption of applications and performance optimizations. Running Average Power Limit (RAPL) is a common choice, as it provides uncomplicated access to the energy measurements. While RAPL's accuracy has been studied and validated on homogeneous memory platforms, no work we are aware of investigated its accuracy on heterogeneous memory platforms, specifically with high-capacity memory (HCM). This paper describes the process of measuring the memory power consumption externally using riser cards in detail. We validate RAPL's accuracy by comparing results obtained from Intel's Ice Lake-SP system equipped with DDR4 DRAM and Intel Optane Persistent Memory Modules (PMM). In addition, we verify the accuracy of our instrumentation setup by comparing the results from an older Broadwell system with the results in the literature. We show that the RAPL values on a heterogeneous memory system report a higher offset from the reference measurements. The difference is more pronounced at lower memory load for all memory types. Also, we find that RAPL readings are inconsistent between multiple sockets and over time. Based on the evaluated scenarios, we conclude that RAPL overestimates the actual power consumption on heterogeneous memory systems and provide a discussion on the possible causes of this effect.},
  isbn = {979-8-4007-0444-4}
}
@online{greencoding_rapl_sgx,
  author       = {{Green Coding Berlin}},
  title        = {{RAPL, SGX and energy filtering - Influences on power consumption}},
  year         = {2022},
  url          = {https://www.green-coding.io/case-studies/rapl-and-sgx/},
  note         = {Accessed May 2025}
}
@online{intel_rapl_guidance,
  author       = {{Intel Corporation}},
  title        = {{Running Average Power Limit (RAPL) Energy Reporting}},
  year         = {2022},
  url          = {https://www.intel.cn/content/www/cn/zh/developer/articles/technical/software-security-guidance/advisory-guidance/running-average-power-limit-energy-reporting.html},
  note         = {Accessed May 2025}
}
@inproceedings{yang2024accurate,
  title = {Accurate and {{Convenient Energy Measurements}} for {{GPUs}}: {{A Detailed Study}} of {{NVIDIA GPU}}'s {{Built-In Power Sensor}}},
  shorttitle = {Accurate and {{Convenient Energy Measurements}} for {{GPUs}}},
  booktitle = {{{SC24}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Yang, Zeyu and Adamek, Karel and Armour, Wesley},
  year = {2024},
  month = nov,
  pages = {1--17},
  doi = {10.1109/SC41406.2024.00028},
  urldate = {2025-05-09},
  abstract = {GPU has emerged as the go-to accelerator for HPC workloads, however its power consumption has become a major limiting factor for further scaling HPC systems. An accurate understanding of GPU power consumption is essential for further improving its energy efficiency, and consequently reducing the associated carbon footprint. Despite the limited documentation and lack of understanding, NVIDIA GPUs' built-in power sensor is widely used in energy-efficient computing research. Our study seeks to elucidate the internal mechanisms of the power readings provided by nvidia-smi and assess the accuracy of the measurements. We evaluated over 70 different GPUs across 12 architectural generations, and identified several unforeseen problems that can lead to drastic under/overestimation of energy consumed, for example on the A100 and H100 GPUs only 25\% of the runtime is sampled. We proposed several mitigations that could reduce the energy measurement error by an average of 35\% in the test cases we present.},
  keywords = {Accuracy,Energy consumption,Energy efficient computing,Energy measurement,Graphics processing units,Green computing,High performance computing,Measurement standards,Meters,Power demand,Power measurement,Prevention and mitigation,Runtime}
}
@inproceedings{jouppi2017datacenter,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}},
  booktitle = {Proceedings of the 44th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  month = jun,
  series = {{{ISCA}} '17},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3079856.3080246},
  urldate = {2025-05-21},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  isbn = {978-1-4503-4892-8}
}
@misc{k8s_gpu_support,
  title        = {GPUs in Kubernetes},
  author       = {Kubernetes Documentation},
  year         = {2025},
  url          = {https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/},
  note         = {Accessed: 2025-05-09}
}
@misc{datadog_report,
  title        = {2024 Container Report},
  author       = {Datadog, Inc.},
  year         = {2024},
  url          = {https://www.datadoghq.com/container-report/},
  note         = {Accessed: 2025-05-09}
}
@misc{tensorflow_k8s,
  title        = {Running TensorFlow on Kubernetes},
  author       = {TensorFlow Documentation},
  year         = {2025},
  url          = {https://www.tensorflow.org/tfx/serving/serving_kubernetes},
  note         = {Accessed: 2025-05-09}
}
@misc{nvidia_virtualization,
  title        = {NVIDIA Virtualization Resources},
  author       = {NVIDIA Corporation},
  year         = {2025},
  url          = {https://www.nvidia.com/de-de/data-center/virtualization/resources/},
  note         = {Accessed: 2025-05-09}
}
@misc{amd_instinct_virtualization,
  title        = {AMD Instinct Virtualization Documentation},
  author       = {AMD Corporation},
  year         = {2025},
  url          = {https://instinct.docs.amd.com/projects/virt-drv/en/latest/index.html},
  note         = {Accessed: 2025-05-09}
}
@misc{nvidia_mig_user_guide,
  title        = {NVIDIA Multi-Instance GPU (MIG) User Guide},
  author       = {NVIDIA Corporation},
  year         = {2025},
  url          = {https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html},
  note         = {Accessed: 2025-05-09}
}
@misc{nvidia_passthrough,
  title        = {NVIDIA GPU Passthrough Documentation},
  author       = {NVIDIA Corporation},
  year         = {2025},
  url          = {https://docs.nvidia.com/datacenter/tesla/gpu-passthrough/index.html},
  note         = {Accessed: 2025-05-09}
}
@misc{nvidia_smi_docs,
  title        = {NVIDIA System Management Interface (nvidia-smi)},
  author       = {NVIDIA Corporation},
  year         = {2025},
  url          = {https://developer.nvidia.com/system-management-interface},
  note         = {Accessed: 2025-05-09}
}
@inproceedings{yang2024accurate,
  title = {Accurate and {{Convenient Energy Measurements}} for {{GPUs}}: {{A Detailed Study}} of {{NVIDIA GPU}}'s {{Built-In Power Sensor}}},
  shorttitle = {Accurate and {{Convenient Energy Measurements}} for {{GPUs}}},
  booktitle = {{{SC24}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Yang, Zeyu and Adamek, Karel and Armour, Wesley},
  year = {2024},
  month = nov,
  pages = {1--17},
  doi = {10.1109/SC41406.2024.00028},
  urldate = {2025-05-09},
  abstract = {GPU has emerged as the go-to accelerator for HPC workloads, however its power consumption has become a major limiting factor for further scaling HPC systems. An accurate understanding of GPU power consumption is essential for further improving its energy efficiency, and consequently reducing the associated carbon footprint. Despite the limited documentation and lack of understanding, NVIDIA GPUs' built-in power sensor is widely used in energy-efficient computing research. Our study seeks to elucidate the internal mechanisms of the power readings provided by nvidia-smi and assess the accuracy of the measurements. We evaluated over 70 different GPUs across 12 architectural generations, and identified several unforeseen problems that can lead to drastic under/overestimation of energy consumed, for example on the A100 and H100 GPUs only 25\% of the runtime is sampled. We proposed several mitigations that could reduce the energy measurement error by an average of 35\% in the test cases we present.},
  keywords = {Accuracy,Energy consumption,Energy efficient computing,Energy measurement,Graphics processing units,Green computing,High performance computing,Measurement standards,Meters,Power demand,Power measurement,Prevention and mitigation,Runtime}
}
@misc{singhania2024methodology,
  title = {{{FinGraV}}: {{Methodology}} for {{Fine-Grain GPU Power Visibility}} and {{Insights}}},
  shorttitle = {{{FinGraV}}},
  author = {Singhania, Varsha and Aga, Shaizeen and Ibrahim, Mohamed Assem},
  year = {2025},
  month = mar,
  number = {arXiv:2412.12426},
  eprint = {2412.12426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.12426},
  urldate = {2025-05-09},
  abstract = {Ubiquity of AI makes optimizing GPU power a priority as large GPU-based clusters are often employed to train and serve AI models. An important first step in optimizing GPU power consumption is high-fidelity and fine-grain power measurement of key AI computations on GPUs. To this end, we observe that as GPUs get more powerful, the resulting sub-millisecond to millisecond executions make fine-grain power analysis challenging. In this work, we first carefully identify the challenges in obtaining fine-grain GPU power profiles. To address these challenges, we devise FinGraV methodology where we employ execution time binning, careful CPU-GPU time synchronization, and power profile differentiation to collect fine-grain GPU power profiles across prominent AI computations and across a spectrum of scenarios. Using the said FinGraV power profiles, we provide both, guidance on accurate power measurement and, in-depth view of power consumption on state-of-the-art AMD Instinct MI300X. For the former, we highlight a methodology for power differentiation across executions. For the latter, we make several observations pertaining to GPU sub-component power consumption and GPU power proportionality across different scenarios. We believe that FinGraV unlocks both an accurate and a deeper view of power consumption of GPUs and opens up avenues for power optimization of these ubiquitous accelerators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Hardware Architecture}
}

@inproceedings{kandiah2021accelwattch,
  title = {{{AccelWattch}}: {{A Power Modeling Framework}} for {{Modern GPUs}}},
  shorttitle = {{{AccelWattch}}},
  booktitle = {{{MICRO-54}}: 54th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Kandiah, Vijay and Peverelle, Scott and Khairy, Mahmoud and Pan, Junrui and Manjunath, Amogh and Rogers, Timothy G. and Aamodt, Tor M. and Hardavellas, Nikos},
  year = {2021},
  month = oct,
  series = {{{MICRO}} '21},
  pages = {738--753},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3466752.3480063},
  urldate = {2025-05-09},
  abstract = {Graphics Processing Units (GPUs) are rapidly dominating the accelerator space, as illustrated by their wide-spread adoption in the data analytics and machine learning markets. At the same time, performance per watt has emerged as a crucial evaluation metric together with peak performance. As such, GPU architects require robust tools that will enable them to model both the performance and the power consumption of modern GPUs. However, while GPU performance modeling has progressed in great strides, power modeling has lagged behind. To mitigate this problem we propose AccelWattch, a configurable GPU power model that resolves two long-standing needs: the lack of a detailed and accurate cycle-level power model for modern GPU architectures, and the inability to capture their constant and static power with existing tools. AccelWattch can be driven by emulation and trace-driven environments, hardware counters, or a mix of the two, models both PTX and SASS ISAs, accounts for power gating and control-flow divergence, and supports DVFS. We integrate AccelWattch with GPGPU-Sim and Accel-Sim to facilitate its widespread use. We validate AccelWattch on a NVIDIA Volta GPU, and show that it achieves strong correlation against hardware power measurements. Finally, we demonstrate that AccelWattch can enable reliable design space exploration: by directly applying AccelWattch tuned for Volta on GPU configurations resembling NVIDIA Pascal and Turing GPUs, we obtain accurate power models for these architectures.},
  isbn = {978-1-4503-8557-2}
}
@misc{van2025powersensor3,
  title = {{{PowerSensor3}}: {{A Fast}} and {{Accurate Open Source Power Measurement Tool}}},
  shorttitle = {{{PowerSensor3}}},
  author = {van der Vlugt, Steven and Oostrum, Leon and Schoonderbeek, Gijs and van Werkhoven, Ben and Veenboer, Bram and Doekemeijer, Krijn and Romein, John W.},
  year = {2025},
  month = apr,
  number = {arXiv:2504.17883},
  eprint = {2504.17883},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17883},
  urldate = {2025-05-09},
  abstract = {Power consumption is a major concern in data centers and HPC applications, with GPUs typically accounting for more than half of system power usage. While accurate power measurement tools are crucial for optimizing the energy efficiency of (GPU) applications, both built-in power sensors as well as state-of-the-art power meters often lack the accuracy and temporal granularity needed, or are impractical to use. Released as open hardware, firmware, and software, PowerSensor3 provides a cost-effective solution for evaluating energy efficiency, enabling advancements in sustainable computing. The toolkit consists of a baseboard with a variety of sensor modules accompanied by host libraries with C++ and Python bindings. PowerSensor3 enables real-time power measurements of SoC boards and PCIe cards, including GPUs, FPGAs, NICs, SSDs, and domain-specific AI and ML accelerators. Additionally, it provides significant improvements over previous tools, such as a robust and modular design, current sensors resistant to external interference, simplified calibration, and a sampling rate up to 20 kHz, which is essential to identify GPU behavior at high temporal granularity. This work describes the toolkit design, evaluates its performance characteristics, and shows several use cases (GPUs, NVIDIA Jetson AGX Orin, and SSD), demonstrating PowerSensor3's potential to significantly enhance energy efficiency in modern computing environments.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Performance}
}
@article{borbaModelingApproachEstimating2022,
  title = {A Modeling Approach for Estimating Performance and Energy Consumption of Storage Systems},
  author = {Borba, Eric and Tavares, Eduardo and Maciel, Paulo},
  year = {2022},
  month = sep,
  journal = {Journal of Computer and System Sciences},
  volume = {128},
  pages = {86--106},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2022.04.001},
  urldate = {2025-05-18},
  abstract = {Improvements in data storage may be constrained by the lower performance of hard disk drives (HDD) and the higher cost per gigabyte of solid-state drives (SSD). To mitigate these issues, hybrid storage architectures have been conceived. Some works evaluate the performance of storage architectures, but energy consumption is usually neglected and not simultaneously evaluated with performance. This paper presents an approach based on generalized stochastic Petri nets (GSPN) for performance and energy consumption evaluation of individual and hybrid storage systems. The proposed models can represent distinct workloads and also estimate throughput, response time and energy consumption of storage systems. Experiments based on industry-standard benchmarks are adopted to demonstrate the feasibility of the proposed approach.},
  keywords = {Cloud computing,Data management,Energy consumption,Hybrid storage,Performance evaluation,Solid-state drive,Stochastic Petri nets}
}
@inproceedings{hylickAnalysisHardDrive2008a,
  title = {An {{Analysis}} of {{Hard Drive Energy Consumption}}},
  booktitle = {2008 {{IEEE International Symposium}} on {{Modeling}}, {{Analysis}} and {{Simulation}} of {{Computers}} and {{Telecommunication Systems}}},
  author = {Hylick, Anthony and Sohan, Ripduman and Rice, Andrew and Jones, Brian},
  year = {2008},
  month = sep,
  pages = {1--10},
  issn = {2375-0227},
  doi = {10.1109/MASCOT.2008.4770567},
  urldate = {2025-05-21},
  abstract = {The increasing storage capacity and necessary redundancy of data centers and other large-scale IT facilities has drawn attention to the issue of reducing the power consumption of hard drives. This work comprehensively investigates the power consumption of hard drives to determine typical runtime power profiles. We have instrumented at a fine-grained level and present our findings which show that (i) the energy consumed by the electronics of a drive is just as important as the mechanical energy consumption; (ii) the energy required to access data is affected by physical location on a drive; and (iii) the size of data transfers has measurable effect on power consumption.},
  keywords = {Communications technology,Computer science,Delay,Energy consumption,Internet,Jacobian matrices,Protocols,Prototypes,Speech,Telecommunication traffic}
}
@article{choDesignTradeoffsSSDs2015,
  title = {Design {{Tradeoffs}} of {{SSDs}}: {{From Energy Consumption}}'s {{Perspective}}},
  shorttitle = {Design {{Tradeoffs}} of {{SSDs}}},
  author = {Cho, Seokhei and Park, Changhyun and Won, Youjip and Kang, Sooyong and Cha, Jaehyuk and Yoon, Sungroh and Choi, Jongmoo},
  year = {2015},
  month = mar,
  journal = {ACM Trans. Storage},
  volume = {11},
  number = {2},
  pages = {8:1--8:24},
  issn = {1553-3077},
  doi = {10.1145/2644818},
  urldate = {2025-05-18},
  abstract = {In this work, we studied the energy consumption characteristics of various SSD design parameters. We developed an accurate energy consumption model for SSDs that computes aggregate, as well as component-specific, energy consumption of SSDs in sub-msec time scale. In our study, we used five different FTLs (page mapping, DFTL, block mapping, and two different hybrid mappings) and four different channel configurations (two, four, eight, and 16 channels) under seven different workloads (from large-scale enterprise systems to small-scale desktop applications) in a combinatorial manner. For each combination of the aforementioned parameters, we examined the energy consumption for individual hardware components of an SSD (microcontroller, DRAM, NAND flash, and host interface). The following are some of our findings. First, DFTL is the most energy-efficient address-mapping scheme among the five FTLs we tested due to its good write amplification and small DRAM footprint. Second, a significant fraction of energy is being consumed by idle flash chips waiting for the completion of NAND operations in the other channels. FTL should be designed to fully exploit the internal parallelism so that energy consumption by idle chips is minimized. Third, as a means to increase the internal parallelism, increasing way parallelism (the number of flash chips in a channel) is more effective than increasing channel parallelism in terms of peak energy consumption, performance, and hardware complexity. Fourth, in designing high-performance and energy-efficient SSDs, channel switching delay, way switching delay, and page write latency need to be incorporated in an integrated manner to determine the optimal configuration of internal parallelism.}
}
@misc{storedbits_ssd_power,
  author       = {{StoredBits}},
  title        = {SSD Power Consumption: How Much Power Does Your SSD Use?},
  year         = {2023},
  url          = {https://storedbits.com/ssd-power-consumption/},
  note         = {Accessed: 2025-05-21}
}
@article{shinPowerConsumptionCharacterization,
  title = {Power {{Consumption Characterization}} of {{Flash Memory SSD}}},
  author = {Shin, Seungyong and Shin, Dongkun},
  abstract = {Solid state disks (SSDs) are replacing hard disk drives (HDDs) due to their high random access performance and low power consumption. Although SSDs are low power storage devices, the power consumptions of SSDs are not negligible as SSDs adopt parallel architectures. In this paper, we analyze the power and energy consumption behaviors of SSDs depending on the I/O request patterns and provide several hints on energy optimization. In addition, our analysis technique enables to extract several architectural features of target SSD which vendors do not provide to users.},
  langid = {english}
}
@inproceedings{liWhichStorageDevice2014,
  title = {Which {{Storage Device Is}} the {{Greenest}}? {{Modeling}} the {{Energy Cost}} of {{I}}/{{O Workloads}}},
  shorttitle = {Which {{Storage Device Is}} the {{Greenest}}?},
  booktitle = {2014 {{IEEE}} 22nd {{International Symposium}} on {{Modelling}}, {{Analysis}} \& {{Simulation}} of {{Computer}} and {{Telecommunication Systems}}},
  author = {Li, Yan and Long, Darrell D.E.},
  year = {2014},
  month = sep,
  pages = {100--105},
  issn = {2375-0227},
  doi = {10.1109/MASCOTS.2014.20},
  urldate = {2025-05-19},
  abstract = {The performance requirements and amount of work of an I/O workload affect the number of storage devices and the run time needed by the workload, and should be included in the calculation of the cost or energy consumption of storage devices. This paper introduces models to calculate the cost and energy consumption of storage devices for running a variety of workloads, categorized by their dominant requirements. Measurements of two latest hard disk and solid-state drive (SSD) are included to illustrate the models in practice. Contrary to common belief, SSD is not the energy efficient choice for many workloads.},
  keywords = {Bandwidth,device,energy,Energy consumption,Energy measurement,Equations,Lead,Mathematical model,measurement,model,Power measurement,workload}
}
@inproceedings{allaloufStorageModelingPower2009,
  title = {Storage Modeling for Power Estimation},
  booktitle = {Proceedings of {{SYSTOR}} 2009: {{The Israeli Experimental Systems Conference}}},
  author = {Allalouf, Miriam and Arbitman, Yuriy and Factor, Michael and Kat, Ronen I. and Meth, Kalman and Naor, Dalit},
  year = {2009},
  month = may,
  series = {{{SYSTOR}} '09},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1534530.1534535},
  urldate = {2025-05-18},
  abstract = {Power consumption is a major issue in today's datacenters. Storage typically comprises a significant percentage of datacenter power. Thus, understanding, managing, and reducing storage power consumption is an essential aspect of any efforts that address the total power consumption of datacenters. We developed a scalable power modeling method that estimates the power consumption of storage workloads. The modeling concept is based on identifying the major workload contributors to the power consumed by the disk arrays.To estimate the power consumed by a given host workload, our method translates the workload to the primitive activities induced on the disks. In addition, we identified that I/O queues have a fundamental influence on the power consumption. Our power estimation results are highly accurate, with only 2\% deviation for typical random workloads with small transfer sizes (up to 8K), and a deviation of up to 8\% for workloads with large transfer sizes. We successfully integrated our modeling into a power-aware capacity planning tool to predict system power requirements and integrated it into an online storage system to provide online estimation for the power consumed.},
  isbn = {978-1-60558-623-6}
}
@online{storedbits_hdd,
  author       = {{StoreDbits}},
  title        = {{Hard Drive Power Consumption (HDD)}},
  year         = {2023},
  url          = {https://storedbits.com/hard-drive-power-consumption/},
  note         = {Accessed May 2025}
}
@online{storedbits_ssd,
  author       = {{StoreDbits}},
  title        = {{SSD Power Consumption}},
  year         = {2023},
  url          = {https://storedbits.com/ssd-power-consumption/},
  note         = {Accessed May 2025}
}
@article{choDesignTradeoffsSSDs2015,
  title = {Design {{Tradeoffs}} of {{SSDs}}: {{From Energy Consumption}}'s {{Perspective}}},
  shorttitle = {Design {{Tradeoffs}} of {{SSDs}}},
  author = {Cho, Seokhei and Park, Changhyun and Won, Youjip and Kang, Sooyong and Cha, Jaehyuk and Yoon, Sungroh and Choi, Jongmoo},
  year = {2015},
  month = mar,
  journal = {ACM Trans. Storage},
  volume = {11},
  number = {2},
  pages = {8:1--8:24},
  issn = {1553-3077},
  doi = {10.1145/2644818},
  urldate = {2025-05-18},
  abstract = {In this work, we studied the energy consumption characteristics of various SSD design parameters. We developed an accurate energy consumption model for SSDs that computes aggregate, as well as component-specific, energy consumption of SSDs in sub-msec time scale. In our study, we used five different FTLs (page mapping, DFTL, block mapping, and two different hybrid mappings) and four different channel configurations (two, four, eight, and 16 channels) under seven different workloads (from large-scale enterprise systems to small-scale desktop applications) in a combinatorial manner. For each combination of the aforementioned parameters, we examined the energy consumption for individual hardware components of an SSD (microcontroller, DRAM, NAND flash, and host interface). The following are some of our findings. First, DFTL is the most energy-efficient address-mapping scheme among the five FTLs we tested due to its good write amplification and small DRAM footprint. Second, a significant fraction of energy is being consumed by idle flash chips waiting for the completion of NAND operations in the other channels. FTL should be designed to fully exploit the internal parallelism so that energy consumption by idle chips is minimized. Third, as a means to increase the internal parallelism, increasing way parallelism (the number of flash chips in a channel) is more effective than increasing channel parallelism in terms of peak energy consumption, performance, and hardware complexity. Fourth, in designing high-performance and energy-efficient SSDs, channel switching delay, way switching delay, and page write latency need to be incorporated in an integrated manner to determine the optimal configuration of internal parallelism.}
}
@inproceedings{liWhichStorageDevice2014,
  title = {Which {{Storage Device Is}} the {{Greenest}}? {{Modeling}} the {{Energy Cost}} of {{I}}/{{O Workloads}}},
  shorttitle = {Which {{Storage Device Is}} the {{Greenest}}?},
  booktitle = {2014 {{IEEE}} 22nd {{International Symposium}} on {{Modelling}}, {{Analysis}} \& {{Simulation}} of {{Computer}} and {{Telecommunication Systems}}},
  author = {Li, Yan and Long, Darrell D.E.},
  year = {2014},
  month = sep,
  pages = {100--105},
  issn = {2375-0227},
  doi = {10.1109/MASCOTS.2014.20},
  urldate = {2025-05-19},
  abstract = {The performance requirements and amount of work of an I/O workload affect the number of storage devices and the run time needed by the workload, and should be included in the calculation of the cost or energy consumption of storage devices. This paper introduces models to calculate the cost and energy consumption of storage devices for running a variety of workloads, categorized by their dominant requirements. Measurements of two latest hard disk and solid-state drive (SSD) are included to illustrate the models in practice. Contrary to common belief, SSD is not the energy efficient choice for many workloads.},
  keywords = {Bandwidth,device,energy,Energy consumption,Energy measurement,Equations,Lead,Mathematical model,measurement,model,Power measurement,workload}
}
@misc{nvmecli_github,
  author       = {{Linux NVMe Maintainers}},
  title        = {{nvme-cli: NVMe management command line interface}},
  year         = {2025},
  howpublished = {\url{https://github.com/linux-nvme/nvme-cli}},
  note         = {Accessed May 2025}
}
@misc{smartmontools_github,
  author       = {{smartmontools developers}},
  title        = {{smartmontools: Control and monitor storage systems using S.M.A.R.T.}},
  year         = {2025},
  howpublished = {\url{https://github.com/smartmontools/smartmontools/}},
  note         = {Accessed May 2025}
}
@inproceedings{sohanCharacterizing10Gbps2010,
  title = {Characterizing 10 {{Gbps}} Network Interface Energy Consumption},
  booktitle = {{{IEEE Local Computer Network Conference}}},
  author = {Sohan, Ripduman and Rice, Andrew and Andrew, W. Moore and Mansley, Kieran},
  year = {2010},
  month = oct,
  pages = {268--271},
  issn = {0742-1303},
  doi = {10.1109/LCN.2010.5735719},
  urldate = {2025-05-30},
  abstract = {This paper quantifies the energy consumption in six 10 Gbps and four 1 Gbps interconnects at a fine-grained level, introducing two metrics for calculating the energy efficiency of a network interface from the perspective of network throughput and host CPU usage. It further compares the energy efficiency of multiport 1 Gbps to 10 Gbps interconnects.}
}
@book{gough2015energy,
  author    = {Corey Gough and Ian Steiner and Winston A. Sanders},
  title     = {Energy Efficient Servers: Blueprints for Data Center Optimization},
  publisher = {Apress},
  year      = {2015},
  isbn      = {9781430266372},
  doi       = {10.1007/978-1-4302-6638-9}
}
@inproceedings{arjonaarocaMeasurementbasedAnalysisEnergy2014,
  title = {A Measurement-Based Analysis of the Energy Consumption of Data Center Servers},
  booktitle = {Proceedings of the 5th International Conference on {{Future}} Energy Systems},
  author = {Arjona Aroca, Jordi and Chatzipapas, Angelos and Fern{\'a}ndez Anta, Antonio and Mancuso, Vincenzo},
  year = {2014},
  month = jun,
  series = {E-{{Energy}} '14},
  pages = {63--74},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2602044.2602061},
  urldate = {2025-06-01},
  abstract = {Energy consumption is a growing issue in data centers, impacting their economic viability and their public image. In this work we empirically characterize the power and energy consumed by different types of servers. In particular, in order to understand the behavior of their energy and power consumption, we perform measurements in different servers. In each of them, we exhaustively measure the power consumed by the CPU, the disk, and the network interface under different configurations, identifying the optimal operational levels. One interesting conclusion of our study is that the curve that defines the minimal CPU power as a function of the load is neither linear nor purely convex as has been previously assumed. Moreover, we find that the efficiency of the various server components can be maximized by tuning the CPU frequency and the number of active cores as a function of the system and network load, while the block size of I/O operations should be always maximized by applications. We also show how to estimate the energy consumed by an application as a function of some simple parameters, like the CPU load, and the disk and network activity. We validate the proposed approach by accurately estimating the energy of a map-reduce computation in a Hadoop platform.},
  isbn = {978-1-4503-2819-7}
}
@article{basmadjianCloudComputingIts2012,
  title = {Cloud Computing and Its Interest in Saving Energy: The Use Case of a Private Cloud},
  shorttitle = {Cloud Computing and Its Interest in Saving Energy},
  author = {Basmadjian, Robert and Meer, Hermann De and Lent, Ricardo and Giuliani, Giovanni},
  year = {2012},
  month = jun,
  journal = {Journal of Cloud Computing: Advances, Systems and Applications},
  volume = {1},
  number = {1},
  pages = {5},
  issn = {2192-113X},
  doi = {10.1186/2192-113X-1-5},
  urldate = {2025-06-01},
  abstract = {Cloud computing data centres, due to their housing of powerful ICT equipment, are high energy consumers and therefore accountable for large quantities of emissions. Therefore, energy saving strategies applicable to such data centres are a very promising research direction both from the economical and environmental stand point.},
  langid = {english},
  keywords = {Cloud Computing,Energy Conservation,Energy Ethics,Energy Informatics,Energy Management,Energy Security,IT resources,Modelling,Power and energy consumption,Private cloud computing data centre}
}
@inproceedings{arjonaarocaMeasurementbasedAnalysisEnergy2014,
  title = {A Measurement-Based Analysis of the Energy Consumption of Data Center Servers},
  booktitle = {Proceedings of the 5th International Conference on {{Future}} Energy Systems},
  author = {Arjona Aroca, Jordi and Chatzipapas, Angelos and Fern{\'a}ndez Anta, Antonio and Mancuso, Vincenzo},
  year = {2014},
  month = jun,
  series = {E-{{Energy}} '14},
  pages = {63--74},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2602044.2602061},
  urldate = {2025-06-01},
  abstract = {Energy consumption is a growing issue in data centers, impacting their economic viability and their public image. In this work we empirically characterize the power and energy consumed by different types of servers. In particular, in order to understand the behavior of their energy and power consumption, we perform measurements in different servers. In each of them, we exhaustively measure the power consumed by the CPU, the disk, and the network interface under different configurations, identifying the optimal operational levels. One interesting conclusion of our study is that the curve that defines the minimal CPU power as a function of the load is neither linear nor purely convex as has been previously assumed. Moreover, we find that the efficiency of the various server components can be maximized by tuning the CPU frequency and the number of active cores as a function of the system and network load, while the block size of I/O operations should be always maximized by applications. We also show how to estimate the energy consumed by an application as a function of some simple parameters, like the CPU load, and the disk and network activity. We validate the proposed approach by accurately estimating the energy of a map-reduce computation in a Hadoop platform.}
}
@inproceedings{baneshiAnalyzingPerApplicationEnergy2024,
  title = {Analyzing {{Per-Application Energy Consumption}} in a {{Multi-Application Computing Continuum}}},
  booktitle = {2024 9th {{International Conference}} on {{Fog}} and {{Mobile Edge Computing}} ({{FMEC}})},
  author = {Baneshi, Saeedeh and Pathania, Anuj and Akesson, Benny and Pimentel, Andy and Varbanescu, Ana-Lucia},
  year = {2024},
  month = sep,
  pages = {30--37},
  doi = {10.1109/FMEC62297.2024.10710253},
  urldate = {2025-05-30},
  abstract = {In today's digital society, diverse computing de-vices-from edge devices to data centers-support various applications, each with specific performance and energy characteristics. Analyzing application energy consumption is crucial for improving energy efficiency, optimizing resources, and reducing environmental impact. However, while comprehensive energy measurements are feasible for specific configurations, they are impractical for assessing diverse application mappings. Still, stakeholders such as cloud providers, developers, users, and researchers need insights into application-level energy behavior for informed decision-making. In this work, we propose a fine-grained simulation approach for analyzing application energy behavior in edge/cloud environments. We implemented our approach as an enhanced version of the iFogSim framework. We demonstrate its effectiveness by evaluating different multi-application scenarios and configurations for a video surveillance application. Our approach facilitates the fast evaluation of different scenarios and deployment strategies, providing insights that can contribute to more energy-efficient edge/cloud computing systems and digital services.},
  keywords = {Application Energy Consumption,Application Mapping,Computing Continuum,Decision making,Edge Computing,Energy consumption,Energy efficiency,Energy measurement,Multi-access edge computing,Performance evaluation,Scenario analysis,Simulation,Stakeholders,Video surveillance}
}
@article{demaioModellingEnergyConsumption2016,
  title = {Modelling Energy Consumption of Network Transfers and Virtual Machine Migration},
  author = {De Maio, Vincenzo and Prodan, Radu and Benedict, Shajulin and Kecskemeti, Gabor},
  year = {2016},
  month = mar,
  journal = {Future Generation Computer Systems},
  volume = {56},
  pages = {388--406},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.07.007},
  urldate = {2025-06-04},
  abstract = {Reducing energy consumption has become a key issue for data centres, not only because of economical benefits but also for environmental and marketing reasons. Therefore, assessing their energy consumption requires precise models. In the past years, many models targeting different hardware components, such as CPU, storage and network interface cards (NIC) have been proposed. However, most of them neglect energy consumption related to VM migration. Since VM migration is a network-intensive process, to accurately model its energy consumption we also need energy models for network transfers, comprising their complete software stacks with different energy characteristics. In this work, we present a comparative analysis of the energy consumption of the software stack of two of today's most used NICs in data centres, Ethernet and Infiniband. We carefully design for this purpose a set of benchmark experiments to assess the impact of different traffic patterns and interface settings on energy consumption. Using our benchmark results, we derive an energy consumption model for network transfers. Based on this model, we propose an energy consumption model for VM migration providing accurate predictions for paravirtualised VMs running on homogeneous hosts. We present a comprehensive analysis of our model on different machine sets and compare it with other models for energy consumption of VM migration, showing an improvement of up to 24\% in accuracy, according to the NRMSE error metric.},
  keywords = {Benchmarking,Energy consumption,Network interface card,Virtual machine migration}
}
@inproceedings{dargieProbabilisticModelEstimating2013,
  title = {A {{Probabilistic Model}} for {{Estimating}} the {{Power Consumption}} of {{Processors}} and {{Network Interface Cards}}},
  booktitle = {2013 12th {{IEEE International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}}},
  author = {Dargie, Waltenegus and Wen, Jianjun},
  year = {2013},
  month = jul,
  pages = {845--852},
  issn = {2324-9013},
  doi = {10.1109/TrustCom.2013.103},
  urldate = {2025-06-04},
  abstract = {Many of the proposed mechanisms aiming to achieve energy-aware adaptations in server environments rely on the existence of models that estimate the power consumption of the server as well as its individual components. Most existing or proposed models employ performance (hardware) monitoring counters and the CPU utilization to estimate power consumption, but they do not take into account the statistics of the workload the server processes. In this paper we propose a lightweight probabilistic model that can be used to estimate the power consumption of the CPU, the network interface card (NIC), and the server as a whole. We tested the model's accuracy by executing custom-made benchmarks as well as standard benchmarks on two heterogeneous server platforms. The estimation error associated with our model is less than 1\% for the custom-made benchmark whereas it is less than 12\% for the standard benchmark.},
  keywords = {Correlation,Hardware,Network interfaces,Power consumption model: probabilistic model: server power consumption: processor power consumption: NIC power consumption: cumulative distribution function: random variable,Power demand,Program processors,Radiation detectors,Servers}
}
@online{technotes_pci_power_2024,
  author       = {{TechNotes}},
  title        = {{Deciphering the PCI Power States}},
  year         = {2024},
  month        = feb,
  url          = {https://technotes.blog/2024/02/04/deciphering-the-pci-power-states/},
  note         = {Accessed June 2025}
}
@misc{uefi_acpi_6_6,
  author       = {{UEFI Forum}},
  title        = {{Advanced Configuration and Power Interface Specification Version 6.6}},
  year         = {2021},
  month        = sep,
  url          = {https://uefi.org/sites/default/files/resources/ACPI_Spec_6.6.pdf},
  note         = {Accessed April 2025}
}
@article{katalEnergyEfficiencyCloud2022,
  title = {Energy Efficiency in Cloud Computing Data Center: A Survey on Hardware Technologies},
  shorttitle = {Energy Efficiency in Cloud Computing Data Center},
  author = {Katal, Avita and Dahiya, Susheela and Choudhury, Tanupriya},
  year = {2022},
  month = feb,
  journal = {Cluster Computing},
  volume = {25},
  number = {1},
  pages = {675--705},
  issn = {1573-7543},
  doi = {10.1007/s10586-021-03431-z},
  urldate = {2025-06-04},
  abstract = {The internet is expanding its viewpoint into each conceivable part of the cutting-edge economy. Unshackled from our web programs today, the internet is characterizing our way of life, regardless of whether it's sitting in front of the TV or driving an independent auto. The enchantment of the internet appears to be relatively unbounded. In any case, with each new spell there comes an ever-increasing amount of data, and interest for computational power. Cloud computing which is an on-request conveyance of computing power, applications, database storage, and other IT assets by means of the Internet has violently expanded our computerized lives. Though, there have been critical improvements as far as accessibility, fluctuation, time and quality in administrations are concerned; the unbounded development of our computerized way of life requires monstrous measures of power, especially for the data centers that fill in as the mind of the advanced economy. Data organizations foresee a decrease in the quantity of data centers, as more businesses close their little data centers and move towards cloud computing. All things considered, the move by clients towards cloud, will increase the general energy utilization significantly, exceeding any energy productivity increase; which has recorded for over 70\% of data center development in 2018. Many research advancements are already made in this domain for minimizing the energy utilization of the computing types of gear included; for efficient power energy consumption, decrease of carbon impression and e-squander. These procedures are supporters of green cloud computing, which are focused on planning and advancing energy-proficient activities to contain inordinate energy utilization in data centers. This paper discusses different mechanisms for lowering the power utilization in data centers. It provides in depth detail about the various mechanisms that can be employed at the hardware component level so that the utilization of energy by component can be reduced. Techniques that can be applied at network, cluster of servers' level along with the various dynamic power management measures that can be employed at the hardware or firmware level and can lead to energy efficient or green data centers are also studied in detail. The paper concludes with the research challenges for building the green data centers.},
  langid = {english}
}
@inproceedings{elnozahyEnergyEfficientServerClusters2003,
  title = {Energy-{{Efficient Server Clusters}}},
  booktitle = {Power-{{Aware Computer Systems}}},
  author = {Elnozahy, E. N. (Mootaz) and Kistler, Michael and Rajamony, Ramakrishnan},
  editor = {Falsafi, Babak and Vijaykumar, T. N.},
  year = {2003},
  pages = {179--197},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-36612-1_12},
  abstract = {This paper evaluates five policies for cluster-wide power management in server farms. The policies employ various combinations of dynamic voltage scaling and node vary-on/vary-off (VOVO) to reduce the aggregate power consumption of a server cluster during periods of reduced workload. We evaluate the policies using a validated simulator that calculates the energy usage and response times of a Web server cluster serving traces culled from real-life Web server workloads.},
  isbn = {978-3-540-36612-6},
  langid = {english}
}
@incollection{beloglazovChapter3Taxonomy2011,
  title = {Chapter 3 - {{A Taxonomy}} and {{Survey}} of {{Energy-Efficient Data Centers}} and {{Cloud Computing Systems}}},
  booktitle = {Advances in {{Computers}}},
  author = {Beloglazov, Anton and Buyya, Rajkumar and Lee, Young Choon and Zomaya, Albert},
  editor = {Zelkowitz, Marvin V.},
  year = {2011},
  month = jan,
  volume = {82},
  pages = {47--111},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-385512-1.00003-7},
  urldate = {2025-06-09},
  abstract = {Traditionally, the development of computing systems has been focused on performance improvements driven by the demand of applications from consumer, scientific, and business domains. However, the ever-increasing energy consumption of computing systems has started to limit further performance growth due to overwhelming electricity bills and carbon dioxide footprints. Therefore, the goal of the computer system design has been shifted to power and energy efficiency. To identify open challenges in the area and facilitate future advancements, it is essential to synthesize and classify the research on power- and energy-efficient design conducted to date. In this study, we discuss causes and problems of high power/energy consumption, and present a taxonomy of energy-efficient design of computing systems covering the hardware, operating system, virtualization, and data center levels. We survey various key works in the area and map them onto our taxonomy to guide future design and development efforts. This chapter concludes with a discussion on advancements identified in energy-efficient computing and our vision for future research directions.}
}
@inproceedings{basmadjianMethodologyPredictPower2011,
  title = {A Methodology to Predict the Power Consumption of Servers in Data Centres},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Energy-Efficient Computing}} and {{Networking}}},
  author = {Basmadjian, Robert and Ali, Nasir and Niedermeier, Florian and {de Meer}, Hermann and Giuliani, Giovanni},
  year = {2011},
  month = may,
  series = {E-{{Energy}} '11},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2318716.2318718},
  urldate = {2025-06-09},
  abstract = {Until recently, there have been relatively few studies exploring the power consumption of ICT resources in data centres. In this paper, we propose a methodology to capture the behaviour of most relevant energy-related ICT resources in data centres and present a generic model for them. This is achieved by decomposing the design process into four modelling phases. Furthermore, unlike the state-of-the-art approaches, we provide detailed power consumption models at server and storage levels. We evaluate our model for different types of servers and show that it suffers from an error rate of 2\% in the best case, and less than 10\% in the worst case.},
  isbn = {978-1-4503-1313-1}
}
@article{LuoEnergyModeling2014,
author = {Luo, L. and Wu, W.-J and Zhang, F.},
year = {2014},
month = {07},
pages = {1371-1387},
title = {Energy modeling based on cloud data center},
volume = {25},
journal = {Ruan Jian Xue Bao/Journal of Software},
doi = {10.13328/j.cnki.jos.004604}
}
@inproceedings{basmadjian2012evaluating,
  title={Evaluating and modeling power consumption of multi-core processors},
  author={Basmadjian, Robert and De Meer, Hermann},
  booktitle={Proceedings of the 3rd International Conference on Future Energy Systems: Where Energy, Computing and Communication Meet},
  pages={1--10},
  year={2012}
}
@inproceedings{sarood2014maximizing,
  title={Maximizing throughput of overprovisioned hpc data centers under a strict power budget},
  author={Sarood, Osman and Langer, Akhil and Gupta, Abhishek and Kale, Laxmikant},
  booktitle={SC'14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={807--818},
  year={2014},
  organization={IEEE}
}
@inproceedings{kansal2010virtual,
  title={Virtual machine power metering and provisioning},
  author={Kansal, Aman and Zhao, Feng and Liu, Jie and Kothari, Nupur and Bhattacharya, Arka A},
  booktitle={Proceedings of the 1st ACM symposium on Cloud computing},
  pages={39--50},
  year={2010}
}
@article{lin2018cloud,
  title={A cloud server energy consumption measurement system for heterogeneous cloud environments},
  author={Lin, Weiwei and Wang, Haoyu and Zhang, Yufeng and Qi, Deyu and Wang, James Z and Chang, Victor},
  journal={Information Sciences},
  volume={468},
  pages={47--62},
  year={2018},
  publisher={Elsevier}
}
@article{arroba2014server,
  title={Server power modeling for run-time energy optimization of cloud computing facilities},
  author={Arroba, Patricia and Risco-Mart{\'\i}n, Jos{\'e} L and Zapater, Marina and Moya, Jos{\'e} M and Ayala, Jos{\'e} L and Olcoz, Katzalin},
  journal={Energy Procedia},
  volume={62},
  pages={401--410},
  year={2014},
  publisher={Elsevier}
}
@misc{kernelprocfs,
  author       = {{The Linux Kernel Community}},
  title        = {The proc Filesystem},
  howpublished = {\url{https://www.kernel.org/doc/html/latest/filesystems/proc.html}},
  note         = {Accessed: 2025-06-17},
  year         = {2025}
}
@misc{kernelcgroupv2,
  author       = {{The Linux Kernel Community}},
  title        = {Control Group v2 — Linux Kernel Documentation},
  howpublished = {\url{https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html}},
  note         = {Accessed: 2025-06-17},
  year         = {2025}
}
@misc{kernelcgroupv1,
  author       = {{The Linux Kernel Community}},
  title        = {Control Group v1 — Linux Kernel Documentation},
  howpublished = {\url{https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html}},
  note         = {Accessed: 2025-06-17},
  year         = {2025}
}
@misc{ciliumbpf,
  author       = {{Cilium Authors}},
  title        = {eBPF and XDP Reference Guide},
  howpublished = {\url{https://docs.cilium.io/en/latest/reference-guides/bpf/index.html}},
  note         = {Accessed: 2025-06-17},
  year         = {2025}
}
@misc{cadvisorgithub,
  author       = {Google Inc.},
  title        = {cAdvisor: Analyzes Resource Usage and Performance Characteristics of Running Containers},
  howpublished = {\url{https://github.com/google/cadvisor}},
  note         = {Accessed: 2025-06-14},
  year         = {2025}
}
@misc{metricsservergithub,
  author       = {Kubernetes-SIG},
  title        = {metrics-server: Scalable and efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines},
  howpublished = {\url{https://github.com/kubernetes-sigs/metrics-server}},
  note         = {Accessed: 2025-06-17},
  year         = {2025}
}
@inproceedings{cassagnesRiseEBPFNonintrusive2020,
  title = {The Rise of {{eBPF}} for Non-Intrusive Performance Monitoring},
  booktitle = {{{NOMS}} 2020 - 2020 {{IEEE}}/{{IFIP Network Operations}} and {{Management Symposium}}},
  author = {Cassagnes, Cyril and Trestioreanu, Lucian and Joly, Clement and State, Radu},
  year = {2020},
  month = apr,
  pages = {1--7},
  issn = {2374-9709},
  doi = {10.1109/NOMS47738.2020.9110434},
  urldate = {2025-06-14},
  abstract = {In this paper, we explain that container engines are strengthening their isolation mechanisms. Therefore, non-intrusive monitoring becomes a must-have for the performance analysis of containerized user-space application in production environments. After a literature review and background of Linux subsystems and container isolation concepts, we present our lessons learned of using the extended Berkeley packet filter to monitor and profile performance. We carry out the profiling and tracing of several Interledger connectors using two full-fledged implementations of the Interledger protocol specifications.},
  keywords = {Cloud computing,Connectors,Containers,eBPF,Interledger,Linux,Performance,Performance analysis,Production,Profiling,Protocols,Software,Tracing}
}
@book{harchol2013performance,
  title     = {Performance Modeling and Design of Computer Systems: Queueing Theory in Action},
  author    = {Mor Harchol-Balter},
  year      = {2013},
  publisher = {Cambridge University Press},
  isbn      = {9781107027503},
  address   = {New York, NY, USA}
}
@software{codecarbon_2024,
  author       = {CodeCarbon Team},
  title        = {mlco2/codecarbon: v2.4.1},
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v2.4.1},
  doi          = {10.5281/zenodo.11171501},
  url          = {https://doi.org/10.5281/zenodo.11171501}
}
@misc{codecarbon_issue_322,
  author       = {CodeCarbon Contributors},
  title        = {CodeCarbon issue \#322: API endpoint and swagger docs},
  year         = {2022},
  howpublished = {\url{https://github.com/mlco2/codecarbon/issues/322}},
  note         = {Accessed: 2025-06-21}
}
@misc{powerapi2024github,
  author       = {Fieni, Pierre and others},
  title        = {PowerAPI is a green-computing toolbox to measure, analyze, and optimize the energy consumption of the various hardware/software levels composing an infrastructure.},
  year         = {2024},
  howpublished = {\url{https://github.com/powerapi-ng}},
  note         = {Accessed June 2025}
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@misc{scaphandre_github,
  author       = {Hubblo},
  title        = {Scaphandre: Energy consumption monitoring agent},
  year         = {2025},
  howpublished = {\url{https://github.com/hubblo-org/scaphandre}},
  note         = {Accessed: 2025-06-24}
}
@misc{scaphandre_issue280,
  author       = {Hubblo},
  title        = {Overflow in energy counter can lead to wrong power measurements},
  howpublished = {\url{https://github.com/hubblo-org/scaphandre/issues/280}},
  note         = {GitHub issue \#280, \emph{hubblo-org/scaphandre}},
  year         = {2024},
  month        = {February},
  urldate      = {2025-06-25}
}
@article{strempelMeasuringEnergyConsumption,
  title = {Measuring the {{Energy Consumption}} of {{Software}} Written in {{C}} on X86-64 {{Processors}}},
  author = {Strempel, Tom},
  langid = {english},
  year = {2021}
}
@inproceedings{andringaUnderstandingEnergyConsumption2025,
  title = {Understanding the {{Energy Consumption}} of {{Cloud-native Software Systems}}},
  booktitle = {Proceedings of the 16th {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Andringa, Lars and Setz, Brian and Andrikopoulos, Vasilios},
  year = {2025},
  month = may,
  pages = {309--319},
  publisher = {ACM},
  address = {Toronto ON Canada},
  doi = {10.1145/3676151.3719371},
  urldate = {2025-06-29},
  abstract = {As the dependence on software systems running on cloud data centers grows on a daily basis, there is an increasingly stronger motivation to reduce their energy consumption. A necessary but not trivial step in this direction is understanding how energy is consumed in virtualized, multi-tenant environments such as the one provisioned in the cloud. Prior work focuses on isolated, nonvirtualized systems and is difficult to transfer to this context. A number of industry-led approaches have appeared in the meantime in terms of tools and technological stacks building on the concept of observability as the means to achieve this goal. This paper discusses our approach in adopting one such stack and consequently assessing it for fitness to purpose through an experimental procedure. To this effect, we deploy a cloud-native application on a private cloud infrastructure instrumented for measuring energy consumption through a combination of hardware and software means. We combine the information from these instrumentation points into a mapping model to deal with the different virtualization layers and compare the model against the values reported by the observability stack. Furthermore, we use our model to attribute energy consumption across the virtualization layers and understand how energy is consumed at each one.},
  isbn = {979-8-4007-1073-5},
  langid = {english}
}
@inproceedings{cockcroft2006utilization,
  title={Utilization is virtually useless as a metric!},
  author={Cockcroft, Adrian},
  booktitle={Int. CMG Conference},
  pages={557--562},
  year={2006}
}
@misc{Gregg2017CpuUtilizationWrong,
  author       = {Brendan Gregg},
  title        = {CPU Utilization is Wrong},
  howpublished = {Blog post},
  year         = {2017},
  month        = may,
  day          = {9},
  url          = {https://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html},
  note         = {Accessed 29 June 2025},
}
@misc{Tarara2023CpuUtilization,
  author       = {Arne Tarara},
  title        = {CPU Utilization – A Useful Metric?},
  howpublished = {Green Coding Case Study},
  year         = {2023},
  month        = jun,
  day          = {26},
  url          = {https://www.green-coding.io/case-studies/cpu-utilization-usefulness/},
  note         = {Accessed 29 June 2025},
}
@mastersthesis{pijnackerEstimatingContainerlevelPower2024,
  title = {Estimating {{Container-level}} Power Usage in {{Kubernetes}}},
  author = {Pijnacker, Bjorn},
  year = {2024},
  month = nov,
  urldate = {2025-03-17},
  abstract = {Energy efficiency in cloud computing, and specifically Kubernetes, has been a major topic of research in the past years as cloud datacenters grow and the importance of minimizing carbon output increases. Much of this research focuses on the total energy usage of a Kubernetes cluster and attempts to optimize this by various methods such as energy-aware scheduling and datacenter- or cluster-wide metrics, without regard for individual workloads. A lesser explored aspect which can provide useful insights into Kubernetes power usage is a measure of power usage at the Kubernetes container level; thereby providing insight into the power usage for each workload in a cluster. In this research project, we experimentally evaluate the state-of-the-art Kubernetes power estimation tooling: the Cloud-Native Computing Foundation's Kepler. This tool is evaluated on datacenter-grade hardware where its total Kubernetes node measurements and its container power attribution for each of the available configurations and available external sources is considered. We find that this tool does not produce satisfactory power usage metrics in regard to container power attribution and that there are significant limitations in several of the available node power measurement strategies. To combat the limitations in Kepler, we create our own tool named KubeWatt based on a recently introduced power mapping model. The architecture and implementation of KubeWatt are discussed along with an experimental validation of its features and measurements. During this work, several limitations as well as some interesting findings are discussed which may provide avenues for future research.},
  langid = {english},
  school = {University of Groningen},
  keywords = {notion},
  file = {/home/caspar/Zotero/storage/SRWDQPT5/Pijnacker - 2024 - Estimating Container-level power usage in Kubernetes.pdf}
}
@misc{pijnackerContainerlevelEnergyObservability2025,
  title = {Container-Level {{Energy Observability}} in {{Kubernetes Clusters}}},
  author = {Pijnacker, Bjorn and Setz, Brian and Andrikopoulos, Vasilios},
  year = {2025},
  month = apr,
  number = {arXiv:2504.10702},
  eprint = {2504.10702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.10702},
  urldate = {2025-07-02},
  abstract = {Kubernetes has been for a number of years the default cloud orchestrator solution across multiple application and research domains. As such, optimizing the energy efficiency of Kubernetes-deployed workloads is of primary interest towards controlling operational expenses by reducing energy consumption at data center level and allocated resources at application level. A lot of research in this direction aims on reducing the total energy usage of Kubernetes clusters without establishing an understanding of their workloads, i.e. the applications deployed on the cluster. This means that there are untapped potential improvements in energy efficiency that can be achieved through, for example, application refactoring or deployment optimization. For all these cases a prerequisite is establishing fine-grained observability down to the level of individual containers and their power draw over time. A state-of-the-art tool approved by the Cloud-Native Computing Foundation, Kepler, aims to provide this functionality, but has not been assessed for its accuracy and therefore fitness for purpose. In this work we start by developing an experimental procedure to this goal, and we conclude that the reported energy usage metrics provided by Kepler are not at a satisfactory level. As a reaction to this, we develop KubeWatt as an alternative to Kepler for specific use case scenarios, and demonstrate its higher accuracy through the same experimental procedure as we used for Kepler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Software Engineering},
  file = {/home/caspar/Zotero/storage/TCW4AIGW/Pijnacker et al. - 2025 - Container-level Energy Observability in Kubernetes Clusters.pdf;/home/caspar/Zotero/storage/6ZZZZIVQ/2504.html}
}
@inproceedings{amaralKeplerFrameworkCalculate2023,
  title = {Kepler: {{A Framework}} to {{Calculate}} the {{Energy Consumption}} of {{Containerized Applications}}},
  shorttitle = {Kepler},
  booktitle = {2023 {{IEEE}} 16th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Amaral, Marcelo and Chen, Huamin and Chiba, Tatsuhiro and Nakazawa, Rina and Choochotkaew, Sunyanan and Lee, Eun Kyung and Eilam, Tamar},
  year = {2023},
  month = jul,
  pages = {69--71},
  issn = {2159-6190},
  doi = {10.1109/CLOUD60044.2023.00017},
  urldate = {2025-03-10},
  abstract = {Energy accounting is crucial in data centers for optimizing power provisioning, capping, and tuning. This paper introduces the Kepler framework, which estimates power consumption at the process, container, and Kubernetes pod levels. Kepler offers a set of power models applicable to various architectures and metrics. In this study, we propose a generic power model that utilizes hardware counters (HC) and realtime system power metrics (e.g., running average power limit (RAPL)) as independent variables in a regression model. Unlike previous approaches that rely on aggregate power consumption, our methodology measures individual process power consumption to train the power model. We provide step-by-step instructions to measure process power consumption in a controlled environment, considering the activation constant and load-dependent dynamic power consumption in different executions. By following the Greenhouse Gas (GHG) Protocol, our approach ensures the fair distribution of constant power among the user's processes. The results demonstrate significantly improved accuracy with a mean squared error (MSE) as low as 0.010 for the proposed method, compared with an MSE of 0.16 for a simple ratio approach and 0.92 when training the model using aggregated workload power.},
  keywords = {Cloud computing,Computational modeling,container power modeling,eBPF,energy accounting,kubernetes,Measurement,notion,Power demand,Power measurement,Protocols,RAPL,Sustainability,Training},
  file = {/home/caspar/Zotero/storage/FRUJVEH6/Amaral et al. - 2023 - Kepler A Framework to Calculate the Energy Consumption of Containerized Applications.pdf;/home/caspar/Zotero/storage/9H9MAU83/10254956.html}
}
@inproceedings{choochotkaewAdvancingCloudSustainability2023,
  title = {Advancing {{Cloud Sustainability}}: {{A Versatile Framework}} for {{Container Power Model Training}}},
  shorttitle = {Advancing {{Cloud Sustainability}}},
  booktitle = {2023 31st {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Choochotkaew, Sunyanan and Wang, Chen and Chen, Huamin and Chiba, Tatsuhiro and Amaral, Marcelo and Lee, Eun Kyung and Eilam, Tamar},
  year = {2023},
  month = oct,
  pages = {1--4},
  issn = {2375-0227},
  doi = {10.1109/MASCOTS59514.2023.10387542},
  urldate = {2025-07-02},
  abstract = {Estimating power consumption in modern Cloud is important to account for the power consumed by each container. The challenge is that multiple customers are sharing the same hardware platform, where physical information is mostly obscured. In addition, there is the overhead in power consumption that the Cloud control plane induces. This paper addresses these challenges and introduces a pipeline framework for container power model training on the basis of available performance counters and other metrics. The proposed model utilizes machine learning techniques to predict the power consumed by the control plane and associated processes when running together with the user containers, and uses it for isolating the dynamic power consumed by the user-inducing workload. Applying the proposed power model does not require online power measurements, nor does it need machine information, or information on other tenants sharing the same machine. The results of cross-workload, cross-platform experiments demonstrated the higher accuracy of the model when predicting power consumption of unseen containers on unknown platforms, including on virtual machines.},
  keywords = {Analytical models,Cloud,Cloud computing,Computational modeling,Containers,Green computing,Machine learning,Measurement,Power demand,Power measurement,Power model,Training},
  file = {/home/caspar/Zotero/storage/DYKNJMS3/Choochotkaew et al. - 2023 - Advancing Cloud Sustainability A Versatile Framework for Container Power Model Training.pdf}
}
@misc{gesi2024ictguidance,
  author       = "{Global e‑Sustainability Initiative (GeSI) and Carbon Trust}",
  title        = "{ICT Guidance on GHG Protocol Product Life Cycle Accounting and Reporting Standard}",
  howpublished = "\url{https://www.gesi.org/public-resources/ict-guidance-on-ghg-protocol-product-life-cycle-accounting-and-reporting-standard/}",
  year         = 2024,
  month        = nov,
  note         = "Accessed 2 Jul 2025",
}
@misc{amaral2023exploring,
  author       = {Marcelo Amaral and Sunyanan Choochotkaew and Eun Kyung Lee and Huamin Chen and Tamar Eilam},
  title        = {Exploring Kepler’s Potentials: Unveiling Cloud Application Power Consumption},
  year         = {2023},
  month        = {October},
  day          = {11},
  url          = {https://www.cncf.io/blog/2023/10/11/exploring-keplers-potentials-unveiling-cloud-application-power-consumption/},
  note         = {CNCF Blog},
  howpublished = {\url{https://www.cncf.io/blog/2023/10/11/exploring-keplers-potentials-unveiling-cloud-application-power-consumption/}}
}
@misc{kepler_docs,
  title        = {Kepler: Kubernetes Efficient Power Level Exporter Documentation},
  author       = {{Sustainable Computing}},
  howpublished = {\url{https://sustainable-computing.io/}},
  note         = {Accessed: 2025-07-08},
  year         = {2025}
}