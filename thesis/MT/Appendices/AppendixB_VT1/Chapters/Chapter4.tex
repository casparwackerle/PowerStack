% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

\section{Test Procedure} % Main chapter title
\label{Chapter4}

This chapter describes the test procedure used to verify KEPLER-produced metrics. The verification process involves executing dynamic workloads on a Kubernetes cluster and analyzing the correlation between the workload and the KEPLER-reported metrics. For better intuitive understanding, all Joule-based metrics are converted to Watts, and all operations-based metrics are converted to IOPS.

The collected data is visualized through diagrams for easier interpretation. However, this thesis does not provide a detailed energy efficiency analysis. Instead, the goal is to verify whether KEPLER metrics reliably correlate with workload fluctuations, ensuring its suitability for a more in-depth energy efficiency study in future work.

\section{Test Setup}
All test workloads were Ansible-created within a dedicated Kubernetes namespace, referred to as the \textit{testing-namespace}. While the cluster itself was designed to be largely hardware-independent, the test setup requires manual adjustments when deployed on different hardware. Specifically, CPU and memory allocations for test pods should be reviewed, and the storage disk used for disk I/O experiments must be empty and correctly identified.

\subsection{Benchmarking Pod}
A dedicated Ubuntu-based \textit{benchmarking pod} was provisioned (using Ansible) to serve as the central test agent for all experiments. This pod would allow completely self-contained testing in the cluster, without external traffic or - more importantly - an external machine available for longer tests. The benchmarking pod is configured with a fully functional \texttt{kubectl} setup, an OpenSSH client for authentication, and essential tools such as \texttt{wget}, \texttt{curl}, \texttt{vim} and \texttt{git}.

\subsection{Testing Pods}
Test workloads were deployed as DaemonSets to ensure that every node in the cluster hosted the required test pods. Depending on the experiment, different ressource allocations were made:
\begin{itemize}
    \item For the CPU stress testing, a test and a load pod were deployed with 2.5 vCPU, 1 GB memory each.
    \item For the Memory stress testing, a test and a load pod were deployed with 150m vCPU, 25 GB memory each.
    \item For Network I/O and disk I/O stress testing, a test pod was deployed with 2.5 vCPU, 20 GB memory.
\end{itemize}
The resources were allocated to ensure an even split between CPU \textit{testing} and \textit{background load pods}, while memory was similarly distributed between memory-intensive workloads. A resource margin was maintained to prevent system instability. Benchmarking tools were installed on each pod, including \texttt{stress-ng}\parencite{stress-ng} for CPU and memory stress tests, \texttt{fio}\parencite{fio} for disk I/O testing, and \texttt{iperf3}\parencite{iperf3} for network performance measurement .

\subsection{Disk Formatting and Mounting}
For disk I/O experiments, an unused HDD on each worker node was partitioned, formatted, and mounted using Ansible. To ensure persistence, the mounting process was configured with an entry in \texttt{/etc/fstab}.

\section{Test Procedure}
Since energy consumption is not calculated beyond the node level, all tests were conducted on a single worker node. The test pod (either high-CPU or high-memory) generated workloads at predefined levels of 10\%, 30\%, 50\%, 70\%, and 90\% for a fixed duration of 30 minutes per workload level. For CPU and meory testing, Tests were executed under two conditions: an idle cluster and a busy cluster, where the busy cluster was simulated by running background load pods at 90\% utilization. Since disk and network usage are not by default restricted in Kubernetes, this distinction as not made for Disk I/O and Network I/O tests.

\subsection{CPU Stress Test}
CPU-intensive workloads were generated using \texttt{stress-ng}, with a CPU worker initiated on each available core. This test was executed under both idle and busy cluster conditions.

\subsection{Memory Stress Test}
Memory-intensive workloads were generated using \texttt{stress-ng}, where a single virtual memory worker allocated all available memory. The test was executed under both idle and busy cluster conditions.

\subsection{Disk I/O Stress Test}
Disk performance was assessed using \texttt{fio}. The experiment consisted of two phases: measuring maximum IOPS through read operations on the mounted HDD, followed by read operations at predefined percentages of the maximum IOPS. To eliminate caching effects, random read operations were used exclusively, and direct I/O was enabled.

\subsection{Network I/O Stress Test}
Network performance was evaluated using \texttt{iperf3}. First, the maximum bandwidth between pods on different nodes was measured. Next, controlled tests were conducted at various percentages of the maximum bandwidth. To mitigate server-side overhead, only client-side results were analyzed.

\section{Data Analysis}
Data collected from each experiment was analyzed in two main steps using Python.

\subsection{Data Querying}
Prometheus was queried to extract KEPLER metrics for the duration of each experiment. The retrieved data was formatted and stored as CSV files. The analysis relied on the python libraries \texttt{pandas}, \texttt{requests} and \texttt{datetime} for data querying and processing.

\subsection{Diagrams}
Visualization of KEPLER metric data was performed using \texttt{matplotlib}. Each diagram featured:
\begin{itemize}
\item X-axis: Time
\item Primary Y-axis: KEPLER metric values (Watts or operations per second)
\item Secondary Y-axis: Test workload percentage
\item A moving average overlay to improve readability
\end{itemize}
By correlating workload levels with KEPLER metrics, the structured analysis validated the suitability of KEPLER for future energy efficiency studies.


