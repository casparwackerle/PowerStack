% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex
\chapter{Test Procedure}
\label{vt1_Chapter4}

This chapter describes the test procedure used to verify Kepler-produced metrics. The verification process involves executing dynamic workloads on the Kubernetes cluster and analyzing the correlation between workload intensity and the metrics reported by Kepler. For better intuitive understanding, all Joule-based metrics are converted to Watts, and all operations-based metrics are converted to IOPS.

The collected data is visualized through diagrams to support interpretation. However, this thesis does not provide a detailed energy-efficiency analysis. Instead, the goal is to verify whether Kepler metrics reliably correlate with workload fluctuations, thereby confirming its suitability for a more in-depth energy-efficiency study in future work.

\section{Test Setup}

All test workloads were created using Ansible within a dedicated Kubernetes namespace, referred to as the \textit{testing-namespace}. While the cluster was designed to be largely hardware-independent, the test setup requires manual adjustments when deployed on different hardware. Specifically, CPU and memory allocations for test pods must be reviewed, and the storage disk used for Disk I/O experiments must be empty and correctly identified.

\subsection{Benchmarking Pod}

A dedicated Ubuntu-based \textit{benchmarking pod} was provisioned (using Ansible) to serve as the central test agent for all experiments. This pod enabled fully self-contained testing inside the cluster, without any dependency on external machines. The benchmarking pod was configured with a complete \texttt{kubectl} setup, an OpenSSH client, and essential tools such as \texttt{wget}, \texttt{curl}, \texttt{vim}, and \texttt{git}.

\subsection{Testing Pods}

Test workloads were deployed as DaemonSets to ensure that every node in the cluster hosted the required test pods. Depending on the experiment, different resource allocations were used:

\begin{itemize}
    \item \textbf{CPU stress testing:} A test pod and a background load pod were deployed, each with 2.5 vCPU and 1~GB memory.
    \item \textbf{Memory stress testing:} A test pod and a background load pod were deployed, each with 150m vCPU and 25~GB memory.
    \item \textbf{Network I/O and Disk I/O testing:} A single test pod was deployed with 2.5 vCPU and 20~GB memory.
\end{itemize}

Resources were allocated to ensure an even split between CPU \textit{testing} and \textit{background load} pods, and a similar balance for memory-intensive workloads. A resource margin was maintained to prevent system instability. Benchmarking tools were installed on each pod, including \texttt{stress-ng}\parencite{stress-ng} for CPU and memory stress tests, \texttt{fio}\parencite{fio} for Disk I/O testing, and \texttt{iperf3}\parencite{iperf3} for network performance measurements.

\subsection{Disk Formatting and Mounting}

For Disk I/O experiments, an unused HDD on each worker node was partitioned, formatted, and mounted using Ansible. Persistence across reboots was ensured through an \texttt{/etc/fstab} entry.

\section{Test Procedure}

Since energy consumption is not calculated beyond the node level, all tests were conducted on a single worker node. The test pod (either high-CPU or high-memory) generated workloads at predefined levels of 10\%, 30\%, 50\%, 70\%, and 90\% for a fixed duration of 30 minutes per workload level.

For CPU and memory testing, each experiment was run under two cluster conditions:

\begin{itemize}
    \item \textbf{Idle cluster:} No background load.
    \item \textbf{Busy cluster:} Background pods at 90\% utilization.
\end{itemize}

Because disk and network usage are not restricted by default in Kubernetes, this distinction was not applied for Disk I/O and Network I/O tests.

\subsection{CPU Stress Test}

CPU-intensive workloads were generated using \texttt{stress-ng}, with a CPU worker initiated on each available core. This test was performed under both idle and busy cluster conditions.

\subsection{Memory Stress Test}

Memory-intensive workloads were generated using \texttt{stress-ng}, where a virtual memory worker allocated all available memory. As with the CPU tests, experiments were conducted under idle and busy cluster conditions.

\subsection{Disk I/O Stress Test}

Disk performance was evaluated using \texttt{fio}. The test consisted of two phases: first, the maximum achievable IOPS were measured using random read operations on the mounted HDD. Next, controlled read operations were issued at predefined percentages of the measured maximum. Random reads and direct I/O were used exclusively to eliminate caching effects.

\subsection{Network I/O Stress Test}

Network performance was evaluated using \texttt{iperf3}. First, the maximum bandwidth between pods on different nodes was measured. Then, controlled tests were executed at various percentages of the maximum bandwidth. To avoid server-side measurement overhead, only client-side results were analyzed.

\section{Data Analysis}

Data collected from each experiment was analyzed in two phases using Python.

\subsection{Data Querying}

Prometheus was queried to extract Kepler metrics corresponding to each experiment’s duration. The retrieved data was stored as CSV files. The analysis relied on the Python libraries \texttt{pandas}, \texttt{requests}, and \texttt{datetime} for data querying and processing.

\subsection{Diagrams}

Visualization of Kepler metric data was performed using \texttt{matplotlib}. Each diagram included:

\begin{itemize}
\item X-axis: Time
\item Primary Y-axis: Kepler metric values (Watts or operations per second)
\item Secondary Y-axis: Test workload percentage
\item A moving-average overlay to improve readability
\end{itemize}

By correlating workload levels with Kepler metrics, the structured analysis validated Kepler’s suitability for future energy-efficiency studies.
