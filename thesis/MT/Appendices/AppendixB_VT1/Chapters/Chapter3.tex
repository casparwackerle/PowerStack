% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

\chapter{Implementation}
\label{vt1_Chapter3}

This chapter describes the implementation and configuration of the various components used in this project. All automation scripts are designed to be idempotent and are executed using shell scripts located in the \texttt{Powerstack/scripts} directory. In general, configuration is performed via the central configuration file (\texttt{Powerstack/configs/inventory.yml}), unless stated otherwise. Sensitive information is stored in the Ansible Vault file (\texttt{Powerstack/configs/vault.yml}).

\section{K3s Installation}

This section outlines the steps involved in setting up a Kubernetes cluster using K3s on bare-metal servers. Installation was automated using an Ansible playbook forked from the official \texttt{k3s-io/k3s-ansible}\parencite{k3s-ansible} repository, with customizations for internal IP-based communication.

\subsection{Preparing the Nodes}

Before running the Ansible playbook, the following prerequisites must be met on all servers:

\begin{itemize}
    \item \textbf{Operating system:} Ubuntu~22.04 (kernel version 5.15.0).
    \item \textbf{Passwordless SSH:} A user with sudo privileges must have passwordless SSH access to each server.
    \item \textbf{Networking:} Each server must have both an internal IP (for cluster traffic) and an external IP (for VPN or external management).
    \item \textbf{Local Ansible control node setup:}
    \begin{itemize}
        \item \textbf{ansible-community}~9.2.0 (version 8.0+ required).
        \item \textbf{Python}~3.12.3 and \textbf{Jinja}~3.1.2.
        \item \textbf{kubectl}~1.31.3.
    \end{itemize}
\end{itemize}

\subsection{K3s Installation with Ansible}

The playbook supports x64, arm64, and armhf architectures. For this project, it was tested only on x64.

\subsubsection{Configuration Details}

\begin{itemize}
    \item Internal and external IP addresses for all servers must be defined.
    \item One server must be designated as the control-plane node.
    \item Default values such as \texttt{ansible\_user}, \texttt{ansible\_port}, and the \texttt{k3s\_version} may be adjusted if necessary.
\end{itemize}

\subsubsection{Kubectl Configuration}

\begin{itemize}
    \item The playbook automatically installs and configures \code{kubectl} on the Ansible control node by copying the Kubernetes config file from the control-plane node.
    \item The user must rename the copied file from \texttt{config-new} to \texttt{config} and select the PowerStack context using:
    \\
    \texttt{kubectl config use-context powerstack}
\end{itemize}

\section{NFS Installation and Setup}

\subsection{NFS Installation with Ansible}

The NFS server and clients were fully automated via an Ansible playbook. Before beginning the automated setup, the following manual step must be completed:

\begin{itemize}
\item \textbf{Disk selection:} A disk must be selected on the control-plane node for persistent storage. This disk will be reformatted and all existing data will be lost.
\end{itemize}

The Ansible playbook performs the following actions:

\begin{itemize}
\item \textbf{Disk preparation:} The selected disk is partitioned (if required) and formatted with a single Btrfs partition occupying the full disk. The partition is mounted at \texttt{/mnt/data}, and an entry is added to \texttt{/etc/fstab} for persistence across reboots.
\item \textbf{NFS server setup:} The \texttt{nfs-kernel-server} package is installed and configured on the control-plane node. The directory \texttt{/mnt/data} is exported as an NFS share for the worker nodes.
\item \textbf{NFS client setup:} On each worker node, the \texttt{nfs-common} package is installed. The NFS share is mounted, and an \texttt{/etc/fstab} entry is added to ensure persistence.
\end{itemize}

\subsubsection{Configuration Details}

\begin{itemize}
    \item The NFS network range must be specified, and all nodes must be part of that network.
    \item The export path must be defined.
\end{itemize}

\section{Rancher Installation and Setup}

\subsection{Rancher Installation with Ansible and Helm}

Although not strictly required for the project, Rancher was deployed in the
\texttt{cattle-system} namespace to support debugging and system analysis. The installation was automated using an Ansible playbook that integrates Helm. The key steps were:

\begin{itemize}
\item \textbf{Helm installation:} Helm was installed on the control-plane node to deploy Rancher and its dependencies.
\item \textbf{Namespace creation:} The \texttt{cattle-system} namespace was created for the Rancher deployment.
\item \textbf{Cert-Manager deployment:} Cert-Manager was installed to manage TLS certificates.
\item \textbf{Rancher deployment:} Rancher was installed using the official Helm chart. During installation:
  \begin{itemize}
  \item \textbf{Hostname:} A hostname was defined for accessing Rancher.
  \item The chart was configured with \texttt{--set tls=external} to enable external access.
  \item \textbf{Bootstrap password:} A secure bootstrap password was set for the default administrator account.
  \end{itemize}
\item \textbf{Ingress configuration:} An ingress resource was created to route traffic to Rancher via the defined hostname.
\end{itemize}

\section{Monitoring Stack Installation and Setup with Ansible}

The monitoring stack (Prometheus, Grafana, and AlertManager) was deployed using the \texttt{kube-prometheus-stack}\parencite{prometheus_helm_charts} Helm chart from the \texttt{prometheus-community/helm-charts} repository. Although the repository was forked for convenience, no upstream modifications were made, ensuring compatibility with future updates.

\subsection{Prometheus and Grafana Installation with Ansible and Helm}

The installation was automated using Ansible roles, ensuring idempotency and centralized configuration management. The following key steps were executed:
\begin{itemize}
    \item \textbf{Persistent Storage Configuration:}
    \begin{itemize}
        \item Directories for Prometheus, Grafana, and AlertManager were created on the NFS-mounted disk.
        \item A custom \texttt{StorageClass} was defined for NFS storage. The default \texttt{local-path} \texttt{StorageClass} was overridden to ensure it is no longer the default.
        \item PersistentVolumes (PVs) were created for Prometheus, Grafana, and AlertManager. A PersistentVolumeClaim (PVC) was created explicitly for Grafana, while the PVCs for Prometheus and AlertManager were managed by the Helm chart.
    \end{itemize}

    \item \textbf{Helm Chart Installation:}
    \begin{itemize}
        \item A Helm values file was generated dynamically using a Jinja template. This template incorporated variables from the central Ansible configuration file to ensure consistency. Sensitive information, such as the Grafana admin password, was included in the values file and removed from the control node after installation to mitigate security risks.
        \item The Helm chart was installed via an Ansible playbook. The following customizations were applied through the generated values file:
        \begin{itemize}
            \item PVC sizes for Prometheus and AlertManager were set based on the central configuration.
            \item A Grafana admin password was defined.
            \item Prometheus scrape configurations were extended to include Kepler endpoints.
            \item Changes to the \texttt{securityContext} were applied to allow Prometheus to scrape Kepler metrics.
        \end{itemize}
    \end{itemize}

    \item \textbf{Service Port Forwarding:}
    \begin{itemize}
        \item Prometheus, Grafana, and AlertManager services were exposed using static \texttt{NodePort}s defined in the central configuration file, enabling external access.
    \end{itemize}

    \item \textbf{Cleanup:}
    \begin{itemize}
        \item A cleanup playbook was executed to remove sensitive configuration files from both the control-plane node and the Ansible control node.
    \end{itemize}
\end{itemize}

\subsection{Removal Playbook}

An Ansible playbook was created to handle complete uninstallation of the monitoring stack. This ensures that all PVs and PVCs are explicitly removed, avoiding residual artifacts in the Kubernetes cluster.

\section{Kepler Installation and Setup with Ansible and Helm}

\subsection{Preparing the Environment}

The Kepler deployment uses the official Kepler Helm chart repository. Before deploying Kepler, several prerequisites must be met to ensure correct operation.

\subsubsection{Redfish Interface}

The Redfish Scalable Platforms Management API is a RESTful API specification for out-of-band systems management. On the Lenovo servers used in this project, Redfish exposes IPMI-based power metrics, which Kepler accesses through its Redfish interface. To verify Redfish functionality, navigate to the Lenovo XClarity Controller and ensure that the following setting is enabled:

\begin{itemize}
    \item \textbf{IPMI over LAN:} Located under \texttt{Network → Service Enablement and Port Assignment}.
\end{itemize}

Redfish API functionality can be tested using the following endpoints in a web browser:

\begin{itemize}
    \item General Redfish information:
    \\
    \texttt{https://<BMC-IP>/redfish/v1}
    \item Power metrics:
    \\
    \texttt{https://<BMC-IP>/redfish/v1/Chassis/1/Power\#\textbackslash{}PowerControl}
\end{itemize}

\subsubsection{Kernel Configuration}

Kepler requires kernel-level access for \code{eBPF} tracing, which involves calling the \code{perf\_event\_open} syscall. By default, this syscall is restricted. To enable Kepler’s tracing functionality, an Ansible role adjusts the kernel parameter \texttt{perf\_event\_paranoid} via \texttt{sysctl} without requiring a reboot.

The restriction level can be verified by reading \texttt{/proc/sys/kernel/perf\_event\_paranoid}. For this project, all restrictions were removed by setting the value to \texttt{-1}.

\subsection{Kepler Deployment with Ansible and Helm}

Kepler was deployed using the Kepler Helm chart\parencite{Kepler_helm_chart} from the\\
\texttt{sustainable-computing-io/Kepler-helm-chart} repository, with Ansible automating configuration and deployment. Deployment parameters were centralized in a Jinja template, rendered locally, and copied to the control-plane node before installation.

Key configurations in the Helm values file include:

\begin{itemize}
    \item \textbf{Enabled metrics:} Various metric sources were activated for detailed energy monitoring.
    \item \textbf{Service port:} The Kepler service port was defined to allow Prometheus to scrape metrics.
    \item \textbf{Service interval:} The Kepler service interval was set to 10 seconds.
    \item \textbf{Redfish metrics:} Redfish/IPMI metrics were enabled, and Redfish credentials were provided. These credentials match those used for the Lenovo XClarity Controller interface. Note that the BMC IP differs from the node’s IP address.
\end{itemize}

\subsection{Verifying Kepler Metrics}

After deployment, it was essential to verify that Kepler correctly collected and exposed metrics. Verification involved the following steps:

\subsubsection{Prometheus Scraping}

After deployment, successful Prometheus scraping of the Kepler endpoints was verified via the Prometheus web interface.

\subsubsection{Metric Availability}

All Kepler metrics were inspected individually in the Prometheus web interface to ensure that non-zero values were being reported. Each metric presented a single data point, offering additional confirmation that the corresponding data source was being monitored correctly.

\subsubsection{Kepler Logs}

The Kepler logs were reviewed to examine which data sources were successfully utilized:

\lstinputlisting[
    language=bash,
    caption=\texttt{kepler.log},
    basicstyle=\ttfamily\tiny
]{Appendices/AppendixB_VT1/Code/kepler.log}

\subsubsection{Power Metrics from ACPI / IPMI}

When both ACPI and IPMI were enabled for platform power measurement, Kepler preferred IPMI as its primary data source. In this case, Kepler used IPMI for overall platform energy, while relying on ACPI to derive lower-level component estimates. This behavior is expected, as IPMI typically provides more complete platform-level information. In the absence of IPMI data, Kepler automatically falls back to ACPI as the sole power source.

\subsubsection{Redfish Issues}

Kepler occasionally failed to handle individual Redfish data values correctly. These incidents were sporadic and varied across different metrics. The underlying cause could not be resolved within the scope of this thesis. The following log entry illustrates such an error:

\begin{lstlisting}[language=bash]
Failed to get power: json: cannot unmarshal number 3.07 into Go struct field Voltages.Voltages.ReadingVolts of type int
\end{lstlisting}

\subsubsection{Error Message \texttt{cpu0/online}}

The following error message is noteworthy:

\begin{lstlisting}[language=bash]
WARNING: failed to read int from file: open /sys/devices/system/cpu/cpu0/online: no such file or directory
\end{lstlisting}

This warning occurs because the Intel Xeon processor used in this project does not support core offlining,  the dynamic disabling of individual CPU cores at runtime. While core offlining is an interesting feature for energy-efficiency analysis, this limitation can be accepted as a hardware constraint of the project.
