\chapter{Experimental Evaluation of Tycho}
\label{chap:experimental-evaluation}

\section{Evaluation Scope and Evidence Types}
\label{sec:eval-scope}

This chapter evaluates Tycho as an accuracy-focused energy measurement and attribution system operating under realistic observability constraints.
The objective is not to establish ground-truth energy values, but to assess whether Tycho’s outputs are internally consistent, temporally coherent, and physically interpretable given the characteristics of its input signals.

Tycho integrates heterogeneous measurement sources spanning hardware, firmware, kernel, and container layers, each with distinct temporal resolution, latency, and semantic scope.
Accordingly, many relevant correctness properties are structural rather than scalar, concerning conservation, attribution completeness, stability under repeated execution, and coherent cross-layer behavior rather than numerical agreement with an external reference.

Two complementary evidence types are used, ordered by dependency.

First, Tycho underwent continuous qualitative validation throughout development.
New mechanisms were exercised until their behavior aligned with design intent across relevant operating regimes before further features were added.
Validation relied on direct inspection of intermediate system state, including logs and analysis traces, live dashboards, targeted host- and Kubernetes-level stress scenarios, and systematic cross-checking of raw input signals against derived attribution outputs.
This tier establishes internal correctness, semantic coherence, and robustness against structural failure modes such as energy leakage, temporal misalignment, or attribution instability.

Second, Tycho was exercised through a structured repeated execution plan comprising 30 repetitions of a multi-phase workload suite on both an idle and a busy server.
These runs provide broad scenario coverage for additional consistency checks and yield a controlled dataset from which a small number of representative workload scenarios are analyzed in depth.
The resulting analyses are illustrative rather than verificatory, exposing Tycho’s behavior, capabilities, and boundary conditions under realistic execution.
Where repetitions are evaluated quantitatively, they serve only to illustrate stability and variability; no claims of statistical significance are made.

Together, these two evidence tiers support an evaluation focused on interpretability, consistency, and robustness within the limits imposed by the available measurement signals.


\section{Qualitative Validation and Consistency Assessment}
\label{sec:qualitative-validation}

This section establishes internal correctness and semantic coherence of Tycho through invariant-driven qualitative validation, providing the foundation on which the targeted experimental analyses in the following section rely.

\subsection{Invariant-Driven Qualitative Validation Methodology}
\label{sec:validation-methodology}

Qualitative validation in this work is defined as the systematic verification of structural correctness properties that must hold independently of workload choice, operating regime, or numerical calibration.
Rather than evaluating individual output values against an external reference, validation focuses on invariants whose violation would render attribution results uninterpretable, irrespective of apparent numerical plausibility.

Validation was performed continuously and at feature granularity throughout development.
Each newly introduced mechanism was exercised until its behavior aligned with design intent across the relevant operating regimes before subsequent features were added.
This process relied on direct inspection of intermediate system state, including structured logs and analysis traces, live dashboards, and selectively enabled debug metrics exposing internal buffers, windows, and attribution state.
Targeted host- and Kubernetes-level stress scenarios were used to provoke boundary conditions under which structural failures would be expected to surface.

The validation strategy was explicitly invariant-driven.
Core properties monitored across all executions included conservation of energy across attribution boundaries, monotonicity of counters, completeness of attribution, stable separation of idle, dynamic, and residual components, and coherent behavior under repeated, steady, bursty, and concurrent workloads.
These invariants impose global constraints on system behavior and provide strong failure detectability: violations propagate across layers and time windows and manifest as observable inconsistencies between raw inputs, intermediate representations, and exported metrics.

No artifact-oriented quantitative validation dataset was produced for this tier.
This is a consequence of the validation objective rather than a limitation of testing effort.
Correctness was established through sustained, adversarial inspection under diverse execution conditions, where invariant violations would be immediately visible.
Accordingly, this validation tier establishes internal correctness and semantic coherence of Tycho’s attribution pipeline, but does not claim numerical accuracy with respect to an external ground truth.

\subsection{Layer-Specific Validation Focus}
\label{sec:validation-layers}

Qualitative validation was applied across all major layers of the Tycho pipeline, with layer-specific checks designed to surface structural failures at their point of origin while preserving end-to-end interpretability.

\subsubsection{Signal Ingestion and Metric Integrity}
\label{sec:validation-ingestion}

Validation of signal ingestion focused on excluding classes of errors that would compromise the semantic integrity of downstream attribution.
Given the heterogeneity of Tycho’s input signals, correctness at this layer is primarily concerned with continuity, plausibility, and stable interpretation over time rather than with numerical agreement across sources.

The principal ingestion risks considered were missing or irregular samples, counter discontinuities, inconsistent units or scaling, unstable identity mapping across devices or workloads, and silent gaps that could propagate undetected into higher-level metrics.
Validation therefore emphasized monotonicity of cumulative counters, absence of unexplained discontinuities, stable labeling and device identity resolution across successive observations, and predictable behavior under steady, bursty, and concurrent activity.

Independent signal sources were routinely cross-checked for qualitative plausibility.
CPU, GPU, and system-level metrics were inspected jointly to ensure that observed activity patterns were mutually consistent and that energy attribution in one domain did not exhibit unexplained coupling to unrelated activity in another.
Sustained agreement in qualitative behavior across independent sources provided confidence that ingestion pipelines preserved intended semantics.

Known limitations of individual sources were explicitly accounted for during validation.
In particular, system-level power metrics obtained via Redfish were observed to exhibit variable reporting latency relative to CPU and GPU counters.
This behavior was treated as an expected characteristic of the source rather than as an ingestion defect.
Validation confirmed that such latency manifested as delayed but coherent updates, without introducing discontinuities, counter violations, or semantic contradictions that would undermine subsequent analysis.

\subsubsection{Temporal Coherence and Delay Handling}
\label{sec:validation-temporal}

Temporal validation focused on ensuring coherent analysis behavior in the presence of heterogeneous timing characteristics, asynchronous updates, and source-specific latency.
Given that Tycho does not enforce strict timestamp alignment at ingestion, correctness at this layer depends on stable window materialization, preserved causal ordering, and predictable interpretation of delayed signals during analysis.

Validation targeted failure modes such as temporal misalignment between sources, unstable or inconsistent window boundaries, sensitivity to short-term jitter, and systematic bias introduced by delayed updates.
Intermediate timing state, analysis windows, and derived metrics were inspected under controlled transitions between idle, steady, bursty, and concurrent workloads, as well as during long-running executions where accumulated drift or buffer exhaustion would become observable.

Variable latency of system-level power metrics was a central consideration.
Rather than attempting to minimize or mask such delay, validation emphasized robustness of temporal fusion, confirming that late-arriving samples were incorporated consistently without violating conservation, monotonicity, or attribution completeness.
Pathological temporal behavior would manifest as negative attribution, unstable residuals, or inconsistent phase transitions; no such effects were observed outside explicitly characterized boundary conditions.

Observed lag and smoothing effects in derived metrics were therefore interpreted as expected consequences of source characteristics and aggregation granularity rather than as temporal failures.
Under all tested execution regimes, temporal handling preserved stable, interpretable behavior across interacting metric streams.

\subsubsection{Attribution Semantics and Conservation}
\label{sec:validation-attribution}

Validation of attribution semantics centered on enforcing conservation of energy as a non-negotiable invariant.
For every analysis window, all observed energy must be accounted for exactly once, either through explicit workload attribution or as transparently exposed residual or system-level energy, up to numerical tolerance.
Any silent loss, duplication, or implicit discarding of energy would immediately undermine interpretability and was therefore treated as a structural failure.

Validation focused on confirming stable separation and interaction of idle, dynamic, and residual components across execution regimes.
Attribution behavior was inspected under steady, bursty, and concurrent workloads to ensure that conservation and attribution completeness held independently of scheduling dynamics, workload overlap, or resource contention.
Violations would manifest as unexplained discrepancies between aggregate and attributed energy or as unstable residual behavior and were actively monitored throughout development and testing.

Boundary cases were explicitly identified and characterized.
In particular, extreme latency of certain system-level power signals can induce transient distortions in residual attribution.
These effects are attributable to input signal characteristics rather than to attribution semantics and remain bounded and detectable.
Their presence does not violate conservation and does not compromise interpretability when properly contextualized.

\subsubsection{End-to-End Observability and Sanity Checks}
\label{sec:validation-observability}

End-to-end observability served as a primary validation instrument throughout development.
Tycho was continuously inspected as a live system across multiple abstraction levels, including raw input metrics, intermediate analysis state, and exported outputs, enabling direct cross-checks between independently derived views of system behavior.

Validation emphasized a small set of canonical sanity patterns that must hold for any interpretable attribution system.
These included predictable responses to workload start and termination, stability under steady-state execution, symmetry under equivalent workload configurations, and absence of unexplained oscillations or phase-dependent artifacts.
Departures from these patterns would indicate semantic or temporal inconsistencies and were actively sought during exploratory and adversarial testing.

Tycho’s debug mode, which selectively exposes internal buffers, windows, and attribution state, enabled targeted inspection of intermediate behavior during runtime.
This capability allowed rapid localization of potential issues and ensured that observed high-level behavior was grounded in coherent internal state rather than emergent artifacts.

Across sustained observation under diverse workload regimes, raw metrics, intermediate analysis state, and exported outputs converged toward consistent qualitative behavior.
No unexplained or contradictory patterns were observed, providing strong assurance of end-to-end semantic coherence.

\subsection{Summary of Qualitative Assurance}
\label{sec:validation-summary}

The qualitative validation described above establishes high confidence in Tycho’s internal correctness and semantic coherence.
Across sustained and adversarial inspection of all major subsystems, no violations of fundamental invariants, unexplained inconsistencies, or contradictory behaviors were observed.
This assurance provides the foundation for the targeted experimental analyses that follow, which illustrate Tycho’s behavior and limitations under concrete workload scenarios rather than re-establishing correctness.

% ----------------------------------------------------------------------
\section{Targeted Experimental Evaluation}
\label{sec:tier2-tests}

This section presents a set of targeted, end-to-end experiments designed to illustrate Tycho’s behavior under structured and repeatable workload conditions.
The experiments do not serve to verify correctness, which is established through qualitative validation, but to expose observable behavior, strengths, and boundary conditions in concrete scenarios.
Accordingly, the analyses are descriptive and interpretive, and no claims of formal correctness or statistical significance are made.

\subsection{Overview of Executed Test Scenarios}
\label{sec:test-overview}

All targeted experiments were derived from a single declarative test plan and executed end-to-end under controlled and repeatable conditions.
This subsection describes the execution model, test environment, and scenario structure, and serves as a reference point for the analyses presented in the following sections.

\subsubsection{Test Setup and Execution Environment}

All experiments were performed on a dedicated compute node acting as the system under test (SUT).
The node was a DALCO-integrated server (product \code{G494-ZU0-\newline AAP1-000}, MegaRAC SP-X BMC) equipped with an AMD EPYC~9554 (64 cores), 192\,GB DDR5 memory, dual NVMe storage devices, and two GPUs (NVIDIA RTX~4000 Ada and NVIDIA T4).
The SUT executed only the monitoring stack, workload pods, and the Tycho exporter, ensuring that observed signals reflect workload and system behavior rather than Kubernetes control-plane activity.

The operating system was Ubuntu~22.04~LTS with a Linux~6.8 kernel (\code{6.8.0-90-\newline generic}, \code{PREEMPT\_DYNAMIC}) on \code{x86\_64}.
GPU support was provided by the NVIDIA proprietary driver (\code{580.95.05}) with CUDA~13.0.
Kubernetes was deployed directly on the servers using PowerStack~\cite{PowerStack}.
To minimize measurement interference, the Kubernetes control plane ran on a separate node, and the SUT operated exclusively as a worker.
Full root access was available and required for eBPF-based instrumentation.

A known hardware limitation of the AMD platform is the absence of a RAPL DRAM energy domain.
All Tycho functionality related to DRAM attribution was therefore validated independently on an Intel-based control node and is excluded from the AMD-based experimental scenarios presented here.

Experimental analyses focus on energy domains and metrics that are fully supported and observable on the SUT, in particular CPU package and core energy via RAPL and GPU energy via vendor telemetry.
These domains provide stable counters and sufficient dynamic range for illustrating attribution behavior under the tested workload scenarios.

\subsubsection{Test Plan Structure and Scenario Catalog}

All experiments were executed from a single declarative test plan that defines workload structure, execution order, repetition count, and timing parameters.
An Ansible-based automation workflow materializes the plan, provisions an isolated Kubernetes testing namespace, and orchestrates execution end-to-end without manual intervention.
This approach ensures reproducibility across runs while allowing workload structure to be modified independently of deployment mechanics.

Unless stated otherwise, all scenarios were executed with 30 repetitions and analyzed at a fixed aggregation granularity of 3\,s.
Each repetition executes the complete scenario sequence in a deterministic order, yielding a consistent temporal structure across runs.

The test plan consists of an ordered sequence of workload scenarios, each designed to exercise a specific attribution-relevant aspect of Tycho’s behavior.
Explicit sleep phases are inserted between all workload phases to allow the system to return to an idle state and to reduce thermal, scheduling, and buffer carry-over effects.
For analysis, only steady-state execution intervals are considered; warm-up phases and transient effects at workload boundaries are excluded.

The following catalog enumerates the executed scenarios and serves as the reference structure for the targeted analyses presented in the subsequent subsections.


\begin{enumerate}
  \item \textbf{Idle baseline (start).}
  Establishes the initial noise floor and short-term stability of idle attribution.
  \begin{itemize}
    \item Duration: 180\,s
    \item Workloads: none (sleep only)
    \item Purpose: idle variance estimation, initial drift detection
  \end{itemize}

  \item \textbf{CPU warm-up ramp.}
  Transitional phase used exclusively for preconditioning.
  \begin{itemize}
    \item Workload: \code{stress-ng} CPU ramp (\code{matrixprod})
    \item Duration: 60\,s
    \item Structure: stepped load increase
    \item Purpose: stabilization of subsequent CPU measurements
    \item Note: excluded from quantitative analysis
  \end{itemize}

  \item \textbf{CPU steady baseline (matrix multiplication).}
  Reference case for steady-state CPU dynamic energy attribution.
  \begin{itemize}
    \item Workload: \code{stress-ng} CPU (\code{matrixprod})
    \item Duration: 120\,s
    \item CPU request: 16\,000\,mCPU
    \item Concurrency: single pod
    \item Purpose: steady-state stability and repeatability
  \end{itemize}

  \item \textbf{CPU burst train.}
  Periodic on/off CPU activity to probe temporal aggregation and duty-cycle behavior.
  \begin{itemize}
    \item Workload: \code{stress-ng} CPU burst (\code{matrixprod})
    \item Duration: 180\,s
    \item Structure: 9\,s on / 9\,s off
    \item CPU request: 16\,000\,mCPU
    \item Purpose: burst attribution fidelity and temporal smearing
  \end{itemize}

  \item \textbf{CPU jitter train.}
  Irregular CPU burst timing to assess robustness under non-uniform activity.
  \begin{itemize}
    \item Workload: \code{stress-ng} CPU jitter (\code{matrixprod})
    \item Duration: 180\,s
    \item Structure: random gaps between 3\,s and 15\,s
    \item CPU request: 16\,000\,mCPU
    \item Purpose: attribution stability under irregular timing
  \end{itemize}

  \item \textbf{CPU discrimination (concurrent, heterogeneous pair I).}
  Concurrent execution of computationally distinct CPU workloads.
  \begin{itemize}
    \item Workloads: \code{stress-ng} \code{int128} and \code{fft}
    \item Duration: 180\,s
    \item CPU request: 8\,000\,mCPU per workload
    \item Concurrency: two pods
    \item Purpose: workload-level energy differentiation
  \end{itemize}

  \item \textbf{CPU discrimination (concurrent, heterogeneous pair II).}
  Alternative discrimination scenario with different computational characteristics.
  \begin{itemize}
    \item Workloads: \code{stress-ng} \code{bitops} and \code{matrixprod}
    \item Duration: 180\,s
    \item CPU request: 8\,000\,mCPU per workload
    \item Concurrency: two pods
    \item Purpose: method-dependent energy signatures
  \end{itemize}

  \item \textbf{CPU idle-allocation fairness (busy vs.\ noop).}
  Co-scheduled active and inactive workloads to probe idle energy assignment semantics.
  \begin{itemize}
    \item Workloads: active \code{stress-ng} CPU (\code{bitops}) and idle sleep pod
    \item Duration: 180\,s
    \item CPU request: 8\,000\,mCPU per workload
    \item Purpose: idle energy sharing behavior
  \end{itemize}

  \item \textbf{GPU idle baseline.}
  Baseline measurement of GPU idle energy in the absence of GPU workloads.
  \begin{itemize}
    \item Duration: 120\,s
    \item Workloads: none (GPU idle)
    \item Purpose: GPU idle stability and cross-domain isolation
  \end{itemize}

  \item \textbf{GPU steady baseline.}
  Reference case for steady-state GPU dynamic energy attribution.
  \begin{itemize}
    \item Workload: steady GPU burn
    \item Duration: 180\,s
    \item GPU request: 1 device
    \item Purpose: stability and repeatability of GPU attribution
  \end{itemize}

  \item \textbf{GPU burst train.}
  Periodic GPU activity to probe temporal aggregation limits.
  \begin{itemize}
    \item Workload: GPU burst burn
    \item Duration: 240\,s
    \item Structure: 3\,s on / 3\,s off
    \item GPU request: 1 device
    \item Purpose: burst visibility under coarse aggregation
  \end{itemize}

  \item \textbf{GPU concurrency (two workloads).}
  Concurrent GPU workloads to assess proportional energy splitting.
  \begin{itemize}
    \item Workloads: two identical steady GPU burns
    \item Duration: 180\,s
    \item GPU request: 1 device per workload
    \item Purpose: two-way GPU energy attribution
  \end{itemize}

  \item \textbf{GPU concurrency (three workloads).}
  Higher-concurrency GPU scenario to probe scaling and slicing robustness.
  \begin{itemize}
    \item Workloads: three identical steady GPU burns
    \item Duration: 180\,s
    \item GPU request: 1 device per workload
    \item Purpose: multi-workload GPU attribution consistency
  \end{itemize}

  \item \textbf{Idle baseline (end).}
  Re-establishes idle conditions after all workload phases.
  \begin{itemize}
    \item Duration: 180\,s
    \item Workloads: none (sleep only)
    \item Purpose: end-of-run drift and stability check
  \end{itemize}
\end{enumerate}

\paragraph{Visualization Conventions and Variability}
All scatter plots in this section depict per-repetition mean workload power over the corresponding steady-state execution window.
Vertical error bars represent within-run variability of the attributed power signal across the underlying aggregation intervals.

Under background system load, CPU time is subject to scheduling contention and time-sliced execution.
As a result, instantaneous dynamic power attribution may fluctuate substantially between aggregation intervals even within a single repetition.
Large error bars in busy-system scenarios are therefore expected and reflect genuine execution variability under contention rather than measurement noise or attribution instability.
In contrast, idle-attributed components exhibit minimal variability and remain stable across repetitions.

\subsection{CPU Idle-Allocation Fairness (Busy vs.\ Noop)}
\label{sec:test-cpu-idle-fairness}

\paragraph{Objective}
This scenario probes Tycho’s idle CPU energy attribution semantics under concurrent workloads with identical resource declarations, contrasting an active CPU-bound workload with an idle (noop) workload.

\paragraph{Observed Behavior}
Figures~\ref{fig:cpu-idle-fairness-idle} and~\ref{fig:cpu-idle-fairness-busy} show the workload-level CPU package-domain power attributed to an active and a noop pod under idle and busy system conditions, respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_cpu_busy_vs_noop_idle_share_pkg_idle_node.png}
  \caption[CPU idle-allocation fairness (idle node)]%
  {Idle-node execution: RAPL PKG workload power attributed to a busy and a noop pod.
  Dynamic power is attributed exclusively to the active workload, while idle power is evenly shared between both pods across repetitions.}
  \label{fig:cpu-idle-fairness-idle}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_cpu_busy_vs_noop_idle_share_pkg_busy_node.png}
  \caption[CPU idle-allocation fairness (busy node)]%
  {Busy-node execution: RAPL PKG workload power under background system load.
  Idle power attribution remains unchanged, while dynamic power attributed to the active workload is reduced and exhibits increased variance due to contention.}
  \label{fig:cpu-idle-fairness-busy}
\end{figure}

Across both system states, the active workload is consistently attributed substantial dynamic CPU energy, while the noop workload exhibits no measurable dynamic component.
Dynamic energy attributed to the active workload is higher on the idle system and decreases under background load, accompanied by increased variability due to contention effects.
In contrast, idle CPU package energy is attributed nearly equally to both workloads and remains approximately constant regardless of whether the system is idle or busy, or whether a workload is actively executing or idle.

\paragraph{Interpretation}
The observed behavior indicates that Tycho attributes idle CPU energy based on declared resources rather than instantaneous execution intensity.
Dynamic energy attribution remains sensitive to effective CPU availability and contention, while idle allocation is invariant under differing workload behavior and background system load.
Together, these results demonstrate fair and workload-agnostic idle energy distribution alongside execution-sensitive dynamic attribution.

\paragraph{Limitations}
This scenario does not assess absolute attribution accuracy, asymmetric resource requests, or fine-grained temporal effects outside steady-state execution.

\subsection{CPU Discrimination under Heterogeneous Concurrent Workloads}
\label{sec:test-cpu-discrimination-heterogeneous}

\paragraph{Objective}
This scenario examines whether Tycho can consistently distinguish \emph{dynamic} CPU energy consumption between heterogeneous CPU-bound workloads executing concurrently under identical resource declarations.
Beyond absolute attribution, the experiment further investigates whether normalization by instruction count exposes residual, workload-dependent energy characteristics.

\paragraph{Observed Behavior}
Figures~\ref{fig:cpu-discrimination-heterogeneous-idle} and~\ref{fig:cpu-discrimination-heterogeneous-busy} show the workload-level \emph{dynamic} CPU core-domain power attributed to four heterogeneous workloads under idle and busy system conditions, respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_mean_dynamic_power_core_all_idle_node.png}
  \caption[CPU discrimination under heterogeneous workloads (idle node)]%
  {Idle-node execution: RAPL core-domain \emph{dynamic} workload power attributed to four heterogeneous CPU workloads with identical resource requests.
  Distinct workload-dependent power consumption levels are consistently observable across repetitions.}
  \label{fig:cpu-discrimination-heterogeneous-idle}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_mean_dynamic_power_core_all_busy_node.png}
  \caption[CPU discrimination under heterogeneous workloads (busy node)]%
  {Busy-node execution: RAPL core-domain \emph{dynamic} workload power under background system load.
  Variance increases due to contention, while the relative separation between workload types remains clearly visible.}
  \label{fig:cpu-discrimination-heterogeneous-busy}
\end{figure}

Across repetitions on the idle system, each workload exhibits a clearly distinguishable dynamic power profile.
The relative ordering between workload types remains stable, with BitOps consistently consuming the highest dynamic power, followed by FFT and \code{int128}, while \code{MatrixProd} exhibits the lowest power consumption.
Under background system load, the overall variance of the attributed dynamic power increases due to contention and scheduling interference, yet the qualitative separation between workloads persists.

\paragraph{Interpretation}
The absolute dynamic power differences primarily reflect differences in instruction throughput and execution-unit utilization.
Simple integer workloads such as BitOps sustain high instruction retirement rates with minimal dependency on memory or complex control flow, resulting in high dynamic power.
In contrast, workloads such as \code{MatrixProd} are more strongly influenced by memory access patterns and data dependencies, leading to reduced execution-unit utilization and lower sustained dynamic power.
The persistence of this ordering under concurrent execution indicates that Tycho preserves workload-specific dynamic attribution characteristics despite contention.

To isolate whether these differences are intrinsic to the workloads or predominantly driven by instruction volume, the same experiment is repeated with dynamic energy normalized by the number of retired instructions.
Figures~\ref{fig:cpu-discrimination-heterogeneous-idle-normalized} and~\ref{fig:cpu-discrimination-heterogeneous-busy-normalized} present the resulting \emph{dynamic energy per instruction} for idle and busy system conditions, respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_mean_dynamic_power_core_all_idle_node_normalized.png}
  \caption[CPU energy per instruction under heterogeneous workloads (idle node)]%
  {Idle-node execution: RAPL core-domain \emph{dynamic} CPU energy normalized by instruction count.
  Values are shown as per-repetition means, with an aggregated mean and 95\% confidence interval across repetitions.}
  \label{fig:cpu-discrimination-heterogeneous-idle-normalized}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_mean_dynamic_power_core_all_busy_node_normalized.png}
  \caption[CPU energy per instruction under heterogeneous workloads (busy node)]%
  {Busy-node execution: Dynamic CPU energy per instruction under background system load.
  While variance increases due to contention, workload-dependent differences remain observable.}
  \label{fig:cpu-discrimination-heterogeneous-busy-normalized}
\end{figure}

Normalization substantially reduces the separation between workloads, demonstrating that a large fraction of the absolute dynamic energy differences is explained by differing instruction counts.
This behavior serves as a consistency check for instruction-level attribution and confirms that higher-power workloads primarily consume more energy because they retire more instructions.

Crucially, the normalized results do not collapse to a single constant.
Instead, statistically significant differences in energy per instruction remain observable, particularly on the idle system.
For idle execution, FFT and \code{int128} exhibit consistently higher energy per instruction than BitOps and \code{MatrixProd}, despite comparable instruction counts.

A plausible explanation lies in differences in instruction mix and microarchitectural utilization.
FFT workloads typically involve floating-point and vectorized arithmetic, activating execution units that are more energy-intensive than simple integer ALUs.
Similarly, \code{int128} operations are often decomposed into multiple micro-operations and wide datapath activity on 64-bit architectures, increasing backend utilization per retired instruction.
In contrast, BitOps primarily exercise simple integer pipelines, while \code{MatrixProd} frequently encounters memory-related stalls that limit execution-unit activity per instruction.
These effects are more clearly expressed on the idle system, where higher operating frequencies and reduced contention expose microarchitectural differences between workloads.

\paragraph{Limitations}
This scenario does not establish causal energy costs of individual instruction types, nor does it generalize across microarchitectures or frequency configurations.
The experiment focuses exclusively on CPU core-domain energy and does not explore interactions with memory, uncore components, or heterogeneous accelerators.

\subsection{GPU Workload Separation under Concurrent Execution}
\label{sec:test-gpu-concurrent-discrimination}

\paragraph{Objective}
This scenario evaluates whether Tycho can attribute GPU energy independently to concurrent workloads executing on different physical devices.

\paragraph{Observed Behavior}
Figure~\ref{fig:gpu-concurrent-discrimination} shows the workload-level GPU power attributed to two concurrent workloads, each bound to a distinct physical GPU.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_gpu_concurrent_2pods.png}
  \caption[GPU workload separation under concurrent execution]%
  {Concurrent GPU workloads executing on two physical GPUs.
  The higher-performance device consistently exhibits higher power consumption across repetitions.
  Observed attribution remains stable under background CPU load.}
  \label{fig:gpu-concurrent-discrimination}
\end{figure}

Across all repetitions, the two GPUs exhibit stable and clearly separated energy consumption levels.
The higher-performance device consistently consumes more power than the lower-tier GPU, with limited variability between runs.
Background CPU load does not introduce any systematic change in the attributed GPU power for either workload.

\paragraph{Interpretation}
The observed behavior indicates that GPU energy attribution in Tycho is device-specific and isolated from host CPU activity.
Concurrent execution on multiple GPUs does not introduce cross-device interference in the attributed energy signals.

\paragraph{Limitations}
This scenario does not examine contention on shared GPU resources, short-lived kernels, or mixed GPU workloads on a single device.

\subsection{GPU Workload Behavior under Oversubscription}
\label{sec:test-gpu-concurrent-oversubscription}

\paragraph{Objective}
This scenario examines GPU energy attribution under oversubscription, where more concurrent workloads are present than available physical GPUs, requiring multiple workloads to share a single device via time-multiplexed execution.

\paragraph{Observed Behavior}
Figure~\ref{fig:gpu-concurrent-oversubscription} shows the workload-level GPU power attributed to three concurrent workloads executing on two physical GPUs, resulting in one single-workload and one two-workload device configuration.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Figures/diagrams/scatter_gpu_concurrent_3pods.png}
  \caption[GPU workload behavior under oversubscription]%
  {Three concurrent GPU workloads executing on two physical GPUs.
  Single-workload execution exhibits stable power with low variance, while co-located workloads share GPU power and show increased variability due to time-multiplexed execution.
  Total device power remains approximately constant across configurations.}
  \label{fig:gpu-concurrent-oversubscription}
\end{figure}

When a GPU executes a single workload, the attributed power is high and consistent, with comparatively low variance across repetitions.
When two workloads share a GPU, both exhibit similar average power consumption, while the total power attributed to the device remains approximately constant.
Workloads co-located on the same GPU show a noticeably higher variance than single-workload execution, consistent with time-multiplexed scheduling.
These patterns remain stable across repetitions and are unaffected by background CPU load.

\paragraph{Interpretation}
The observed behavior indicates that Tycho conserves GPU energy under oversubscription and partitions device power across concurrent workloads without duplication or inflation.
Increased variability reflects time-multiplexed execution rather than attribution instability.

\paragraph{Limitations}
This scenario does not examine finer-grained GPU scheduling behavior, short-lived kernels, memory-bound workloads, or oversubscription levels beyond two concurrent workloads per device.

\subsection{Component Power Response to Sudden Load Transitions}
\label{sec:test-sudden-load-transitions}

\paragraph{Objective}
This scenario illustrates Tycho’s component-level power attribution behavior during abrupt and intense load transitions on an otherwise idle node.
The goal is to expose transient effects and metric source limitations rather than to establish statistically representative behavior.

\paragraph{Observed Behavior}
Figure~\ref{fig:sudden-load-transitions} shows two exemplary time series of component power during a sudden onset of CPU and GPU stress, respectively.
Each plot represents a single execution and is intended to illustrate characteristic behavior observed across multiple runs.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/diagrams/line_sudden_cpu_stress.png}
    \caption{Sudden CPU stress}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/diagrams/line_sudden_gpu_stress.png}
    \caption{Sudden GPU stress}
  \end{subfigure}
  \caption[Component power during sudden load transitions]%
  {Component-level power attribution during abrupt CPU and GPU load on an idle node.}
  \label{fig:sudden-load-transitions}
\end{figure}

In both cases, the system exhibits a stable idle baseline prior to load application.
When CPU or GPU stress is applied abruptly, the corresponding component metric responds immediately, while the system-level power reported via Redfish increases with a noticeable delay.
During this transition period, the residual power term exhibits a pronounced temporary dip, despite no plausible reduction in the energy consumption of residual components.

This pattern is consistently observable for both CPU- and GPU-dominated load transitions.
Once the system-level metric converges to the higher load state, the residual component recovers and stabilizes.

\paragraph{Interpretation}
The observed transient behavior reflects fundamental latency differences between metric sources.
CPU and GPU component metrics react rapidly to workload changes, while Redfish-based system power responds more slowly due to its coarse temporal characteristics, as discussed in earlier chapters.
During this interval, Tycho attributes the rapidly increasing component power before the delayed system power increase is fully observed, resulting in a temporary residual imbalance.

This effect does not indicate an attribution error or implementation fault.
Tycho deliberately avoids fabricating or extrapolating system-level energy beyond its tightly scoped modeling mechanisms.
Instead, it reports observed values as provided by the underlying measurement sources, preserving physical transparency even when temporary inconsistencies arise.
To make such situations explicit, Tycho emits a window plausibility indicator that marks windows affected by implausible intermediate attribution states.

\paragraph{Limitations}
These examples are anecdotal and not statistically representative.
The scenario does not isolate individual contributors to delayed system power response, such as thermal inertia or fan activation thresholds, and does not attempt to distinguish metric latency from genuine physical response delays.

\section{Summary of Evaluation Findings}
\label{sec:eval-summary}

The evaluation presented in this chapter demonstrates that Tycho exhibits stable, interpretable, and internally consistent behavior across a diverse range of workload patterns and system conditions.
Qualitative validation establishes confidence in the correctness and semantic coherence of the attribution pipeline, while targeted experimental scenarios illustrate how these properties manifest under realistic execution.

Taken together, the results show that Tycho preserves key structural invariants, responds predictably to changes in workload structure and system load, and exposes meaningful attribution behavior within the limits imposed by available measurement signals.
The evaluation does not aim to establish numerical ground truth accuracy, but provides a sound empirical basis for reasoning about Tycho’s capabilities, limitations, and intended use as an analysis and research instrument.

A synthesized interpretation of these findings and their implications is presented in the following chapter.
