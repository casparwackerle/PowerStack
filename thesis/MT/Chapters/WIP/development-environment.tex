\section{System Environment for Development, Build and Debugging}
\label{sec:tycho_sysenv}
This section documents the environment used to develop, build, and debug \textit{Tycho}; detailed guides live in \cite{TychoRepo}.

\subsection{Host Environment and Assumptions}
\label{sec:tycho_sysenv_host}

All development and debugging activities for \textit{Tycho} were performed on bare-metal servers rather than virtualized instances. Development matched the evaluation target and preserved access to hardware telemetry such as RAPL, NVML, and BMC Redfish. The host environment consisted of Lenovo ThinkSystem SR530 servers (Xeon Bronze 3104, 64 GB DDR4, SSD+HDD, Redfish-capable BMC).

The systems ran Ubuntu 22.04 with a Linux 5.15 kernel. Full root access was available and required in order to access privileged interfaces such as eBPF. Kubernetes was installed directly on these servers using PowerStack\cite{PowerStack}, and served as the platform for deploying and testing \textit{Tycho}. Access was via VPN and SSH within the university network.

\subsection{Build Toolchain}
\label{sec:tycho_sysenv_build}

Two complementary workflows are used: a dev path (local build, run directly on a node for interactive debugging) and a deploy path (build a container image, push to GHCR, deploy as a privileged DaemonSet via \textit{PowerStack}).

\subsubsection{Local builds}
\label{subsec:tycho_sysenv_build_local}
The implementation language is Go, using \code{go version go1.25.1} on \code{linux/amd64}. The \code{Makefile} orchestrates routine tasks. The target \code{make build} compiles the exporter into \path{_output/bin/<os>_<arch>/kepler}. Targets for cross builds are available for \code{linux/amd64} and \code{linux/arm64}. The build injects version information at link time through \code{LDFLAGS} including the source version, the revision, the branch, and the build platform. This supports traceability when binaries or images are compared during experiments.

\subsubsection{Container images}
\label{subsec:tycho_sysenv_build_images}
Container builds use Docker Buildx with multi arch output for \code{linux/amd64} and \code{linux/arm64}. Images are pushed to the GitHub Container Registry under the project repository. For convenience there are targets that build a base image and optional variants that enable individual software components when required. 

\subsubsection{Continuous integration}
\label{subsec:tycho_sysenv_build_ci}
GitHub Actions produces deterministic images with an immutable commit-encoded tag, a time stamped dev tag, and a latest for \code{main}. Builds are triggered on pushes to the main branches and on demand. Buildx cache shortens builds without affecting reproducibility.

\subsubsection{Versioning and reproducibility}
\label{subsec:tycho_sysenv_versioning}
Development proceeds on feature branches with pull requests into \code{main}. Release images are produced automatically for commits on \code{main}. Development images are produced for commits on \code{dev} and for feature branches when needed. Dependency management uses Go modules with a populated \path{vendor/} directory. The files \path{go.mod} and \path{go.sum} pin the module versions, and \code{go mod vendor} materializes the dependency tree for offline builds. 

\subsection{Debugging Environment}
\label{sec:tycho_sysenv_debug}
The debugger used for \textit{Tycho} is \textbf{Delve} in headless mode with a Debug Adapter Protocol listener. This provides a stable front end for interactive sessions while the debugged process runs on the target node. Delve was selected because it is purpose built for Go, supports remote attach, and integrates reliably with common editors without altering the build configuration beyond standard debug symbols.

\subsubsection{Remote debugging setup}
\label{subsec:tycho_sysenv_debug_remote}
Debug sessions are executed on a Kubernetes worker node. The exporter binary is started under Delve in headless mode with a DAP listener on a dedicated TCP port. The workstation connects over an authenticated channel. In practice an SSH tunnel is used to forward the listener port from the node to the workstation. This keeps the debugger endpoint inaccessible from the wider network and avoids additional access controls on the cluster. To prevent metric interference the node used for debugging excludes the deployed DaemonSet, so only the debug instance is active on that host.

\subsubsection{Integration with the editor}
\label{subsec:tycho_sysenv_debug_ide}
The editor is configured to attach through the Debug Adapter Protocol. In practice a minimal launch configuration points the adapter at the forwarded listener. Breakpoints, variable inspection, step control, and log capture work without special handling. No container specific extensions are required because the debugged process runs directly on the node.

The editor attaches over the SSH-forwarded DAP port; the inner loop is build locally with \code{make}, launch under Delve with a DAP listener, attach via SSH, inspect, adjust, repeat. When the goal is to validate behavior in a cluster setting rather than to step through code, the deploy oriented path is used instead. In that case the image is built and pushed, and observation relies on logs and metrics rather than an attached debugger.

\subsubsection{Limitations and challenges}
\label{subsec:tycho_sysenv_debug_limits}
Headless remote debugging introduces some constraints. Interactive sessions depend on network reachability and an SSH tunnel, which adds a small amount of latency. The debugged process must retain the privileges needed for eBPF and access to hardware counters, which narrows the choice of where to run sessions on multi tenant systems. Running a second exporter in parallel on the same node would distort measurements, which is why the DaemonSet is excluded on the debug host. Container based debugging is possible but less convenient given the need to coordinate with cluster security policies. For these reasons, most active debugging uses a locally built binary that runs directly on the node, while container based deployments are reserved for integration tests and evaluation runs.

\subsection{Supporting Tools and Utilities}
\label{sec:tycho_sysenv_util}

\subsubsection{Configuration and local orchestration}
\label{subsec:tycho_sysenv_util_config}
A lightweight configuration file \path{config.yaml} consolidates development toggles that influence local runs and selective deployment. Repository scripts read this file and translate high level options into concrete command line flags and environment variables for the exporter and for auxiliary processes. This keeps day to day operations consistent without editing manifests or code, and aligns with the two workflows in \S~\ref{sec:tycho_sysenv_build}. Repository scripts map configuration keys to explicit flags for local runs, debug sessions, and ad hoc deploys.

\subsubsection{Container, cluster, and monitoring utilities}
\label{subsec:tycho_sysenv_util}
Supporting tools: Docker, kubectl, Helm, k3s, Rancher, Ansible, Prometheus, Grafana. Each is used only where it reduces friction, for example Docker for image builds, kubectl for interaction, and Prometheus/Grafana for observability.

\subsection{Relevance and Limitations}
\label{sec:tycho_sysenv_relevance}

\subsubsection{Scope and contribution}
\label{subsec:tycho_sysenv_relevance_scope}
The development, build, and debugging environment described in \S~\ref{sec:tycho_sysenv_build} and \S~\ref{sec:tycho_sysenv_debug} is enabling infrastructure rather than a scientific contribution. Its purpose is to make modifications to \textit{Tycho} feasible and to support evaluation, not to advance methodology in software engineering or tooling.

Documenting the environment serves reproducibility and auditability. A reader can verify that results were obtained on bare-metal with access to the required telemetry, and can reconstruct the build pipeline from source to binary and container image. The references to the repository at the start of this section in \S~\ref{sec:tycho_sysenv} provide the operational detail that is intentionally omitted from the main text.

\subsubsection{Boundaries and omissions}
\label{subsec:tycho_sysenv_relevance_bounds}
Installation steps, editor-specific configuration, system administration, security hardening, and multi tenant policy are out of scope; concrete commands live in the repository. Where concrete commands matter for reproducibility they are available in the repository documentation cited in \S~\ref{sec:tycho_sysenv}.