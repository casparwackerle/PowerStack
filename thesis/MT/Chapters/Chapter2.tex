\chapter{Background and Related Research}
\label{ch:background}

% This chapter surveys existing research and practical knowledge about server-level
% energy measurement, hardware and software telemetry sources, and attribution tools.
% It provides the empirical and factual basis for the conceptual foundations in
% Chapter~\ref{ch:concepts}. This chapter MUST NOT contain Tycho design decisions,
% conceptual attribution models, or implementation details.

\noindent This chapter summarises the current state of research and industrial knowledge on server-level energy measurement. Its focus is limited to what the literature reports about available telemetry sources, measurement techniques, and existing attribution tools. The discussion is descriptive rather than conceptual: it does not introduce attribution principles, methodological reasoning, or design considerations, which are addressed in \chapref{ch:concepts}. Extended background material is available in \hyperref[appendix_vt2]{Appendix~A} and the present chapter integrates only those findings that are directly relevant for understanding the research landscape.

\section{Energy Measurement in Modern Server Systems}
\label{sec:energy_measurement_systems}

The energy consumption of modern servers arises from a heterogeneous set of subsystems, including CPUs, GPUs, memory, storage devices, network interfaces, and platform management components. Prior research highlights that these subsystems expose highly unequal visibility into their power behaviour, since measurement capabilities, granularity, and accuracy differ significantly across hardware generations and vendors \parencite{lin2020taxonomy, long2022review}. Some domains provide direct telemetry, while others can only be approximated through software-derived activity metrics. As a result, no single interface offers complete or temporally consistent power information, and most studies rely on a single source or combine multiple sources to approximate system-level consumption. This fragmented measurement landscape forms the basis for much of the existing work on power modelling, validation, and multi-source energy estimation in server environments.

\subsection{Energy Attribution in Multi-Tenant Environments}
\label{subsec:attribution_context}

Several studies identify containerised and multi-tenant systems as challenging environments for energy attribution. Containers share the host kernel and rely on common processor, memory, storage, and network subsystems, which removes the isolation boundaries present in virtual machines and prevents direct measurement of per-container power. Research reports that workloads running concurrently on the same node create interference effects across hardware domains, leading to utilisation patterns that correlate only loosely with actual energy consumption \parencite{lin2020taxonomy}. Modern orchestration platforms further increase attribution difficulty through highly dynamic execution behaviour: containers are created, destroyed, and rescheduled at high frequency, often numbering in the thousands on large clusters. These rapid lifecycle changes produce volatile metadata and short-lived resource traces that are difficult to align with node-level telemetry. Collectively, the literature treats container-level energy attribution as an estimation problem constrained by incomplete observability, heterogeneous measurement quality, and continuous runtime churn.

\subsection{Telemetry Layers in Contemporary Architectures}
\label{subsec:telemetry_layers}

Modern servers expose power and activity information through two largely independent telemetry layers. The first consists of in-band mechanisms that are visible to the operating system, including on-die energy counters, GPU management interfaces, and kernel-level resource statistics. These interfaces typically offer higher sampling rates and finer granularity, but their accuracy and coverage vary across hardware generations and vendors. Prior work notes that in-band telemetry often represents estimated rather than directly measured power and that several domains, such as network and storage devices, expose only partial or indirect information.

The second layer is out-of-band telemetry provided by baseboard management controllers through interfaces such as IPMI or Redfish. These systems aggregate sensor readings independently of the host and report stable, whole-system power values at coarse temporal resolution. Empirical studies show that out-of-band telemetry provides useful system-level accuracy, although update intervals and measurement precision differ substantially between vendors \parencite{wang2019empirical}. Compared with instrument-based measurements, which remain the benchmark for high-fidelity evaluation but are impractical at scale, both in-band and out-of-band methods represent trade-offs between granularity, availability, and measurement reliability.

Combined, these layers form a heterogeneous telemetry landscape in which sampling rates, accuracy, and domain coverage differ significantly, motivating the use of multi-source measurement approaches in research.

\subsection{Challenges for Container-Level Measurement}
\label{subsec:container_challenges}

Existing research identifies several factors that complicate accurate energy measurement for containerised workloads. Large-scale trace analyses show that cloud environments exhibit substantial churn, with many tasks being short-lived and resource demands changing rapidly over time \parencite{reissHeterogeneityDynamicityClouds2012}. Such dynamism limits the observability of fine-grained resource usage and makes it difficult to capture short execution intervals with sufficient temporal resolution.

Monitoring studies further report inconsistencies across the different layers that expose resource information for containers. In multi-cloud settings, observability often depends on heterogeneous monitoring stacks, leading to fragmented visibility and non-uniform coverage of system activity \parencite{waseemContainerizationMultiCloudEnvironment2025}. Even within a single host, performance counters obtained from container-level interfaces may diverge from system-level measurements. Empirical evaluations demonstrate that container-level CPU and I/O counters can underestimate actual activity by a non-negligible margin, and that co-located workloads introduce contention effects that distort these metrics \parencite{casalicchioStateoftheartContainerTechnologies2020}.

These findings indicate that container-level measurement operates under conditions of rapid workload turnover, heterogeneous monitoring behaviour, and imperfect resource visibility. As a consequence, the literature treats container energy attribution as a problem constrained by incomplete and potentially biased measurement signals rather than as a directly measurable quantity.

%====================================================================
\section{Hardware and Software Telemetry Sources}
\label{sec:measurement_techniques}
\appchapterref{A}{vt2_Chapter2} provides a detailed analysis of this topic; the present section summarizes the key research findings.

% This is one of the main sections (3–4 pages).
% It must describe WHAT telemetry exists and WHAT research says about it.
% It must NOT explain conceptual interpretations of these sources (Chapter 3).

%--------------------------------------------------------------------
\subsection{Direct Hardware Measurement}
\label{subsec:direct_measurement}

% Placeholder (0.3 page):
% - Summarise research on physical instrumentation (power meters, shunts).
% - Emphasise commissioning cost, impracticality, and limited real-world use.
% - DO NOT go into electrical engineering detail or conceptual attribution.
% - Keep concise.

%--------------------------------------------------------------------
\subsection{Legacy Telemetry Interfaces (ACPI, IPMI)}
\label{subsec:legacy_telemetry}

% Placeholder (0.3 page):
% - Explain that ACPI only reports states, not power values.
% - Mention IPMI as predecessor to Redfish.
% - DO NOT describe ACPI concepts such as C/P states (Chapter 3).
% - Keep strictly factual and empirical.

%--------------------------------------------------------------------
\subsection{Redfish Power Telemetry}
\label{subsec:redfish}

% Placeholder (1 page):
% - Describe Redfish power reporting based on literature:
%   update intervals, precision, multi-second staleness.
% - Discuss vendor variability and research findings on accuracy.
% - Describe strengths: reliable whole-node power.
% - Describe limitations: insufficient granularity for short workloads.
% - DO NOT describe conceptual use of external telemetry (Chapter 3).
% - DO NOT mention Tycho's polling model.

%--------------------------------------------------------------------
\subsection{RAPL Power Domains}
\label{subsec:rapl}

% Placeholder (1.5–2.0 pages):
% - Describe RAPL domains as reported in research:
%   package, core, uncore, DRAM (and AMD’s reduced support).
% - Summarise validation studies (accuracy, limitations).
% - Discuss research observations about domain ambiguity and idle-state issues.
% - Mention access mechanisms (MSR, powercap) only at high level.
% - DO NOT describe conceptual domain meaning or attribution implications 
%   (Chapter 3).
% - DO NOT discuss sampling theory or timing behaviour (Chapter 3).

%--------------------------------------------------------------------
\subsection{GPU Telemetry (NVML)}
\label{subsec:nvml}

% Placeholder (1–1.5 pages):
% - Describe NVML power/usage metrics, internal averaging, sampling delays.
% - Summarise research on accuracy and limitations, including MIG ambiguity.
% - DO NOT explain conceptual GPU power-domain behaviour (Chapter 3).
% - DO NOT mention Tycho's GPU-specific approach.

%--------------------------------------------------------------------
\subsection{Software-Exposed Resource Metrics}
\label{subsec:software_metrics}

% This merged section replaces three previous subsections.

% Placeholder (1 page):
% - Summarise kernel-exposed CPU time accounting, I/O statistics, network counters.
% - Mention missing or unusable telemetry for NICs, storage, PCIe.
% - Describe how literature treats model-based estimation (regression, AI models).
% - Emphasise limitations such as missing domains and poor applicability.
% - DO NOT explain conceptual interpretation of resource counters 
%   (reserved for Chapter 3).
% - DO NOT mix with attribution philosophies.

%====================================================================
\section{Temporal Behaviour of Telemetry Sources}
\label{sec:temporal_behaviour}
A comprehensive treatment can be found in \appchapterref{A}{vt2_Chapter2}, but the following discussion focuses on the aspects relevant for the present thesis.

% This section synthesises multi-source timing behaviour based on empirical research.
% It must NOT introduce conceptual timing theory (Chapter 3).

%--------------------------------------------------------------------
\subsection{Sampling Characteristics and Update Cycles} --------->> MERGE WITH NEXT SECTION:
\label{subsec:temporal_sampling}

% Placeholder (0.5–0.7 page):
% - Describe research findings on RAPL, NVML, Redfish update intervals.
% - Include empirical observations of nondeterministic refresh times.
% - DO NOT explain sampling vs event-time (Chapter 3).
% - DO NOT introduce timing models.

%--------------------------------------------------------------------
\subsection{Sensor-Internal Averaging and Missing Timestamps}
\label{subsec:averaging_timestamps}

% Placeholder (0.5 page):
% - Summarise research showing lack of timestamps in RAPL and NVML.
% - Mention internal sensor averaging reported in literature.
% - DO NOT describe conceptual implications for attribution (Chapter 3).

%--------------------------------------------------------------------
\subsection{Domain Boundary Ambiguity}
\label{subsec:domain_ambiguity}

% Placeholder (0.5 page):
% - Summarise research showing uncertain boundaries of RAPL domains.
% - Mention MIG-related ambiguity for GPU domains.
% - DO NOT explain conceptual importance of domains (Chapter 3).

%--------------------------------------------------------------------
\subsection{Validation Methodologies in Prior Research}
\label{subsec:validation_methods}

% Placeholder (0.5 page):
% - Summarise how prior studies validated measurements (meters, cross-domain comparison).
% - Discuss known limitations.
% - DO NOT discuss how validation will be performed in this thesis.
% - Keep strictly historical/literature-based.

%====================================================================
\section{Existing Tools and Related Work}
\label{sec:related_tools}
\appchapterref{A}{vt2_Chapter4}
% Expected length: 3–4 pages.
% DO NOT discuss Tycho or Tycho architecture.
% DO summarise empirical findings from prior work (Kepler, Kubewatt, others).

%--------------------------------------------------------------------
\subsection{General Tools (Brief Overview)} ->>>>In “Existing Tools,” potentially merge “General Tools” + short mentions of other tools into a single paragraph
\label{subsec:general_tools}

% Placeholder (0.3 page):
% - Briefly summarise Scaphandre, Smartwatts, and 1–2 other tools.
% - Emphasise their measurement scope and known limitations.
% - DO NOT provide conceptual attribution explanation (Chapter 3).
% - Keep this high-level and concise.

%--------------------------------------------------------------------
\subsection{Kepler}
\label{subsec:kepler}

% Placeholder (1.5–2.0 pages):
% - Describe Kepler’s architecture based purely on literature:
%   * metric sources
%   * ratio-based attribution strategy
%   * sampling behaviour
% - Summarise empirical misattribution issues identified by research and VT2:
%   * idle power artefacts
%   * latency mismatch
%   * short-lived workload instability
%   * system-process handling issues
% - Emphasise Kepler's shift toward deployability (literature statements).
% - DO NOT mention Tycho, Tycho improvements, or motivation for Tycho.

%--------------------------------------------------------------------
\subsection{Kubewatt}
\label{subsec:kubewatt}

% Placeholder (1.0–1.5 pages):
% - Summarise Kubewatt’s findings validating Kepler:
%   * incorrect idle attribution
%   * metadata/race-condition problems
%   * visibility issues with completed pods
% - Summarise Kubewatt’s improvements:
%   * static vs dynamic separation
%   * simplified proportional models
% - Summarise Kubewatt’s limitations as reported in thesis:
%   * missing domains
%   * limited metric scope
%   * no calibration
% - DO NOT reference Tycho or compare to Tycho.

%====================================================================
\section{Research Gaps}
\label{sec:research_gaps}

% This section synthesises gaps identified throughout Chapter 2.
% ABSOLUTELY NO Tycho discussion.
% NO conceptual discussion (that belongs to Chapter 3).

%--------------------------------------------------------------------
\subsection{Gap 1: Temporal Misalignment}
\label{subsec:gap_temporal}

% Placeholder (0.5 page):
% - Integrate research showing misaligned update intervals and delays.
% - DO NOT discuss strategies for alignment (Chapter 3 or 4).

%--------------------------------------------------------------------
\subsection{Gap 2: Missing Timestamps and Averaging}
\label{subsec:gap_timestamps}

% Placeholder (0.3 page):
% - Summarise lack of timestamps and internal averaging as documented by research.

%--------------------------------------------------------------------
\subsection{Gap 3: Domain Boundary Ambiguity}
\label{subsec:gap_boundaries}

% Placeholder (0.3–0.5 page):
% - Integrate findings on poorly documented boundaries (RAPL, MIG).

%--------------------------------------------------------------------
\subsection{Gap 4: Metadata-Lifecycle Inconsistencies}
\label{subsec:gap_metadata}

% Placeholder (0.3–0.5 page):
% - Synthesise research showing cgroup/pod lifecycle inconsistencies.

%--------------------------------------------------------------------
\subsection{Gap 5: Idle Power Attribution Issues}
\label{subsec:gap_idle}

% Placeholder (0.3 page):
% - Highlight known issues in idle power handling.

%--------------------------------------------------------------------
\subsection{Gap 6: Limited Multi-Domain Integration}
\label{subsec:gap_multidomain}

% Placeholder (0.3 page):
% - Summarise research noting lack of integrated CPU–GPU–system attribution.

%--------------------------------------------------------------------
\subsection{Gap 7: Missing Calibration and Uncertainty Treatment}
\label{subsec:gap_calibration}

% Placeholder (0.3–0.5 page):
% - Summarise research calling for calibration or uncertainty-aware methods.

%====================================================================
\section{Summary}
\label{sec:background_summary}

% Placeholder (0.5 page):
% - Summarise fundamental insights: heterogeneous telemetry behaviour, 
%   incomplete domain coverage, lifecycle inconsistency, lack of calibration.
% - Prepare the reader for Chapter 3, stating explicitly that Chapter 3
%   provides the conceptual foundations needed to understand the design 
%   of an accuracy-oriented system (without yet describing Tycho).

