\chapter{Background and Related Research}
\label{ch:background}

% This chapter surveys existing research and practical knowledge about server-level
% energy measurement, hardware and software telemetry sources, and attribution tools.
% It provides the empirical and factual basis for the conceptual foundations in
% Chapter~\ref{ch:concepts}. This chapter MUST NOT contain Tycho design decisions,
% conceptual attribution models, or implementation details.

\noindent This chapter summarises the current state of research and industrial knowledge on server-level energy measurement. Its focus is limited to what the literature reports about available telemetry sources, measurement techniques, and existing attribution tools. The discussion is descriptive rather than conceptual: it does not introduce attribution principles, methodological reasoning, or design considerations, which are addressed in \chapref{ch:concepts}. Extended background material is available in \hyperref[appendix_vt2]{Appendix~A} and the present chapter integrates only those findings that are directly relevant for understanding the research landscape.

\section{Energy Measurement in Modern Server Systems}
\label{sec:energy_measurement_systems}

The energy consumption of modern servers arises from a heterogeneous set of subsystems, including CPUs, GPUs, memory, storage devices, network interfaces, and platform management components. Prior research highlights that these subsystems expose highly unequal visibility into their power behaviour, since measurement capabilities, granularity, and accuracy differ significantly across hardware generations and vendors \parencite{lin2020taxonomy, long2022review}. Some domains provide direct telemetry, while others can only be approximated through software-derived activity metrics. As a result, no single interface offers complete or temporally consistent power information, and most studies rely on a single source or combine multiple sources to approximate system-level consumption. This fragmented measurement landscape forms the basis for much of the existing work on power modelling, validation, and multi-source energy estimation in server environments.

\subsection{Energy Attribution in Multi-Tenant Environments}
\label{subsec:attribution_context}

Several studies identify containerised and multi-tenant systems as challenging environments for energy attribution. Containers share the host kernel and rely on common processor, memory, storage, and network subsystems, which removes the isolation boundaries present in virtual machines and prevents direct measurement of per-container power. Research reports that workloads running concurrently on the same node create interference effects across hardware domains, leading to utilisation patterns that correlate only loosely with actual energy consumption \parencite{lin2020taxonomy}. Modern orchestration platforms further increase attribution difficulty through highly dynamic execution behaviour: containers are created, destroyed, and rescheduled at high frequency, often numbering in the thousands on large clusters. These rapid lifecycle changes produce volatile metadata and short-lived resource traces that are difficult to align with node-level telemetry. Collectively, the literature treats container-level energy attribution as an estimation problem constrained by incomplete observability, heterogeneous measurement quality, and continuous runtime churn.

\subsection{Telemetry Layers in Contemporary Architectures}
\label{subsec:telemetry_layers}

Modern servers expose power and activity information through two largely independent telemetry layers. The first consists of in-band mechanisms that are visible to the operating system, including on-die energy counters, GPU management interfaces, and kernel-level resource statistics. These interfaces typically offer higher sampling rates and finer granularity, but their accuracy and coverage vary across hardware generations and vendors. Prior work notes that in-band telemetry often represents estimated rather than directly measured power and that several domains, such as network and storage devices, expose only partial or indirect information.

The second layer is out-of-band telemetry provided by baseboard management controllers through interfaces such as IPMI or Redfish. These systems aggregate sensor readings independently of the host and report stable, whole-system power values at coarse temporal resolution. Empirical studies show that out-of-band telemetry provides useful system-level accuracy, although update intervals and measurement precision differ substantially between vendors \parencite{wang2019empirical}. Compared with instrument-based measurements, which remain the benchmark for high-fidelity evaluation but are impractical at scale, both in-band and out-of-band methods represent trade-offs between granularity, availability, and measurement reliability.

Combined, these layers form a heterogeneous telemetry landscape in which sampling rates, accuracy, and domain coverage differ significantly, motivating the use of multi-source measurement approaches in research.

\subsection{Challenges for Container-Level Measurement}
\label{subsec:container_challenges}

Existing research identifies several factors that complicate accurate energy measurement for containerised workloads. Large-scale trace analyses show that cloud environments exhibit substantial churn, with many tasks being short-lived and resource demands changing rapidly over time \parencite{reissHeterogeneityDynamicityClouds2012}. Such dynamism limits the observability of fine-grained resource usage and makes it difficult to capture short execution intervals with sufficient temporal resolution.

Monitoring studies further report inconsistencies across the different layers that expose resource information for containers. In multi-cloud settings, observability often depends on heterogeneous monitoring stacks, leading to fragmented visibility and non-uniform coverage of system activity \parencite{waseemContainerizationMultiCloudEnvironment2025}. Even within a single host, performance counters obtained from container-level interfaces may diverge from system-level measurements. Empirical evaluations demonstrate that container-level CPU and I/O counters can underestimate actual activity by a non-negligible margin, and that co-located workloads introduce contention effects that distort these metrics \parencite{casalicchioStateoftheartContainerTechnologies2020}.

These findings indicate that container-level measurement operates under conditions of rapid workload turnover, heterogeneous monitoring behaviour, and imperfect resource visibility. As a consequence, the literature treats container energy attribution as a problem constrained by incomplete and potentially biased measurement signals rather than as a directly measurable quantity.


\section{Hardware and Software Telemetry Sources}
\label{sec:measurement_techniques}

This section outlines the primary telemetry sources used to observe power and resource behaviour in modern server systems. It summarises established research on external measurement devices, firmware-level interfaces, on-die energy counters, accelerator telemetry, and kernel-exposed resource metrics. The emphasis is on reporting the properties and empirical characteristics documented in prior work, without interpreting these signals conceptually or analysing their temporal behaviour, which are addressed in later sections. A comprehensive technical discussion is provided in \appchapterref{A}{vt2_Chapter2}; the present section extracts only the findings relevant for understanding the measurement landscape.

\subsection{Direct Hardware Measurement}
\label{subsec:direct_measurement}

Direct physical instrumentation remains the most accurate method for measuring server power consumption. External power meters or inline shunt-based devices can capture node-level energy usage with high fidelity, and research frequently uses such instrumentation as a ground truth for validating software-reported power values. Studies employing dedicated measurement setups, such as custom DIMM-level sensing boards, demonstrate that high-frequency sampling and component-level granularity are technically feasible but require bespoke hardware and non-trivial integration effort \parencite{desrochers2016validation}. Lin et al.\ classify these approaches as offering very high data credibility but only coarse spatial granularity and limited scalability in operational environments \parencite{lin2020taxonomy}.

Recent work on specialised sensors, such as the PowerSensor3 platform\parencite{van2025powersensor3} for high-rate voltage and current monitoring of GPUs and other accelerators, illustrates ongoing interest in hardware-centric power measurement. However, these systems share the same fundamental drawback: deployment across production servers is complex, costly, and incompatible with large-scale or multi-tenant settings. As a consequence, direct instrumentation is predominantly used in controlled experiments or for validation of other telemetry sources, rather than as a primary measurement mechanism in real-world server infrastructures.

\subsection{Legacy Telemetry Interfaces (ACPI, IPMI)}
\label{subsec:legacy_telemetry}

Early power-related telemetry on server platforms was primarily exposed through ACPI and IPMI. ACPI provides a standardised interface for configuring and controlling hardware power states, but it does not offer real-time energy or power readings. The interface exposes only abstract performance and idle states defined by the firmware \parencite{uefi_acpi_6_6}, and these states do not include the instantaneous power information required for empirical energy measurement. Consequently, ACPI has seen little use in modern power estimation research.

IPMI, accessed through the baseboard management controller, represents an older class of out-of-band telemetry that predates Redfish. Although widely supported across server hardware, IPMI power values are known to be coarse, slowly refreshed, and often inaccurate when compared with external instrumentation. Empirical studies report multi-second averaging windows, substantial quantisation effects, and unreliable idle power readings \parencite{kavanagh2016accuracy, kavanagh2019rapid}. These limitations, together with the availability of more precise alternatives, have led IPMI to be largely superseded by Redfish on contemporary server platforms.

\subsection{Redfish Power Telemetry}
\label{subsec:redfish}

Redfish is the modern out-of-band management interface available on contemporary server platforms and is designed as the successor to IPMI. It exposes system-level telemetry through a RESTful API implemented on the baseboard management controller (BMC), providing access to whole-node power readings derived from on-board sensors. Prior work consistently shows that Redfish delivers higher precision than IPMI, with lower quantisation artefacts and more stable readings across power ranges \parencite{wang2019empirical}. In controlled experiments, Redfish achieved a mean absolute percentage error of roughly three percent when compared to a high-accuracy power analyser, outperforming IPMI in all evaluated power intervals.

A key limitation of Redfish is its temporal granularity. Empirical studies report that power values exhibit non-negligible staleness, with refresh delays of approximately 200\,ms \parencite{wang2019empirical}. This latency restricts the ability of Redfish to capture short bursts of activity or rapid fluctuations in dynamic workloads. Accuracy and responsiveness also vary across vendors, reflecting differences in embedded sensors, BMC firmware, and management controller architectures.

The interface is widely deployed in real-world infrastructure. Modern enterprise servers from Dell, HPE, Lenovo, Cisco, and Supermicro routinely expose power telemetry via Redfish as part of their standard BMC firmware \parencite{herrlinAccessingOnboardServer2021}. Out-of-band monitoring studies further highlight that Redfish avoids the overheads and failure modes associated with in-band agents \parencite{aliRedfishNagiosScalableOutofBand2022}. In practice, Redfish implementations tend to provide stable low-frequency updates suitable for coarse-grained power reporting.

Preliminary measurements conducted for this thesis also observed irregular update intervals on the evaluated hardware, occasionally extending into the multi-second range. While this behaviour is specific to a single system and not generalisable, it reinforces the literature’s position that Redfish telemetry exhibits meaningful vendor-dependent variability and remains unsuitable for fine-grained temporal correlation.

Overall, Redfish provides accessible, reliable whole-node power telemetry at coarse temporal resolutions, making it valuable for long-interval monitoring and for validating other measurement sources, but inappropriate for attributing energy consumption to short-lived or rapidly fluctuating containerised workloads.

\subsection{RAPL Power Domains}
\label{subsec:rapl}

Running Average Power Limit (RAPL) provides hardware-backed energy counters for several internal power domains of a processor package. Originally introduced by Intel and later adopted in a compatible form by AMD, RAPL exposes energy measurements via model-specific registers that can be accessed directly or through higher-level interfaces such as the Linux \texttt{powercap} framework or the \texttt{perf-events} subsystem \parencite{intel-sdm, raffin2024dissecting}. Raffin et~al.\ provide a detailed comparison of these access mechanisms, noting that MSR, powercap, perf-events, and eBPF differ mainly in convenience, required privileges, and robustness; all can retrieve equivalent RAPL readings when implemented correctly \parencite{raffin2024dissecting}. They recommend accessing RAPL via the powercap interface, which is easiest to implement reliably and suffers from no overhead penalties when compared with more low-level methods.

Intel platforms typically expose several well-established RAPL domains, including the processor package, the core subsystem, and (on many server architectures) a DRAM domain \parencite{hackenberg2015energy}. These domains have been validated extensively against external measurement equipment. Studies report that the combination of package and DRAM energy tracks CPU-and-memory power with good accuracy from Haswell onwards, which has led to RAPL becoming the primary fine-grained energy source in server-oriented research \parencite{hackenberg2013power, desrochers2016validation, alt2024experimental, kennes2023measuring}. More recent work on hybrid architectures such as Alder Lake confirms that RAPL continues to correlate well with external measurements under load, while precision decreases somewhat in low-power regimes \parencite{schone2024energy}. Across these studies, RAPL is generally regarded as sufficiently accurate for scientific analysis when its domain boundaries and update characteristics are considered \parencite{raffin2024dissecting}.

AMD implements a RAPL-compatible interface with a similar programming model but a reduced set of domains. Zen 1 through Zen 4 processors expose package and core domains only, without a dedicated DRAM domain \parencite{schone2021energy, raffin2024dissecting}. Sch\"one et~al.\ show that, as a consequence, memory-related energy may not be represented explicitly in AMD’s RAPL output, leading to a smaller portion of total system energy being observable through the package domain alone \parencite{schone2021energy}. This limitation primarily concerns domain completeness rather than measurement correctness: for compute-intensive workloads, package-domain values behave consistently, but workloads with significant memory activity exhibit a larger gap relative to whole-system measurements because DRAM energy is not separately reported. Raffin et~al.\ further note that, on the evaluated Zen-based server, different kernel interfaces initially exposed inconsistent domain sets; this was later corrected upstream, illustrating that AMD support is evolving and still maturing within the Linux ecosystem \parencite{raffin2024dissecting}.

Technical considerations also apply to both Intel and AMD platforms. RAPL counters have finite width and wrap after sufficiently large energy accumulation, requiring consumers to implement overflow correction \parencite{khan2018rapl, raffin2024dissecting}. The counters do not include timestamps, and empirical work shows that actual update intervals may deviate from nominal values, complicating precise temporal correlation with other telemetry \parencite{hackenberg2013power, jay2023experimental}. On some Intel platforms, security hardening measures such as energy filtering reduce temporal granularity for certain domains to mitigate side-channel risks \parencite{lipp2021platypus, intel2023, schone2024energy}. In virtualised environments, RAPL access may be trapped by the hypervisor, increasing latency and introducing small deviations from bare-metal behaviour \parencite{jay2023experimental}.

In summary, RAPL provides a widely used and comparatively fine-grained source of processor-side energy telemetry. Intel platforms typically offer multiple validated domains, including DRAM, enabling a broader view of CPU-and-memory energy. AMD platforms expose fewer domains and therefore provide a more limited perspective on total system power, particularly for memory-intensive workloads. These differences in domain coverage, measurement scope, and software integration need to be taken into account when using RAPL as a basis for energy analysis.

\subsection{GPU Telemetry}
\label{subsec:gpu_telemetry}

Unlike CPUs, where power and utilization telemetry is supported through standardised
interfaces, GPU energy visibility relies primarily on vendor-specific mechanisms.
For NVIDIA devices, two interfaces dominate this landscape: the \textit{NVIDIA Management
Library} (NVML), which has become the industry standard, and the \textit{Data Center GPU
Manager} (DCGM), a less widely used management layer that also exposes telemetry.

\subsubsection{NVML}
NVML is NVIDIA’s primary interface for device-level monitoring and underpins tools such
as \texttt{nvidia-smi}.  
It provides access to power, energy (on selected data-center GPUs), GPU utilization,
memory usage, clock frequencies, thermal state, and various health and throttle
indicators.  
Among these, power and utilization are most relevant for energy analysis.

NVML power values represent board-level estimates derived from on-device sensing
circuits and are shaped by internal averaging and architecture-dependent update
behaviour.  
Recent empirical studies across modern devices show that NVML produces fresh samples only
intermittently and applies smoothing that reduces the visibility of short-lived power
changes, while steady-state power levels remain comparatively accurate
\parencite{yang2024accurate}.  
On the Grace--Hopper GH200, these effects are pronounced: NVML reflects a coarse internal
averaging interval and therefore underrepresents short kernels and transient peaks
relative to higher-frequency system interfaces
\parencite{hernandezPreliminaryStudyFineGrained2025}.  
These findings indicate that NVML captures long-term power behaviour reliably but
inherently limits fine-grained visibility.  
Despite these constraints, existing studies consistently find that NVML provides
reasonably accurate steady-state power estimates on modern data-center GPUs and currently
represents the most reliable and widely supported mechanism for obtaining GPU power
telemetry in practical systems \parencite{hernandezPreliminaryStudyFineGrained2025}.

GPU utilization provides contextual information about device activity.  
It reports the proportion of time during which the GPU is executing any workload rather
than the fraction of computational capacity in use, making it a coarse activity
indicator rather than a detailed performance metric
\parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{DCGM}
DCGM is NVIDIA’s management and observability framework designed for data-center
deployments.  
It aggregates telemetry, performs health monitoring, exposes thermal and throttle state,
and provides detailed visibility in environments that employ Multi-Instance GPU (MIG)
partitioning.  
However, DCGM’s power and utilization metrics are derived from the same underlying
measurement sources as NVML.  
In practice, DCGM is far less commonly used for energy analysis because it does not
provide higher-fidelity power telemetry; instead, it applies additional aggregation and
is typically deployed with coarse sampling intervals, especially when used through
exporters in cluster monitoring systems.  
DCGM therefore represents an alternative access path to the same measurements rather than
a distinct source of energy-related information.  

DCGM is considerably less common in both research and operational practice, with most
GPU monitoring systems relying primarily on NVML while DCGM appears only occasionally in
cluster-level deployments \parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{Summary}
NVML and DCGM jointly define the available mechanisms for GPU telemetry in cloud
environments.  
NVML is the dominant and broadly supported interface for power and utilization
measurement, while DCGM extends it with operational metadata and management integration.
Current studies consistently show that both interfaces expose averaged, device-level
power estimates that capture long-term behaviour but are inherently limited in their
ability to represent short-duration activity or fine-grained workload structure.  
These characteristics form the scientific foundation for later discussions of temporal
behaviour and measurement methodology.

\subsection{Software-Exposed Resource Metrics}
\label{subsec:software_metrics}

In addition to hardware telemetry, Linux and Kubernetes expose a wide range of
software-level resource metrics that describe system and workload activity.
These metrics do not measure power directly but provide essential behavioural
context that complements RAPL, Redfish, and GPU telemetry.

\subsubsection{CPU and Memory Activity Metrics}

Linux provides several complementary mechanisms for tracking CPU and memory
usage.  
Global counters such as \path{/proc/stat} record cumulative CPU time since boot,
while per-task statistics in \path{/proc/<pid>} expose user-mode and kernel-mode
execution time with high granularity \parencite{kernelprocfs}.  
Control groups (cgroups) provide container-level CPU and memory accounting and
form the primary basis for utilisation metrics inside Kubernetes
\parencite{kernelcgroupv1, kernelcgroupv2}.  
Higher-level tools such as cAdvisor and metrics-server aggregate this
information via Kubelet, but at significantly lower update rates.

Event-driven approaches provide substantially finer resolution.  
eBPF allows dynamic attachment to kernel events such as context switches,
scheduling decisions, and I/O operations, enabling near-real-time capture of
per-task CPU activity with low overhead
\parencite{ciliumbpf, cassagnesRiseEBPFNonintrusive2020}.  
Hardware performance counters accessed through \code{perf} offer insight into
instruction counts, cycles, cache behaviour, and stalls
\parencite{Gregg2017CpuUtilizationWrong}.  
These sources provide detailed behavioural information but still represent
utilisation rather than energy.

\subsubsection{Storage Activity Metrics}

Storage subsystems do not expose real-time power telemetry, yet Linux provides a
rich set of activity indicators.  
Per-process statistics in \path{/proc/<pid>/io} track bytes read and written,
while cgroup I/O controllers report aggregated container-level metrics.  
Subsystem-specific tools such as \code{smartctl} and \code{nvme-cli} reveal
additional device characteristics, queue behaviour, and state transitions
\parencite{smartmontools_github, nvmecli_github}.  

In the absence of hardware power sensors, multiple works propose
workload-dependent energy models for storage devices
\parencite{choDesignTradeoffsSSDs2015, liWhichStorageDevice2014,
borbaModelingApproachEstimating2022}.  
These models can yield accurate estimates when calibrated for a specific device
but do not generalise across heterogeneous hardware due to differences in flash
controllers, firmware, and internal data paths.

\subsubsection{Network and PCIe Device Metrics}

Network interfaces provide byte and packet counters via \path{/proc/net/dev}, but
expose no dedicated power telemetry.  
Research models for NIC energy consumption exist
\parencite{sohanCharacterizing10Gbps2010, basmadjianCloudComputingIts2012,
baneshiAnalyzingPerApplicationEnergy2024}, yet all rely on device-specific idle
and active power characteristics that are not available at runtime.  
Similarly, PCIe devices support abstract power states as defined by the PCIe
specification \parencite{technotes_pci_power_2024}, but these states do not
reflect instantaneous power usage and thus offer only coarse activity signals.

\subsubsection{Secondary System Components}

Components such as fans, motherboard logic, and power delivery subsystems rarely
expose fine-grained telemetry.  
Although some BMC implementations report coarse sensor values, these readings
are inconsistent across platforms and generally unsuitable for high-resolution
analysis.  
Consequently, research commonly treats these subsystems as part of the residual
power that scales with the activity of primary components
\parencite{basmadjianCloudComputingIts2012}.

\subsubsection{Model-Based Estimation Approaches}

Because software-visible metrics capture detailed workload behaviour, many works
propose inferring energy consumption from utilisation using regression or
stochastic models
\parencite{fan2007power, hsu2011power, song2013unified,
arjonaarocaMeasurementbasedAnalysisEnergy2014}.  
While these models can be effective when fitted to a specific hardware platform,
their accuracy depends heavily on device-specific parameters, making them
unsuitable as a general mechanism for heterogeneous server environments.  
Machine-learning-based estimators share the same limitation: high accuracy when
trained for a fixed configuration, poor portability without extensive retraining.

\subsubsection{Summary}

Software-exposed metrics provide high-resolution visibility into CPU, memory,
I/O, and network activity.  
They are indispensable for correlating workload behaviour with hardware power
signals, especially for components that lack native telemetry. 
Model-based estimation remains possible but inherently platform-specific, and
therefore unsuitable as a universal foundation for fine-grained attribution in
heterogeneous environments.

\section{Temporal Behaviour of Telemetry Sources}
\label{sec:temporal_behaviour}

A comprehensive discussion of timing behaviour across hardware and software telemetry 
interfaces is provided in \appchapterref{A}{vt2_Chapter2}. 
The present section summarises the aspects most relevant for understanding the 
limitations of existing measurement sources and sets the empirical foundation for 
later discussions of attribution challenges.

%--------------------------------------------------------------------
\subsection{Update Cycles and Sampling Variability}
\label{subsec:temporal_sampling}

% PURPOSE OF THIS SUBSECTION:
% Provide an empirical synthesis of how often telemetry sources produce new values, 
% how predictable these updates are, and how this differs across technologies.
%
% MUST INCLUDE:
% - RAPL: extremely fast pollable interface, but stable updates only above ~20 Hz;
%         occasional zero-delta samples; heterogeneous domain update frequencies.
% - NVML: fixed internal update periods but unreliable sample freshness;
%         intermittent update availability; delays of 100–300 ms between physical 
%         change and reported update; architecture-dependent differences (Ampere, 
%         Hopper, GH200).
% - DCGM: often sampled at coarse intervals (default ~1 s); inherits NVML update 
%         behaviour.
% - Redfish: vendor-dependent behaviour; generally low update rates; empirically 
%            irregular intervals (sub-second to multi-second); occasional missing 
%            or skipped updates; no guarantee of sync across sensors.
% - eBPF and perf-events: not “sampled”; event-driven or syscall-driven; update 
%                         behaviour dominated by system activity rather than 
%                         sensor refresh cycles.
%
% MUST NOT INCLUDE:
% - Any conceptual explanation of sampling vs event time (Chapter 3).
% - Any Tycho-specific reasoning (delay calibration, time alignment).
% - Any discussion of consequences for attribution.
%
% STYLE:
% Keep descriptive, neutral, empirical. Reference VT2 results only by summary, 
% not by reproducing detailed experiments.


%--------------------------------------------------------------------
\subsection{Sensor-Internal Averaging and Missing Timestamps}
\label{subsec:averaging_timestamps}

% PURPOSE OF THIS SUBSECTION:
% Show that telemetry values do not represent instantaneous measurements and do 
% not carry explicit temporal metadata. Prepare the empirical ground for Gap 2.
%
% MUST INCLUDE:
% - RAPL: provides only cumulative energy counters; no timestamps, no indication 
%         of when the underlying sensor last updated.
% - NVML: all power values are internally averaged over undocumented windows; 
%         smoothing hides short peaks; no sample timestamps; utilisation metric 
%         also averaged separately.
% - DCGM: scrape-time timestamps only; underlying values have same averaging as NVML.
% - Redfish: values reflect BMC-internal sensing and averaging; no documented 
%            averaging window; no timestamps indicating measurement time.
%
% MUST NOT INCLUDE:
% - Conceptual interpretation (e.g. “this leads to misalignment”).
% - Any discussion of reconstruction, interpolation, or Tycho’s mitigation.
%
% STYLE:
% Emphasise empirical findings; treat averaging and timestamp absence as 
% observational facts.


%--------------------------------------------------------------------
\subsection{Domain Boundary Ambiguity}
\label{subsec:domain_ambiguity}

% PURPOSE OF THIS SUBSECTION:
% Summarise the uncertainty about what exactly is being measured in each domain.
% Provide empirical background for Gap 3.
%
% MUST INCLUDE:
% - RAPL: Intel’s pkg / core / uncore / dram domains partially overlap; boundaries 
%         are architecture-dependent and not fully documented. 
% - AMD: only pkg and core domains; no DRAM domain; unclear coverage of memory-related 
%        activity; observed differences between perf-events and powercap listings.
% - GPU (NVML/DCGM): “board power” aggregates multiple rails; no per-SM or per-memory 
%                    domains; MIG instances share a single power domain.
% - Redfish: chassis or PSU-level values may include fans, VRMs, or unrelated components 
%            depending on vendor implementation.
%
% MUST NOT INCLUDE:
% - Interpretation such as “this complicates attribution” (reserved for Chapter 3).
% - Methodological responses or design implications.
%
% STYLE:
% State factual ambiguity without judgement; stay architecture-neutral.


%--------------------------------------------------------------------
\subsection{Validation Methodologies in Prior Research}
\label{subsec:validation_methods}

% PURPOSE OF THIS SUBSECTION:
% Provide historical context on how prior work evaluated the correctness of telemetry.
% This prepares the ground for understanding the reliability of published results.
%
% MUST INCLUDE:
% - Description of common validation practices:
%   * External hardware power meters (wall power or inline sensors).
%   * Cross-domain comparison (e.g. RAPL pkg + dram vs AC measurement).
#   * Synthetic workloads for stress-testing temporal response (short kernels, bursts).
#   * Architecture-comparative studies (Ampere vs Hopper, Intel vs AMD).
#   * Occasional use of performance counters as secondary reference.
% - Known limitations of these methods:
%   * External meters have coarse sampling.
%   * Lack of timestamps complicates sample alignment.
%   * Domain ambiguity limits interpretability of discrepancies.
%
% MUST NOT INCLUDE:
% - Any discussion of how validation will be performed in this thesis.
% - Any critique or proposal of improved methods (belongs to research gaps or methods).
%
% STYLE:
% Brief, descriptive, purely historical. Avoid excessive methodological detail.




%====================================================================
\section{Existing Tools and Related Work}
\label{sec:related_tools}
\appchapterref{A}{vt2_Chapter4}
% Expected length: 3–4 pages.
% DO NOT discuss Tycho or Tycho architecture.
% DO summarise empirical findings from prior work (Kepler, Kubewatt, others).

%--------------------------------------------------------------------
\subsection{General Tools (Brief Overview)} ->>>>In “Existing Tools,” potentially merge “General Tools” + short mentions of other tools into a single paragraph
\label{subsec:general_tools}

% Placeholder (0.3 page):
% - Briefly summarise Scaphandre, Smartwatts, and 1–2 other tools.
% - Emphasise their measurement scope and known limitations.
% - DO NOT provide conceptual attribution explanation (Chapter 3).
% - Keep this high-level and concise.

%--------------------------------------------------------------------
\subsection{Kepler}
\label{subsec:kepler}

% Placeholder (1.5–2.0 pages):
% - Describe Kepler’s architecture based purely on literature:
%   * metric sources
%   * ratio-based attribution strategy
%   * sampling behaviour
% - Summarise empirical misattribution issues identified by research and VT2:
%   * idle power artefacts
%   * latency mismatch
%   * short-lived workload instability
%   * system-process handling issues
% - Emphasise Kepler's shift toward deployability (literature statements).
% - DO NOT mention Tycho, Tycho improvements, or motivation for Tycho.

%--------------------------------------------------------------------
\subsection{Kubewatt}
\label{subsec:kubewatt}

% Placeholder (1.0–1.5 pages):
% - Summarise Kubewatt’s findings validating Kepler:
%   * incorrect idle attribution
%   * metadata/race-condition problems
%   * visibility issues with completed pods
% - Summarise Kubewatt’s improvements:
%   * static vs dynamic separation
%   * simplified proportional models
% - Summarise Kubewatt’s limitations as reported in thesis:
%   * missing domains
%   * limited metric scope
%   * no calibration
% - DO NOT reference Tycho or compare to Tycho.

%====================================================================
\section{Research Gaps}
\label{sec:research_gaps}

% This section synthesises gaps identified throughout Chapter 2.
% ABSOLUTELY NO Tycho discussion.
% NO conceptual discussion (that belongs to Chapter 3).

%--------------------------------------------------------------------
\subsection{Gap 1: Temporal Misalignment}
\label{subsec:gap_temporal}

% Placeholder (0.5 page):
% - Integrate research showing misaligned update intervals and delays.
% - DO NOT discuss strategies for alignment (Chapter 3 or 4).

%--------------------------------------------------------------------
\subsection{Gap 2: Missing Timestamps and Averaging}
\label{subsec:gap_timestamps}

% Placeholder (0.3 page):
% - Summarise lack of timestamps and internal averaging as documented by research.

%--------------------------------------------------------------------
\subsection{Gap 3: Domain Boundary Ambiguity}
\label{subsec:gap_boundaries}

% Placeholder (0.3–0.5 page):
% - Integrate findings on poorly documented boundaries (RAPL, MIG).

%--------------------------------------------------------------------
\subsection{Gap 4: Metadata-Lifecycle Inconsistencies}
\label{subsec:gap_metadata}

% Placeholder (0.3–0.5 page):
% - Synthesise research showing cgroup/pod lifecycle inconsistencies.

%--------------------------------------------------------------------
\subsection{Gap 5: Idle Power Attribution Issues}
\label{subsec:gap_idle}

% Placeholder (0.3 page):
% - Highlight known issues in idle power handling.

%--------------------------------------------------------------------
\subsection{Gap 6: Limited Multi-Domain Integration}
\label{subsec:gap_multidomain}

% Placeholder (0.3 page):
% - Summarise research noting lack of integrated CPU–GPU–system attribution.

%--------------------------------------------------------------------
\subsection{Gap 7: Missing Calibration and Uncertainty Treatment}
\label{subsec:gap_calibration}

% Placeholder (0.3–0.5 page):
% - Summarise research calling for calibration or uncertainty-aware methods.

%====================================================================
\section{Summary}
\label{sec:background_summary}

% Placeholder (0.5 page):
% - Summarise fundamental insights: heterogeneous telemetry behaviour, 
%   incomplete domain coverage, lifecycle inconsistency, lack of calibration.
% - Prepare the reader for Chapter 3, stating explicitly that Chapter 3
%   provides the conceptual foundations needed to understand the design 
%   of an accuracy-oriented system (without yet describing Tycho).

