\chapter{Background and Related Research}
\label{ch:background}

\noindent This chapter summarises the current state of research and industrial knowledge on server-level energy measurement. Its focus is limited to what the literature reports about available telemetry sources, measurement techniques, and existing attribution tools. The discussion is descriptive rather than conceptual: it does not introduce attribution principles, methodological reasoning, or design considerations, which are addressed in \chapref{ch:concepts}. Extended background material is available in \hyperref[appendix_vt2]{Appendix~A} and the present chapter integrates only those findings that are directly relevant for understanding the research landscape.

\section{Energy Measurement in Modern Server Systems}
\label{sec:energy_measurement_systems}

The energy consumption of modern servers arises from a heterogeneous set of subsystems, including CPUs, GPUs, memory, storage devices, network interfaces, and platform management components. Prior research highlights that these subsystems expose highly unequal visibility into their power behaviour, since measurement capabilities, granularity, and accuracy differ significantly across hardware generations and vendors \parencite{lin2020taxonomy, long2022review}. Some domains provide direct telemetry, while others can only be approximated through software-derived activity metrics. As a result, no single interface offers complete or temporally consistent power information, and most studies rely on a single source or combine multiple sources to approximate system-level consumption. This fragmented measurement landscape forms the basis for much of the existing work on power modelling, validation, and multi-source energy estimation in server environments.

\subsection{Energy Attribution in Multi-Tenant Environments}
\label{subsec:attribution_context}

Several studies identify containerised and multi-tenant systems as challenging environments for energy attribution. Containers share the host kernel and rely on common processor, memory, storage, and network subsystems, which removes the isolation boundaries present in virtual machines and prevents direct measurement of per-container power. Research reports that workloads running concurrently on the same node create interference effects across hardware domains, leading to utilisation patterns that correlate only loosely with actual energy consumption \parencite{lin2020taxonomy}. Modern orchestration platforms further increase attribution difficulty through highly dynamic execution behaviour: containers are created, destroyed, and rescheduled at high frequency, often numbering in the thousands on large clusters. These rapid lifecycle changes produce volatile metadata and short-lived resource traces that are difficult to align with node-level telemetry. Collectively, the literature treats container-level energy attribution as an estimation problem constrained by incomplete observability, heterogeneous measurement quality, and continuous runtime churn.

\subsection{Telemetry Layers in Contemporary Architectures}
\label{subsec:telemetry_layers}

Modern servers expose power and activity information through two largely independent telemetry layers. The first consists of in-band mechanisms that are visible to the operating system, including on-die energy counters, GPU management interfaces, and kernel-level resource statistics. These interfaces typically offer higher sampling rates and finer granularity, but their accuracy and coverage vary across hardware generations and vendors. Prior work notes that in-band telemetry often represents estimated rather than directly measured power and that several domains, such as network and storage devices, expose only partial or indirect information.

The second layer is out-of-band telemetry provided by baseboard management controllers through interfaces such as IPMI or Redfish. These systems aggregate sensor readings independently of the host and report stable, whole-system power values at coarse temporal resolution. Empirical studies show that out-of-band telemetry provides useful system-level accuracy, although update intervals and measurement precision differ substantially between vendors \parencite{wang2019empirical}. Compared with instrument-based measurements, which remain the benchmark for high-fidelity evaluation but are impractical at scale, both in-band and out-of-band methods represent trade-offs between granularity, availability, and measurement reliability.

Combined, these layers form a heterogeneous telemetry landscape in which sampling rates, accuracy, and domain coverage differ significantly, motivating the use of multi-source measurement approaches in research.

\subsection{Challenges for Container-Level Measurement}
\label{subsec:container_challenges}

Existing research identifies several factors that complicate accurate energy measurement for containerised workloads. Large-scale trace analyses show that cloud environments exhibit substantial churn, with many tasks being short-lived and resource demands changing rapidly over time \parencite{reissHeterogeneityDynamicityClouds2012}. Such dynamism limits the observability of fine-grained resource usage and makes it difficult to capture short execution intervals with sufficient temporal resolution.

Monitoring studies further report inconsistencies across the different layers that expose resource information for containers. In multi-cloud settings, observability often depends on heterogeneous monitoring stacks, leading to fragmented visibility and non-uniform coverage of system activity \parencite{waseemContainerizationMultiCloudEnvironment2025}. Even within a single host, performance counters obtained from container-level interfaces may diverge from system-level measurements. Empirical evaluations demonstrate that container-level CPU and I/O counters can underestimate actual activity by a non-negligible margin, and that co-located workloads introduce contention effects that distort these metrics \parencite{casalicchioStateoftheartContainerTechnologies2020}.

These findings indicate that container-level measurement operates under conditions of rapid workload turnover, heterogeneous monitoring behaviour, and imperfect resource visibility. As a consequence, the literature treats container energy attribution as a problem constrained by incomplete and potentially biased measurement signals rather than as a directly measurable quantity.

\section{Hardware and Software Telemetry Sources}
\label{sec:measurement_techniques}

This section outlines the primary telemetry sources used to observe power and resource behaviour in modern server systems. It summarises established research on external measurement devices, firmware-level interfaces, on-die energy counters, accelerator telemetry, and kernel-exposed resource metrics. The emphasis is on reporting the properties and empirical characteristics documented in prior work, without interpreting these signals conceptually or analysing their temporal behaviour, which are addressed in later sections. A comprehensive technical discussion is provided in \appchapterref{A}{vt2_Chapter2}; the present section extracts only the findings relevant for understanding the measurement landscape.

\subsection{Direct Hardware Measurement}
\label{subsec:direct_measurement}

Direct physical instrumentation remains the most accurate method for measuring server power consumption. External power meters or inline shunt-based devices can capture node-level energy usage with high fidelity, and research frequently uses such instrumentation as a ground truth for validating software-reported power values. Studies employing dedicated measurement setups, such as custom DIMM-level sensing boards, demonstrate that high-frequency sampling and component-level granularity are technically feasible but require bespoke hardware and non-trivial integration effort \parencite{desrochers2016validation}. Lin et al.\ classify these approaches as offering very high data credibility but only coarse spatial granularity and limited scalability in operational environments \parencite{lin2020taxonomy}.

Recent work on specialised sensors, such as the PowerSensor3 platform\parencite{van2025powersensor3} for high-rate voltage and current monitoring of GPUs and other accelerators, illustrates ongoing interest in hardware-centric power measurement. However, these systems share the same fundamental drawback: deployment across production servers is complex, costly, and incompatible with large-scale or multi-tenant settings. As a consequence, direct instrumentation is predominantly used in controlled experiments or for validation of other telemetry sources, rather than as a primary measurement mechanism in real-world server infrastructures.

\subsection{Legacy Telemetry Interfaces (ACPI, IPMI)}
\label{subsec:legacy_telemetry}

Early power-related telemetry on server platforms was primarily exposed through ACPI and IPMI. ACPI provides a standardised interface for configuring and controlling hardware power states, but it does not offer real-time energy or power readings. The interface exposes only abstract performance and idle states defined by the firmware \parencite{uefi_acpi_6_6}, and these states do not include the instantaneous power information required for empirical energy measurement. Consequently, ACPI has seen little use in modern power estimation research.

IPMI, accessed through the baseboard management controller, represents an older class of out-of-band telemetry that predates Redfish. Although widely supported across server hardware, IPMI power values are known to be coarse, slowly refreshed, and often inaccurate when compared with external instrumentation. Empirical studies report multi-second averaging windows, substantial quantisation effects, and unreliable idle power readings \parencite{kavanagh2016accuracy, kavanagh2019rapid}. These limitations, together with the availability of more precise alternatives, have led IPMI to be largely superseded by Redfish on contemporary server platforms.

\subsection{Redfish Power Telemetry}
\label{subsec:redfish}

Redfish is the modern out-of-band management interface available on contemporary server platforms and is designed as the successor to IPMI. It exposes system-level telemetry through a RESTful API implemented on the baseboard management controller (BMC), providing access to whole-node power readings derived from on-board sensors. Prior work consistently shows that Redfish delivers higher precision than IPMI, with lower quantisation artefacts and more stable readings across power ranges \parencite{wang2019empirical}. In controlled experiments, Redfish achieved a mean absolute percentage error of roughly three percent when compared to a high-accuracy power analyser, outperforming IPMI in all evaluated power intervals.

A key limitation of Redfish is its temporal granularity. Empirical studies report that power values exhibit non-negligible staleness, with refresh delays of approximately 200\,ms \parencite{wang2019empirical}. This latency restricts the ability of Redfish to capture short bursts of activity or rapid fluctuations in dynamic workloads. Accuracy and responsiveness also vary across vendors, reflecting differences in embedded sensors, BMC firmware, and management controller architectures.

The interface is widely deployed in real-world infrastructure. Modern enterprise servers from Dell, HPE, Lenovo, Cisco, and Supermicro routinely expose power telemetry via Redfish as part of their standard BMC firmware \parencite{herrlinAccessingOnboardServer2021}. Out-of-band monitoring studies further highlight that Redfish avoids the overheads and failure modes associated with in-band agents \parencite{aliRedfishNagiosScalableOutofBand2022}. In practice, Redfish implementations tend to provide stable low-frequency updates suitable for coarse-grained power reporting.

Preliminary measurements conducted for this thesis also observed irregular update intervals on the evaluated hardware, occasionally extending into the multi-second range. While this behaviour is specific to a single system and not generalisable, it reinforces the literature’s position that Redfish telemetry exhibits meaningful vendor-dependent variability and remains unsuitable for fine-grained temporal correlation.

Overall, Redfish provides accessible, reliable whole-node power telemetry at coarse temporal resolutions, making it valuable for long-interval monitoring and for validating other measurement sources, but inappropriate for attributing energy consumption to short-lived or rapidly fluctuating containerised workloads.

\subsection{RAPL Power Domains}
\label{subsec:rapl}

Running Average Power Limit (RAPL) provides hardware-backed energy counters for several internal power domains of a processor package. Originally introduced by Intel and later adopted in a compatible form by AMD, RAPL exposes energy measurements via model-specific registers that can be accessed directly or through higher-level interfaces such as the Linux \texttt{powercap} framework or the \texttt{perf-events} subsystem \parencite{intel-sdm, raffin2024dissecting}. Raffin et~al.\ provide a detailed comparison of these access mechanisms, noting that MSR, powercap, perf-events, and eBPF differ mainly in convenience, required privileges, and robustness; all can retrieve equivalent RAPL readings when implemented correctly \parencite{raffin2024dissecting}. They recommend accessing RAPL via the powercap interface, which is easiest to implement reliably and suffers from no overhead penalties when compared with more low-level methods.

Intel platforms typically expose several well-established RAPL domains, including the processor package, the core subsystem, and (on many server architectures) a DRAM domain \parencite{hackenberg2015energy}. These domains have been validated extensively against external measurement equipment. Studies report that the combination of package and DRAM energy tracks CPU-and-memory power with good accuracy from Haswell onwards, which has led to RAPL becoming the primary fine-grained energy source in server-oriented research \parencite{hackenberg2013power, desrochers2016validation, alt2024experimental, kennes2023measuring}. More recent work on hybrid architectures such as Alder Lake confirms that RAPL continues to correlate well with external measurements under load, while precision decreases somewhat in low-power regimes \parencite{schone2024energy}. Across these studies, RAPL is generally regarded as sufficiently accurate for scientific analysis when its domain boundaries and update characteristics are considered \parencite{raffin2024dissecting}.

AMD implements a RAPL-compatible interface with a similar programming model but a reduced set of domains. Zen 1 through Zen 4 processors expose package and core domains only, without a dedicated DRAM domain \parencite{schone2021energy, raffin2024dissecting}. Sch\"one et~al.\ show that, as a consequence, memory-related energy may not be represented explicitly in AMD’s RAPL output, leading to a smaller portion of total system energy being observable through the package domain alone \parencite{schone2021energy}. This limitation primarily concerns domain completeness rather than measurement correctness: for compute-intensive workloads, package-domain values behave consistently, but workloads with significant memory activity exhibit a larger gap relative to whole-system measurements because DRAM energy is not separately reported. Raffin et~al.\ further note that, on the evaluated Zen-based server, different kernel interfaces initially exposed inconsistent domain sets; this was later corrected upstream, illustrating that AMD support is evolving and still maturing within the Linux ecosystem \parencite{raffin2024dissecting}.

Technical considerations also apply to both Intel and AMD platforms. RAPL counters have finite width and wrap after sufficiently large energy accumulation, requiring consumers to implement overflow correction \parencite{khan2018rapl, raffin2024dissecting}. The counters do not include timestamps, and empirical work shows that actual update intervals may deviate from nominal values, complicating precise temporal correlation with other telemetry \parencite{hackenberg2013power, jay2023experimental}. On some Intel platforms, security hardening measures such as energy filtering reduce temporal granularity for certain domains to mitigate side-channel risks \parencite{lipp2021platypus, intel2023, schone2024energy}. In virtualised environments, RAPL access may be trapped by the hypervisor, increasing latency and introducing small deviations from bare-metal behaviour \parencite{jay2023experimental}.

In summary, RAPL provides a widely used and comparatively fine-grained source of processor-side energy telemetry. Intel platforms typically offer multiple validated domains, including DRAM, enabling a broader view of CPU-and-memory energy. AMD platforms expose fewer domains and therefore provide a more limited perspective on total system power, particularly for memory-intensive workloads. These differences in domain coverage, measurement scope, and software integration need to be taken into account when using RAPL as a basis for energy analysis.

\subsection{GPU Telemetry}
\label{subsec:gpu_telemetry}

Unlike CPUs, where power and utilization telemetry is supported through standardised
interfaces, GPU energy visibility relies primarily on vendor-specific mechanisms.
For NVIDIA devices, two interfaces dominate this landscape: the \textit{NVIDIA Management
Library} (NVML), which has become the industry standard, and the \textit{Data Center GPU
Manager} (DCGM), a less widely used management layer that also exposes telemetry.

\subsubsection{NVML}
NVML is NVIDIA’s primary interface for device-level monitoring and underpins tools such
as \texttt{nvidia-smi}.  
It provides access to power, energy (on selected data-center GPUs), GPU utilization,
memory usage, clock frequencies, thermal state, and various health and throttle
indicators.  
Among these, power and utilization are most relevant for energy analysis.

NVML power values represent board-level estimates derived from on-device sensing
circuits and are shaped by internal averaging and architecture-dependent update
behaviour.  
Recent empirical studies across modern devices show that NVML produces fresh samples only
intermittently and applies smoothing that reduces the visibility of short-lived power
changes, while steady-state power levels remain comparatively accurate
\parencite{yang2024accurate}.  
On the Grace--Hopper GH200, these effects are pronounced: NVML reflects a coarse internal
averaging interval and therefore underrepresents short kernels and transient peaks
relative to higher-frequency system interfaces
\parencite{hernandezPreliminaryStudyFineGrained2025}.  
These findings indicate that NVML captures long-term power behaviour reliably but
inherently limits fine-grained visibility.  
Despite these constraints, existing studies consistently find that NVML provides
reasonably accurate steady-state power estimates on modern data-center GPUs and currently
represents the most reliable and widely supported mechanism for obtaining GPU power
telemetry in practical systems \parencite{hernandezPreliminaryStudyFineGrained2025}.

GPU utilization provides contextual information about device activity.  
It reports the proportion of time during which the GPU is executing any workload rather
than the fraction of computational capacity in use, making it a coarse activity
indicator rather than a detailed performance metric
\parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{DCGM}
DCGM is NVIDIA’s management and observability framework designed for data-center
deployments.  
It aggregates telemetry, performs health monitoring, exposes thermal and throttle state,
and provides detailed visibility in environments that employ Multi-Instance GPU (MIG)
partitioning.  
However, DCGM’s power and utilization metrics are derived from the same underlying
measurement sources as NVML.  
In practice, DCGM is far less commonly used for energy analysis because it does not
provide higher-fidelity power telemetry; instead, it applies additional aggregation and
is typically deployed with coarse sampling intervals, especially when used through
exporters in cluster monitoring systems.  
DCGM therefore represents an alternative access path to the same measurements rather than
a distinct source of energy-related information.  

DCGM is considerably less common in both research and operational practice, with most
GPU monitoring systems relying primarily on NVML while DCGM appears only occasionally in
cluster-level deployments \parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{Summary}
NVML and DCGM jointly define the available mechanisms for GPU telemetry in cloud
environments.  
NVML is the dominant and broadly supported interface for power and utilization
measurement, while DCGM extends it with operational metadata and management integration.
Current studies consistently show that both interfaces expose averaged, device-level
power estimates that capture long-term behaviour but are inherently limited in their
ability to represent short-duration activity or fine-grained workload structure.  
These characteristics form the scientific foundation for later discussions of temporal
behaviour and measurement methodology.

\subsection{Software-Exposed Resource Metrics}
\label{subsec:software_metrics}

In addition to hardware telemetry, Linux and Kubernetes expose a wide range of
software-level resource metrics that describe system and workload activity.
These metrics do not measure power directly but provide essential behavioural
context that complements RAPL, Redfish, and GPU telemetry.

\subsubsection{CPU and Memory Activity Metrics}

Linux provides several complementary mechanisms for tracking CPU and memory
usage.  
Global counters such as \path{/proc/stat} record cumulative CPU time since boot,
while per-task statistics in \path{/proc/<pid>} expose user-mode and kernel-mode
execution time with high granularity \parencite{kernelprocfs}.  
Control groups (cgroups) provide container-level CPU and memory accounting and
form the primary basis for utilisation metrics inside Kubernetes
\parencite{kernelcgroupv1, kernelcgroupv2}.  
Higher-level tools such as cAdvisor and metrics-server aggregate this
information via Kubelet, but at significantly lower update rates.

Event-driven approaches provide substantially finer resolution.  
eBPF allows dynamic attachment to kernel events such as context switches,
scheduling decisions, and I/O operations, enabling near-real-time capture of
per-task CPU activity with low overhead
\parencite{ciliumbpf, cassagnesRiseEBPFNonintrusive2020}.  
Hardware performance counters accessed through \code{perf} offer insight into
instruction counts, cycles, cache behaviour, and stalls
\parencite{Gregg2017CpuUtilizationWrong}.  
These sources provide detailed behavioural information but still represent
utilisation rather than energy.

\subsubsection{Storage Activity Metrics}

Storage subsystems do not expose real-time power telemetry, yet Linux provides a
rich set of activity indicators.  
Per-process statistics in \path{/proc/<pid>/io} track bytes read and written,
while cgroup I/O controllers report aggregated container-level metrics.  
Subsystem-specific tools such as \code{smartctl} and \code{nvme-cli} reveal
additional device characteristics, queue behaviour, and state transitions
\parencite{smartmontools_github, nvmecli_github}.  

In the absence of hardware power sensors, multiple works propose
workload-dependent energy models for storage devices
\parencite{choDesignTradeoffsSSDs2015, liWhichStorageDevice2014,
borbaModelingApproachEstimating2022}.  
These models can yield accurate estimates when calibrated for a specific device
but do not generalise across heterogeneous hardware due to differences in flash
controllers, firmware, and internal data paths.

\subsubsection{Network and PCIe Device Metrics}

Network interfaces provide byte and packet counters via \path{/proc/net/dev}, but
expose no dedicated power telemetry.  
Research models for NIC energy consumption exist
\parencite{sohanCharacterizing10Gbps2010, basmadjianCloudComputingIts2012,
baneshiAnalyzingPerApplicationEnergy2024}, yet all rely on device-specific idle
and active power characteristics that are not available at runtime.  
Similarly, PCIe devices support abstract power states as defined by the PCIe
specification \parencite{technotes_pci_power_2024}, but these states do not
reflect instantaneous power usage and thus offer only coarse activity signals.

\subsubsection{Secondary System Components}

Components such as fans, motherboard logic, and power delivery subsystems rarely
expose fine-grained telemetry.  
Although some BMC implementations report coarse sensor values, these readings
are inconsistent across platforms and generally unsuitable for high-resolution
analysis.  
Consequently, research commonly treats these subsystems as part of the residual
power that scales with the activity of primary components
\parencite{basmadjianCloudComputingIts2012}.

\subsubsection{Model-Based Estimation Approaches}

Because software-visible metrics capture detailed workload behaviour, many works
propose inferring energy consumption from utilisation using regression or
stochastic models
\parencite{fan2007power, hsu2011power, song2013unified,
arjonaarocaMeasurementbasedAnalysisEnergy2014}.  
While these models can be effective when fitted to a specific hardware platform,
their accuracy depends heavily on device-specific parameters, making them
unsuitable as a general mechanism for heterogeneous server environments.  
Machine-learning-based estimators share the same limitation: high accuracy when
trained for a fixed configuration, poor portability without extensive retraining.

\subsubsection{Summary}

Software-exposed metrics provide high-resolution visibility into CPU, memory,
I/O, and network activity.  
They are indispensable for correlating workload behaviour with hardware power
signals, especially for components that lack native telemetry. 
Model-based estimation remains possible but inherently platform-specific, and
therefore unsuitable as a universal foundation for fine-grained attribution in
heterogeneous environments.

\section{Temporal Behaviour of Telemetry Sources}
\label{sec:temporal_behaviour}

A comprehensive treatment of temporal characteristics can be found in 
\appchapterref{A}{vt2_Chapter2}, but the present section focuses on the empirical, 
source-specific behaviours that constrain fine-grained power and energy estimation on 
real systems. Modern server platforms expose a heterogeneous set of telemetry 
interfaces, and their timing properties vary substantially: some update at fixed 
intervals, others employ internal averaging or smoothing, several expose counters 
without timestamps, and many lack guarantees on refresh regularity. These behaviours 
shape the effective temporal resolution with which workload-induced power changes can 
be observed.

The purpose of this section is not to develop a conceptual theory of sampling or to 
explain why timing matters for attribution (both are deferred to \chapref{ch:concepts}), nor to 
introduce Tycho’s timing engine (\chapref{ch:architecture}). Rather, it establishes the empirical 
constraints imposed by the telemetry sources themselves. These include sensor refresh 
intervals, stability of consecutive updates, delays between physical behaviour and 
reported values, the presence or absence of timestamps, and the distinction between 
instantaneous versus internally averaged measurements.

The subsections that follow describe these temporal properties for each telemetry 
source individually and summarise the practical limits they impose on high-resolution 
energy analysis.

\subsection{RAPL Update Intervals and Sampling Stability}
\label{subsec:rapl_timing}

RAPL exposes energy \emph{counters} rather than instantaneous power values.
These counters accumulate energy since boot and can be read at arbitrarily high
frequency, but their usefulness is determined entirely by how often the internal
measurement logic refreshes them, a timing behaviour that is undocumented and
domain-dependent.

\paragraph{Domain-specific internal update rates}
Intel specifies the RAPL time unit as 0.976\,ms for the slowest-updating
domains, while others, notably the PP0 (core) domain, may refresh
significantly faster \parencite{schone2024energy}.  
In practice, however, these theoretical limits do not translate into usable
temporal resolution because RAPL provides no timestamps: the moment of counter
refresh is unknown to the reader.  At sub-millisecond sampling rates, the lack
of timestamps combined with irregular refresh behaviour introduces substantial
relative error, since differences between consecutive reads may reflect counter
staleness rather than actual power dynamics \parencite{khan2018rapl}.

\paragraph{Noise introduced by security-driven filtering}
To mitigate power-side channels such as Platypus, Intel optionally introduces
randomised noise through the \code{ENERGY\_FILTERING\_ENABLE} mechanism
\parencite{intel2023}.  
This filtering increases the effective minimum granularity from roughly
1\,ms to approximately 8\,ms for the PP0 domain \parencite{schone2024energy}.  
While average energy over longer intervals remains accurate, instantaneous
increments become less reliable at very short timescales.

\paragraph{Practical sampling limits}
Despite the nominal sub-millisecond timing, empirical work consistently shows
that high-frequency polling offers no practical benefit.  
Multiple studies report that sampling faster than the internal update period
only produces repeated counter values and amplifies read noise
\parencite{khan2018rapl}.  
Jay et~al.\ demonstrate that at polling rates slower than 50\,Hz, the relative
error falls below 0.5\,\% \parencite{jay2023experimental}.  
Consequently, typical measurement practice (and the limits adopted in this
thesis) treats RAPL as reliable only at tens-of-milliseconds resolution, not at
the theoretical millisecond scale suggested by its nominal time unit.

\paragraph{Summary}
Although RAPL counters can be read extremely quickly, the effective temporal
resolution is constrained by undocumented refresh intervals, absence of
timestamps, optional security filtering, and substantial measurement noise at
high polling rates.  
For practical purposes, sampling at approximately 20--50\,ms intervals yields
the most stable and accurate results, while sub-millisecond polling is
inadvisable due to high relative error and counter staleness.

\subsection{GPU Update Intervals and Sampling Freshness}
\label{subsec:gpu_timing}

GPU power telemetry is exposed primarily through NVML, with DCGM providing an alternative access path that builds on the same underlying measurement source.
Unlike CPU-side interfaces integrated into the processor package, GPU power monitoring is performed entirely by the device itself: internal sensing circuits and firmware determine how often new values are produced, how they are averaged, and when they are published to software. As a result, refresh behaviour varies substantially across architectures, and the temporal properties of the reported values depend on device-internal update cycles rather than the rate at which the host system issues queries, which limits the achievable resolution of any external sampling strategy.

\paragraph{Internal update cycles and sampling freshness}
Empirical studies consistently show that NVML publishes new power values only intermittently, even when queried at high frequency.
Yang et~al.\ report sampling availability as low as roughly twenty–twenty-five percent across more than seventy modern data-center GPUs, meaning that the majority of polls return previously published values rather than fresh measurements \parencite{yang2024accurate}.

Typical internal update periods fall on the order of tens to several hundreds of milliseconds, with architectural variation between GPU generations.
Hernandez et~al.\ report that newer architectures apply more aggressive smoothing and exhibit longer gaps between updates, reflecting slower publication cadence at the firmware level \parencite{hernandezPreliminaryStudyFineGrained2025}.
Overall, empirical evaluations show that NVML’s internal update interval may lie on the order of hundreds of milliseconds and that repeated queries do not guarantee the retrieval of a new sample at every call \parencite{yang2024accurate}.
NVML power readings do not represent instantaneous electrical measurements; they reflect firmware-level integration and smoothing over a device-internal averaging window, the duration of which varies by GPU generation and is not publicly documented..

\paragraph{Reaction delay to workload-induced power changes}
A related characteristic is NVML’s reaction delay: when GPU power changes due to workload activity, the corresponding update becomes visible only after a lag.
Multiple studies document delays in the range of approximately one to three hundred milliseconds before a new NVML value reflects the underlying power transition \parencite{yang2024accurate}.
This delay is distinct from averaging effects and arises from deferred publication of internally accumulated measurements.
On some recent architectures, the delay can be longer due to device-level smoothing layers that defer updates until sufficient internal samples have been collected \parencite{hernandezPreliminaryStudyFineGrained2025}.

\paragraph{Update regularity and jitter}
NVML update cycles are not perfectly periodic.
Even when a nominal internal cadence is observable, individual publish times exhibit modest jitter, and occasional missed or skipped updates can result in sequences of identical values.
These effects are pronounced on certain consumer-class devices and in configurations that partition the GPU, such as MIG, although they are also present to a lesser degree on data-center accelerators \parencite{yang2024accurate}.
Such irregularity introduces uncertainty regarding the true measurement time of any retrieved value, especially in the sub-second range.

\paragraph{DCGM sampling behaviour}
DCGM relies on the same underlying measurement path as NVML and therefore inherits NVML’s internal update characteristics.
In practice, DCGM is commonly accessed through its exporter, which introduces an additional periodic sampling stage (typically around one second) resulting in markedly coarser temporal behaviour than NVML’s native cadence.
As a result, DCGM-based power telemetry rarely offers sub-second resolution in operational environments \parencite{weakleyMonitoringCharacterizingGPU2025}.

\paragraph{GPU utilization update cycles}
NVML’s GPU utilization metric follows its own internal update cadence, separate from power.
It is typically refreshed more frequently (on the order of tens of milliseconds) although the exact timing remains undocumented.
While this metric does not track computational efficiency, its shorter update interval provides a comparatively more responsive indicator of device activity \parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsection{Redfish Sensor Refresh Intervals and Irregularity}
\label{subsec:redfish_timing}

Redfish exposes power telemetry through the baseboard management controller (BMC) and therefore inherits the temporal behaviour of its embedded sensing hardware and firmware. In contrast to on-chip interfaces such as RAPL or NVML, Redfish is designed for management-plane observability rather than high-frequency monitoring. Prior studies consistently report that Redfish refreshes whole-node power values at coarse intervals, typically ranging from several hundred milliseconds to multiple seconds, with the exact cadence depending on vendor, BMC firmware, and underlying sensor design \parencite{wang2019empirical, aliRedfishNagiosScalableOutofBand2022}. The Redfish standard does not define a minimum update frequency, and available documentation provides little insight into internal sampling or averaging strategies.

\paragraph{Measurement semantics of Redfish power values}
Redfish does not expose instantaneous electrical measurements. Instead, the reported values originate from on-board monitoring chips connected to shunt-based sensors and are subsequently processed inside the BMC. Vendor documentation indicates that these sensors inherently integrate power over tens to hundreds of milliseconds, and that additional firmware-level smoothing may be applied before values are published through the Redfish API \parencite{aliRedfishNagiosScalableOutofBand2022}. Empirical evaluations support this interpretation: Wang et~al.\ show that Redfish exhibits reaction delays of roughly two hundred milliseconds and displays particularly stable behaviour under steady loads, consistent with block-averaged rather than instantaneous sampling \parencite{wang2019empirical}. Because neither the sensor integration window nor any BMC filtering policies are defined in the standard, the temporal semantics of published values remain implementation-dependent.

Redfish power readings include a timestamp field, but this value reflects the BMC’s observation time rather than the sampling instant of the physical power sensor. In many implementations, timestamps are rounded to seconds, which limits their utility for reconstructing sub-second dynamics and prevents reliable inference of the underlying sampling moment.

Beyond published work, empirical observations from the system used in this thesis reveal that Redfish update intervals may exhibit substantial variability. While nominal refresh periods appear regular over longer windows, individual samples occasionally show multi-second gaps, repeated values, or irregular spacing. Such behaviour is consistent with a telemetry source operating on management-plane scheduling and BMC workload constraints rather than real-time guarantees. These observations do not generalise across vendors but illustrate the degree of temporal uncertainty that can occur in practice.

Overall, Redfish provides a widely supported mechanism for obtaining whole-system power readings and is well suited for coarse-grained monitoring or validation of other telemetry sources. Its coarse refresh intervals, lack of sensor-level timestamps, and implementation-dependent irregularities, however, make it unsuitable for analysing short-duration phenomena or for use as a primary source in high-resolution energy attribution.

\subsection{Timing of Software-Exposed Metrics}
\label{subsec:sw_timing}

Software-exposed resource metrics differ fundamentally from hardware-integrated telemetry sources: rather than publishing sampled power or energy values at device-defined intervals, the Linux kernel exposes cumulative counters whose temporal behaviour is almost entirely determined by when they are read. These interfaces therefore provide quasi-continuous visibility into system activity, but without intrinsic update cycles or timestamps that would define the sampling moment of the underlying measurement.

\paragraph{Cumulative counters in \texttt{/proc} and cgroups}
Kernel interfaces such as \path{/proc/stat}, per-task entries under \path{/proc/<pid>}, and the CPU accounting files in cgroups expose resource usage as monotonically increasing counters.  
These values are updated by the kernel during scheduler events, timer interrupts, and context-switch accounting, rather than at fixed intervals.  
As a consequence, their effective temporal resolution is determined entirely by the user’s polling cadence: reading them more frequently produces more detailed deltas, but the kernel does not provide any guarantee about when a counter was last updated.  
None of these counters include timestamps, and their update timing may vary across systems due to tickless operation, kernel configuration, and workload characteristics.

\paragraph{Disk and network I/O statistics}
I/O-related counters follow the same principle.  
Entries such as \path{/proc/<pid>/io}, cgroup I/O files, and interface statistics in \path{/proc/net/dev} are incremented as part of the corresponding driver paths when I/O operations occur.  
They do not refresh periodically and therefore exhibit update patterns that mirror workload activity rather than a regular cadence.  
Temporal interpretation again depends entirely on the polling rate of the monitoring system.

\paragraph{eBPF-based event timing}
In contrast to cumulative counters, eBPF enables event-driven monitoring with explicit timestamps.  
Kernel probes attached to scheduler events, I/O paths, or tracepoints can record event times with high precision using the kernel’s monotonic clock.  
As a result, eBPF metrics provide effectively instantaneous temporal resolution and are limited only by the overhead of probe execution and user-space consumption of BPF maps.  
No internal refresh cycle exists; events are timestamped at the moment they occur.

\paragraph{Performance counters and \texttt{perf}-based monitoring}
Hardware performance monitoring counters (PMCs), accessed via \code{perf\_event\_open}, advance continuously within the processor.  
They do not follow a publish interval, and their timing semantics are defined solely by the instant at which user space reads the counter.  
This provides fine-grained and low-latency access to execution metrics such as cycles and retired instructions, with overhead rising only when polling is performed at very high frequencies.

Overall, software-exposed metrics behave as cumulative or event-driven signals rather than sampled telemetry sources. Their temporal characteristics are dominated by polling strategy and kernel-level event timing, with eBPF representing the only interface that attaches precise timestamps directly to system events.

\section{Existing Tools and Related Work}
\label{sec:existing-tools}

Energy observability in containerized environments has attracted increasing attention in recent years, leading to the development of several tools that combine hardware, software, and statistical telemetry to estimate per-workload energy consumption. Despite this diversity, only a small number of tools attempt to attribute energy at container or pod granularity with sufficient detail to inform system-level research. Among these, \emph{Kepler} has emerged as the most widely adopted open-source solution within the cloud-native ecosystem, while \emph{Kubewatt} represents the first focused research effort to critically evaluate and refine Kepler’s attribution methodology. Other frameworks, such as Scaphandre, SmartWatts, or PowerAPI, offer relevant ideas but differ in scope, telemetry assumptions, or operational goals. For this reason, the remainder of this section concentrates primarily on Kepler and Kubewatt, using these two tools to illustrate the architectural and methodological challenges that motivate the research gaps identified at the end of this chapter.

\subsection{Kepler}
\label{sec:kepler}

\subsubsection{Architecture and Metric Sources}
\label{subsec:kepler-architecture}

Kepler\parencite{kepler_energy} is a node-local energy observability agent designed for Kubernetes environments. Its architecture follows a modular dataflow pattern: a set of collectors periodically ingests telemetry from hardware and kernel interfaces, an internal aggregator aligns and normalizes these inputs, and a Prometheus exporter exposes the resulting metrics at container, pod, and node granularity. This structure allows Kepler to integrate heterogeneous telemetry sources while presenting a unified metric interface to external monitoring systems.

Kepler’s collectors obtain process, container, and node telemetry from standard Linux and Kubernetes subsystems. Resource usage statistics are taken from \texttt{/proc}, cgroup hierarchies, and Kubernetes metadata, while hardware-level energy data is read from RAPL domains via the \texttt{powercap} interface. Optional collectors provide GPU metrics through NVIDIA’s NVML library and platform-level power measurements via Redfish or other BMC interfaces. All inputs are treated as cumulative counters or periodically refreshed state, and their effective resolution is therefore determined by Kepler’s sampling configuration. All metrics are updated at same interval and at the same time (default: 60 seconds for redfish, 3 seconds for all other sources). A central responsibility of the aggregator is to map raw per-process telemetry to containers and pods, using cgroup paths and Kubernetes API metadata. The derived metrics are finally exposed via a Prometheus endpoint, enabling integration into common cloud-native observability stacks.

In contrast to generic system monitoring agents, Kepler’s architecture is tailored specifically to Kubernetes. Its emphasis on container metadata, cgroup-based accounting, and workload-oriented metric aggregation distinguishes it from tools that operate primarily at the host or VM level. At the same time, its reliance on standard Linux interfaces keeps deployment overhead low, requiring only node-local access to \texttt{/proc}, cgroups, and the \texttt{powercap} subsystem.

Overall, Kepler’s architectural design reflects a trade-off between flexibility and granularity: while it can ingest diverse telemetry sources and attribute energy at container level, its accuracy is constrained by the timing and resolution of the underlying metrics, as well as the unified sampling cadence chosen for the collectors.

Kepler updates all metrics within a single synchronous loop that triggers every sampling interval. 
This design simplifies integration but enforces a uniform cadence across heterogeneous telemetry sources, 
which contributes to the timing and alignment issues discussed in \S~\ref{subsec:kepler-limitations}. 
The structure is shown in Figures~\ref{fig:kepler_timing_architecture}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/drawio/kepler_activityDiagram.png}
    \caption[Kepler’s synchronous update loop]{Kepler’s synchronous update loop, where all collectors run at a unified sampling interval.}
    \label{fig:kepler_timing_architecture}
\end{figure}

\subsubsection{Attribution Model}
\label{subsec:kepler-attribution}

Kepler’s attribution logic follows a two-stage structure. First, node-level energy is decomposed into \emph{idle} and \emph{dynamic} components for each available power domain (package, core, uncore, DRAM, and optionally GPU or platform-level readings). Second, the dynamic portion is distributed across processes and containers according to their observed resource usage, while idle power is assigned using a domain-specific default policy.

Dynamic power is attributed proportionally using ratio-based models. For each domain, Kepler computes the energy delta over the sampling interval and distributes it according to a usage metric selected for that domain. Instructions, cache misses, and CPU time are used as primary signals, with fallbacks when a metric is unavailable. GPU dynamic power is attributed based on GPU compute utilization. Platform-level power, when available, is treated as a residual domain: after subtracting CPU and DRAM power, the remaining portion is shared across workloads using the designated default metric or, if none is configured (which is the case), an equal split.

Idle energy is handled separately. Kepler maintains a rolling estimate of minimum node-level power for each domain and treats these values as idle baselines. In the default configuration, idle energy is divided evenly across all active workloads during the attribution interval. While this behaviour differs from protocol recommendations that scale idle power by container size, Kepler applies a uniform policy across domains to ensure attribution completeness.

Attribution operates at process granularity, with container and pod values obtained by aggregating the processes mapped to each cgroup. This approach allows Kepler to attribute energy to short-lived or multi-process containers while retaining compatibility with Kubernetes metadata.

Kepler performs attribution at a fixed internal update interval (default: 3\,s). All usage metrics and energy deltas within that interval are aggregated before attribution is computed. Because Prometheus scrapes occur independently of Kepler’s internal loop, the exported time series may reflect misalignment when the scrape period is not a multiple of the update interval. This can lead to visible step patterns or oscillations, particularly for dynamic workloads. Despite these limitations, the model provides a coherent and workload-oriented view of node-level energy consumption suitable for cloud-native observability scenarios.

\subsubsection{Observed Behavior and Limitations}
\label{subsec:kepler-limitations}

Several studies and code-level inspections reveal that Kepler’s attribution behaviour exhibits systematic limitations that affect accuracy and interpretability. The most comprehensive empirical evaluation to date, conducted by Pijnacker et al., demonstrates that attribution inaccuracies arise even when node-level power estimation is reliable\parencite{pijnackerEstimatingContainerlevelPower2024}. Their experiments highlight that idle power is often distributed to containers that are no longer active, including Completed pods, and that dynamic power can be reassigned inconsistently when containers are added or removed. These effects stem from the coupling of process-level accounting with container lifecycle events, which may lag behind cgroup or Kubernetes metadata updates.

Timing mismatches further contribute to attribution artifacts. High-frequency CPU and cgroup statistics are combined with slower telemetry sources such as Redfish, whose update intervals may span tens of seconds. When workload intensity changes during such periods, Kepler may assign disproportionately large or small dynamic energy shares to individual containers. Similar behavior occurs at the Prometheus interface when scrape intervals do not align with Kepler’s internal update loop, producing visible oscillations in the exported time series.

Source-code inspection reinforces these observations. Numerous unimplemented or placeholder sections (e.g. TODO markers) affect key components of the ratio-based attribution model and default configuration paths. In particular, some domains lack explicit usage metrics, leading to fallback behaviour and equal-cost splitting regardless of container activity. GPU attribution relies on a single utilization metric and is therefore sensitive to the temporal behaviour of NVML’s sampling. Together, these issues introduce variability across domains and reduce the transparency of the resulting per-container energy values.

Lifecycle handling also presents challenges. Because container metadata is aggregated from process-level information, short-lived or Completed pods may retain residual energy assignments. Conversely, system processes that cannot be mapped cleanly to Kubernetes abstractions may absorb unassigned power, obscuring the relationship between application behaviour and observed consumption.

Overall, these limitations underscore that Kepler provides a practical but imperfect approximation of container-level energy consumption. The observed behaviour motivates the research gaps identified at the end of this section, particularly the need for finer temporal resolution, explicit handling of idle and residual power, configurable attribution models, and more robust reconciliation across heterogeneous telemetry sources.

\subsubsection{Kepler v0.10.0}
\label{subsec:kepler-v010}

In July 2025, Kepler underwent a substantial architectural redesign with the release of version~0.10.0\parencite{kepler_current}. The new implementation replaces many of the privileged operations used in earlier versions, removing the need for \code{CAP\_BPF} or \code{CAP\_SYSADMIN} and reducing reliance on kernel instrumentation. Instead, Kepler now obtains all workload statistics from read-only \code{/proc} and cgroup interfaces. This reduction in privilege requirements significantly improves deployability and security, particularly for managed Kubernetes environments where eBPF- or perf-based approaches are infeasible.

The redesign also introduces a markedly simplified attribution model. Whereas earlier versions combined multiple hardware and software counters (e.g.\ instructions, cache misses, GPU utilization) to estimate dynamic energy, Kepler~v0.10.0 relies exclusively on CPU time as the usage metric. Node-level dynamic energy is computed by correlating RAPL deltas with aggregate CPU activity, and each workload receives a proportional share of this value based solely on its CPU time fraction. Idle energy is not distributed to containers and instead remains part of a node-level baseline. Containers, processes, and pods are treated as independent consumers drawing from the same active-energy pool, with no dependence on process-derived aggregation.

These changes increase robustness and predictability: the simplified model is easier to reason about, less sensitive to heterogeneous workloads or timing mismatches, and compatible with environments where kernel-level measurement facilities are unavailable. However, the loss of metric flexibility substantially reduces modeling fidelity. Fine-grained distinctions between compute-bound and memory-bound tasks are no longer observable, and the attribution model presumes a strictly linear relationship between CPU time and power consumption. As a result, Kepler~v0.10.0 no longer targets high-accuracy energy attribution but instead emphasises operational stability and minimal overhead.

For the purposes of this thesis, Kepler~v0.10.0 is relevant primarily as an indication of the project’s strategic shift toward simplicity and broad deployability. Its CPU-time-only model is not suitable as a basis for Tycho, whose objectives require higher temporal resolution, more diverse metric inputs, and explicit handling of domain-level energy contributions. Accordingly, the remainder of this chapter focuses on the behaviour of Kepler~v0.9.x, which remains the most representative version for research-oriented attribution discussions.

\subsection{KubeWatt}
\label{sec:kubewatt}

KubeWatt is a proof-of-concept exporter developed by Pijnacker as a direct response to the attribution issues uncovered in Kepler.\parencite{pijnackerEstimatingContainerlevelPower2024,pijnackerContainerlevelEnergyObservability2025}
Rather than extending Kepler’s complex pipeline, KubeWatt implements a deliberately narrow but transparent model that focuses on correcting three specific problems: misattribution of idle power, leakage of energy into generic ``system processes'', and unstable behaviour under pod churn. It targets Kubernetes clusters running on dedicated servers and assumes that a single external power source per node is available (in the prototype, Redfish/iDRAC).

A central design decision is the strict separation between \emph{static} and \emph{dynamic} power. Static power is defined as the baseline cost of running the node and its control plane in an otherwise idle state. KubeWatt measures or estimates this baseline once and treats it as a constant; it is \emph{not} attributed to containers. Dynamic power is then computed as the difference between total node power and this static baseline and is the only quantity distributed across workloads. Control-plane pods are explicitly excluded from the dynamic attribution set, so their idle consumption remains part of the static term and does not pollute application-level metrics.

To obtain the static baseline, KubeWatt provides two initialization modes. In \emph{base initialization}, the cluster is reduced to an almost idle state (only control-plane components), and node power is sampled for a few minutes. The static power value is computed as a simple average, yielding a highly stable estimate under the test conditions. When workloads cannot be stopped, \emph{bootstrap initialization} fits a regression model to time series of node power and CPU utilization collected during normal operation. The regression is evaluated at the average control-plane CPU usage to infer the static baseline. This mode is more sensitive to workload characteristics and SMT effects but provides a practical fallback when base initialization is not feasible.

During normal operation, KubeWatt runs in an \emph{estimation mode} that attributes dynamic node power to containers proportionally to their CPU usage. CPU usage is obtained from the Kubernetes metrics API (\code{metrics.k8s.io}) at node and container level. The denominator explicitly sums only container CPU usage; system processes, cgroup slices, and other non-container activity are excluded by construction. This corrects a key source of error in Kepler, where slice-level metrics and kernel processes could receive non-trivial fractions of node power. Under stable workloads and at the relatively coarse sampling interval used in the prototype, KubeWatt achieves container-level power curves that align well with both iDRAC readings and observed CPU utilisation, and it behaves robustly when large numbers of idle pods are created and deleted.

The scope of KubeWatt is intentionally narrow. It is CPU-only, uses a single external power source per node, assumes that Kubernetes is the only significant workload on the machine, and does not attempt to model GPU, memory, storage, or network energy. It also inherits the temporal limitations of the Kubernetes metrics pipeline and treats Redfish power readings as instantaneous, without explicit latency compensation. Nevertheless, KubeWatt demonstrates that a simple, well-documented ratio model with explicit static--dynamic separation and strict cgroup filtering can eliminate several of Kepler’s most problematic attribution artefacts. These design principles are directly relevant for the attribution redesign pursued in this thesis and inform the requirements placed on Tycho’s more general, multi-source architecture.

\subsection{Other Tools (Brief Overview)}
\label{sec:other-tools}

Beyond Kepler, several tools illustrate the methodological diversity in container- and process-level energy attribution, although they are not central to the Kubernetes-specific challenges addressed in this thesis. Scaphandre\parencite{scaphandre_documentation} provides a lightweight proportional attribution model based exclusively on CPU time and RAPL deltas. Its design emphasises simplicity and portability, offering basic container mapping through cgroups but limited control over sampling behaviour or attribution semantics. SmartWatts\parencite{fieni2020smartwatts}, by contrast, represents a more sophisticated approach: it builds performance-counter-based models that self-calibrate against RAPL measurements and adapt dynamically to the host system. While effective in controlled environments, SmartWatts requires access to perf events, provides only CPU and DRAM models, and is not deeply integrated with Kubernetes abstractions.

A broader ecosystem of lightweight tools (e.g.\ CodeCarbon\parencite{codecarbon} and related library-level estimators) demonstrates further variation in scope and assumptions, but these generally target high-level application profiling rather than system-wide workload attribution. Collectively, these tools highlight a spectrum of design choices (from simplicity and portability to model-driven estimation) but none address the combination of high-resolution telemetry, multi-tenant attribution, and Kubernetes metadata integration that motivates the development of Tycho.

\subsection{Cross-Tool Limitations Informing Research Gaps}
\label{sec:cross-tool-limitations}

Across the surveyed tools, several structural limitations recur despite substantial differences in design philosophy and implementation. First, temporal granularity remains insufficient: although hardware interfaces such as RAPL support millisecond-level updates, most tools aggregate measurements over multi-second intervals. This obscures short-lived workload behaviour and reduces attribution fidelity, particularly in heterogeneous or bursty environments. Second, all tools depend on telemetry sources whose internal semantics are only partially documented. Ambiguities regarding RAPL domain coverage, NVML power reporting, or BMC-derived node power constrain both the interpretability and the auditability of reported metrics, reinforcing the black-box character of current measurement pipelines.

Idle-power handling presents a further source of inconsistency. Tools differ widely in how idle power is defined, whether it is attributed, and to whom. These choices are often implicit, undocumented, or constrained by implementation artefacts, leading to attribution patterns that are difficult to interpret or reproduce. Multi-domain coverage is similarly limited: existing tools focus primarily on CPU and, to a lesser extent, DRAM or GPU consumption, leaving storage, networking, and other subsystems unmodelled despite their relevance to node-level energy use.

Metadata lifecycle management also emerges as a common limitation. Rapid container churn, transient pods, and the interaction between Kubernetes and cgroup identifiers can produce incomplete or stale workload associations, affecting attribution stability. Finally, attribution models themselves are typically rigid. Most tools hard-code a specific proportionality assumption (commonly CPU time or a single hardware counter) and provide limited support for calibration, uncertainty quantification, or alternative modelling philosophies.

Taken together, these limitations reveal structural gaps in current approaches to container-level energy attribution. They motivate the need for tools that combine high-resolution telemetry handling, transparent and configurable attribution logic, robust metadata management, and principled treatment of uncertainty. The next section distills these observations into concrete research gaps that inform the design objectives of Tycho.

\section{Research Gaps}
\label{sec:research-gaps}

This section synthesises the findings from the preceding analyses of telemetry sources, temporal behaviour, and existing tools. 
Across these perspectives, a set of structural limitations emerges that fundamentally constrains accurate and explainable energy attribution in Kubernetes environments. 
These limitations arise at three intertwined layers: the measurement interfaces exposed by hardware and kernel subsystems, the attribution models built on top of these measurements, and the operational context in which Kubernetes workloads execute. 
Taken together, they demonstrate the absence of a framework that provides high temporal precision, transparent modelling assumptions, and robustness to container lifecycle dynamics. 
The gaps identified below define the technical requirements that motivate the design of Tycho.

\subsection*{(1) Measurement Gaps: Temporal Resolution and Telemetry Semantics}

Existing tools do not exploit the full temporal capabilities of modern hardware telemetry. Interfaces such as RAPL offer fast, reliable update frequencies, yet tools operate on fixed multi-second loops, causing short-lived or bursty activity to be temporally averaged away. Moreover, latency mismatches between high-frequency utilization signals (e.g.\ cgroups, perf counters) and low-frequency power interfaces (e.g.\ Redfish/BMC) introduce structural attribution errors that are not explicitly modelled or corrected.

A related issue is the opacity of hardware telemetry. RAPL, NVML, and BMC power sensors provide indispensable data, but their domain boundaries, averaging windows, and internal update behaviour are insufficiently documented. This prevents rigorous interpretation of reported values and inhibits the development of calibration or uncertainty models. Finally, measurement coverage remains incomplete: while CPU and DRAM domains are widely supported, no standardised telemetry exists for storage, networking, or other subsystems. Current tools treat these components either implicitly (as part of ``platform’’ power) or not at all.

\subsection*{(2) Attribution Model Gaps: Rigidity, Idle Power, and Domain Consistency}

Current attribution models rely on rigid proportionality assumptions (typically CPU time, instructions, or a single hardware counter) without considering alternative modelling philosophies. Idle power remains a persistent source of inconsistency: tools variously divide it evenly, proportionally, or not at all, often without documenting the rationale. These choices have substantial effects on per-container energy values, particularly in lightly loaded or heterogeneous systems.

At the domain level, attribution methods are not unified. CPU, DRAM, uncore, GPU, and platform energy are treated through incompatible heuristics, and many domains fall back to equal distribution when no clear usage signal is defined. None of the surveyed tools quantify uncertainty, despite relying on noisy, coarse, or undocumented telemetry sources. As a result, attribution outputs appear deterministic even when they rest on incomplete or ambiguous measurement assumptions.

\subsection*{(3) Metadata and Lifecycle Gaps: Churn, Timing, and Virtualization}

Container-level attribution requires consistent mapping between processes, cgroups, and Kubernetes metadata. Existing tools struggle in scenarios with rapid container churn, ephemeral or Completed pods, and multi-process containers. Mismatches between metadata refresh cycles and metric sampling lead to stale or missing associations, which propagate into attribution artefacts.

Energy attribution inside virtual machines remains essentially unsolved. No standard mechanism exists for exposing host-side telemetry to guest systems in a way that preserves temporal alignment and attribution consistency. The limited QEMU-based passthrough available in Scaphandre is not generalisable, and conceptual proposals (e.g.\ Kepler’s hypercall mechanism) remain unimplemented. Given the prevalence of cloud-hosted Kubernetes clusters, this constitutes a major practical limitation.

\subsection*{(4) Usability, Transparency, and Operational Gaps}

For most tools, implementation assumptions, fallback paths, and attribution decisions are implicit. Users cannot easily distinguish measured values from estimated ones, nor identify the assumptions underlying attribution outputs. This lack of transparency reduces trust and complicates debugging.

Operational constraints further restrict applicability. Tools that require privileged kernel instrumentation (eBPF, \texttt{perf\_event\_open}) are unsuitable for many production clusters, while tools designed around unprivileged access often sacrifice modelling fidelity. At the same time, developers, operators, and researchers have fundamentally different observability needs, yet existing tools optimise for only one audience at a time. None provide configurable attribution modes or role-specific abstractions.

\subsection*{(5) Missing Support for Calibration and Validation}

Beyond isolated exceptions, existing tools provide limited mechanisms for systematic calibration or validation of their attribution models. 
KubeWatt is one of the few systems that performs explicit baseline calibration, offering both an idle-power measurement mode and a statistical fallback for environments without idle windows. 
Kepler offers no structured calibration workflow, and its estimator models lack reproducible training procedures. 
SmartWatts introduces online model recalibration but focuses narrowly on performance-counter regression, leaving node-level baselines, multi-domain alignment, and external ground-truth integration unaddressed.

Across all tools, there is no standardized path to incorporate external measurements (for example from wall-power sensors or BMC-level telemetry) to validate or refine model behaviour. 
Idle power is seldom isolated as a first-class parameter, attribution error is rarely quantified, and no system provides uncertainty estimates that reflect measurement or modelling limitations. 
Without such calibration and validation capabilities, attribution accuracy cannot be assessed, corrected, or improved over time—an essential requirement for any system intended to provide trustworthy, high-resolution energy insights in Kubernetes environments.

\section{Summary}
\label{sec:background_summary}

The analyses in this chapter reveal that modern server platforms provide a heterogeneous and only partially documented set of telemetry interfaces whose temporal and semantic properties fundamentally constrain container-level energy attribution. Hardware-integrated sources such as RAPL and NVML expose valuable domain-level energy information but differ substantially in update behaviour, averaging semantics, and domain completeness. Out-of-band telemetry via Redfish provides stable whole-system measurements but at coarse and irregular temporal granularity. Software-exposed metrics offer fine-grained visibility into workload behaviour, yet they measure utilisation rather than power and depend entirely on polling strategies for temporal interpretation.

Temporal irregularities, internal averaging, and undocumented sensor behaviour reduce the effective precision of all telemetry sources, especially when attempting to capture short-lived workload dynamics. Existing tools aggregate these heterogeneous signals using fixed multi-second sampling loops and rigid proportionality assumptions, which leads to systematic attribution artefacts. Idle power is treated inconsistently across systems, residual power is frequently conflated with workload activity, and multi-domain attribution remains fragmented in both semantics and implementation. Metadata churn and asynchronous refresh cycles further complicate the mapping between processes, containers, and Kubernetes abstractions, reducing the stability and interpretability of attribution outputs.

The cross-tool evaluation confirms these structural limitations. Kepler provides broad telemetry integration but struggles with timing mismatches, incomplete domain semantics, and opaque idle handling. Kubewatt demonstrates the importance of explicit baseline separation and cgroup filtering, yet remains limited to single-domain CPU-based estimation. Other tools illustrate a spectrum of design choices but do not address the combined challenges of high-frequency telemetry, multi-domain attribution, container lifecycle dynamics, and Kubernetes integration.

Together, these findings motivate the need for a framework that provides high temporal fidelity, transparent modelling assumptions, unified domain treatment, explicit handling of idle and residual energy, and robust metadata reconciliation. These requirements form the conceptual and architectural foundations developed in \chapref{ch:concepts}, which introduces the methodological principles guiding the design of Tycho.