\chapter{Background and Related Research}
\label{ch:background}

% This chapter surveys existing research and practical knowledge about server-level
% energy measurement, hardware and software telemetry sources, and attribution tools.
% It provides the empirical and factual basis for the conceptual foundations in
% Chapter~\ref{ch:concepts}. This chapter MUST NOT contain Tycho design decisions,
% conceptual attribution models, or implementation details.

\noindent This chapter summarises the current state of research and industrial knowledge on server-level energy measurement. Its focus is limited to what the literature reports about available telemetry sources, measurement techniques, and existing attribution tools. The discussion is descriptive rather than conceptual: it does not introduce attribution principles, methodological reasoning, or design considerations, which are addressed in \chapref{ch:concepts}. Extended background material is available in \hyperref[appendix_vt2]{Appendix~A} and the present chapter integrates only those findings that are directly relevant for understanding the research landscape.

\section{Energy Measurement in Modern Server Systems}
\label{sec:energy_measurement_systems}

The energy consumption of modern servers arises from a heterogeneous set of subsystems, including CPUs, GPUs, memory, storage devices, network interfaces, and platform management components. Prior research highlights that these subsystems expose highly unequal visibility into their power behaviour, since measurement capabilities, granularity, and accuracy differ significantly across hardware generations and vendors \parencite{lin2020taxonomy, long2022review}. Some domains provide direct telemetry, while others can only be approximated through software-derived activity metrics. As a result, no single interface offers complete or temporally consistent power information, and most studies rely on a single source or combine multiple sources to approximate system-level consumption. This fragmented measurement landscape forms the basis for much of the existing work on power modelling, validation, and multi-source energy estimation in server environments.

\subsection{Energy Attribution in Multi-Tenant Environments}
\label{subsec:attribution_context}

Several studies identify containerised and multi-tenant systems as challenging environments for energy attribution. Containers share the host kernel and rely on common processor, memory, storage, and network subsystems, which removes the isolation boundaries present in virtual machines and prevents direct measurement of per-container power. Research reports that workloads running concurrently on the same node create interference effects across hardware domains, leading to utilisation patterns that correlate only loosely with actual energy consumption \parencite{lin2020taxonomy}. Modern orchestration platforms further increase attribution difficulty through highly dynamic execution behaviour: containers are created, destroyed, and rescheduled at high frequency, often numbering in the thousands on large clusters. These rapid lifecycle changes produce volatile metadata and short-lived resource traces that are difficult to align with node-level telemetry. Collectively, the literature treats container-level energy attribution as an estimation problem constrained by incomplete observability, heterogeneous measurement quality, and continuous runtime churn.

\subsection{Telemetry Layers in Contemporary Architectures}
\label{subsec:telemetry_layers}

Modern servers expose power and activity information through two largely independent telemetry layers. The first consists of in-band mechanisms that are visible to the operating system, including on-die energy counters, GPU management interfaces, and kernel-level resource statistics. These interfaces typically offer higher sampling rates and finer granularity, but their accuracy and coverage vary across hardware generations and vendors. Prior work notes that in-band telemetry often represents estimated rather than directly measured power and that several domains, such as network and storage devices, expose only partial or indirect information.

The second layer is out-of-band telemetry provided by baseboard management controllers through interfaces such as IPMI or Redfish. These systems aggregate sensor readings independently of the host and report stable, whole-system power values at coarse temporal resolution. Empirical studies show that out-of-band telemetry provides useful system-level accuracy, although update intervals and measurement precision differ substantially between vendors \parencite{wang2019empirical}. Compared with instrument-based measurements, which remain the benchmark for high-fidelity evaluation but are impractical at scale, both in-band and out-of-band methods represent trade-offs between granularity, availability, and measurement reliability.

Combined, these layers form a heterogeneous telemetry landscape in which sampling rates, accuracy, and domain coverage differ significantly, motivating the use of multi-source measurement approaches in research.

\subsection{Challenges for Container-Level Measurement}
\label{subsec:container_challenges}

Existing research identifies several factors that complicate accurate energy measurement for containerised workloads. Large-scale trace analyses show that cloud environments exhibit substantial churn, with many tasks being short-lived and resource demands changing rapidly over time \parencite{reissHeterogeneityDynamicityClouds2012}. Such dynamism limits the observability of fine-grained resource usage and makes it difficult to capture short execution intervals with sufficient temporal resolution.

Monitoring studies further report inconsistencies across the different layers that expose resource information for containers. In multi-cloud settings, observability often depends on heterogeneous monitoring stacks, leading to fragmented visibility and non-uniform coverage of system activity \parencite{waseemContainerizationMultiCloudEnvironment2025}. Even within a single host, performance counters obtained from container-level interfaces may diverge from system-level measurements. Empirical evaluations demonstrate that container-level CPU and I/O counters can underestimate actual activity by a non-negligible margin, and that co-located workloads introduce contention effects that distort these metrics \parencite{casalicchioStateoftheartContainerTechnologies2020}.

These findings indicate that container-level measurement operates under conditions of rapid workload turnover, heterogeneous monitoring behaviour, and imperfect resource visibility. As a consequence, the literature treats container energy attribution as a problem constrained by incomplete and potentially biased measurement signals rather than as a directly measurable quantity.

\section{Hardware and Software Telemetry Sources}
\label{sec:measurement_techniques}

This section outlines the primary telemetry sources used to observe power and resource behaviour in modern server systems. It summarises established research on external measurement devices, firmware-level interfaces, on-die energy counters, accelerator telemetry, and kernel-exposed resource metrics. The emphasis is on reporting the properties and empirical characteristics documented in prior work, without interpreting these signals conceptually or analysing their temporal behaviour, which are addressed in later sections. A comprehensive technical discussion is provided in \appchapterref{A}{vt2_Chapter2}; the present section extracts only the findings relevant for understanding the measurement landscape.

\subsection{Direct Hardware Measurement}
\label{subsec:direct_measurement}

Direct physical instrumentation remains the most accurate method for measuring server power consumption. External power meters or inline shunt-based devices can capture node-level energy usage with high fidelity, and research frequently uses such instrumentation as a ground truth for validating software-reported power values. Studies employing dedicated measurement setups, such as custom DIMM-level sensing boards, demonstrate that high-frequency sampling and component-level granularity are technically feasible but require bespoke hardware and non-trivial integration effort \parencite{desrochers2016validation}. Lin et al.\ classify these approaches as offering very high data credibility but only coarse spatial granularity and limited scalability in operational environments \parencite{lin2020taxonomy}.

Recent work on specialised sensors, such as the PowerSensor3 platform\parencite{van2025powersensor3} for high-rate voltage and current monitoring of GPUs and other accelerators, illustrates ongoing interest in hardware-centric power measurement. However, these systems share the same fundamental drawback: deployment across production servers is complex, costly, and incompatible with large-scale or multi-tenant settings. As a consequence, direct instrumentation is predominantly used in controlled experiments or for validation of other telemetry sources, rather than as a primary measurement mechanism in real-world server infrastructures.

\subsection{Legacy Telemetry Interfaces (ACPI, IPMI)}
\label{subsec:legacy_telemetry}

Early power-related telemetry on server platforms was primarily exposed through ACPI and IPMI. ACPI provides a standardised interface for configuring and controlling hardware power states, but it does not offer real-time energy or power readings. The interface exposes only abstract performance and idle states defined by the firmware \parencite{uefi_acpi_6_6}, and these states do not include the instantaneous power information required for empirical energy measurement. Consequently, ACPI has seen little use in modern power estimation research.

IPMI, accessed through the baseboard management controller, represents an older class of out-of-band telemetry that predates Redfish. Although widely supported across server hardware, IPMI power values are known to be coarse, slowly refreshed, and often inaccurate when compared with external instrumentation. Empirical studies report multi-second averaging windows, substantial quantisation effects, and unreliable idle power readings \parencite{kavanagh2016accuracy, kavanagh2019rapid}. These limitations, together with the availability of more precise alternatives, have led IPMI to be largely superseded by Redfish on contemporary server platforms.

\subsection{Redfish Power Telemetry}
\label{subsec:redfish}

Redfish is the modern out-of-band management interface available on contemporary server platforms and is designed as the successor to IPMI. It exposes system-level telemetry through a RESTful API implemented on the baseboard management controller (BMC), providing access to whole-node power readings derived from on-board sensors. Prior work consistently shows that Redfish delivers higher precision than IPMI, with lower quantisation artefacts and more stable readings across power ranges \parencite{wang2019empirical}. In controlled experiments, Redfish achieved a mean absolute percentage error of roughly three percent when compared to a high-accuracy power analyser, outperforming IPMI in all evaluated power intervals.

A key limitation of Redfish is its temporal granularity. Empirical studies report that power values exhibit non-negligible staleness, with refresh delays of approximately 200\,ms \parencite{wang2019empirical}. This latency restricts the ability of Redfish to capture short bursts of activity or rapid fluctuations in dynamic workloads. Accuracy and responsiveness also vary across vendors, reflecting differences in embedded sensors, BMC firmware, and management controller architectures.

The interface is widely deployed in real-world infrastructure. Modern enterprise servers from Dell, HPE, Lenovo, Cisco, and Supermicro routinely expose power telemetry via Redfish as part of their standard BMC firmware \parencite{herrlinAccessingOnboardServer2021}. Out-of-band monitoring studies further highlight that Redfish avoids the overheads and failure modes associated with in-band agents \parencite{aliRedfishNagiosScalableOutofBand2022}. In practice, Redfish implementations tend to provide stable low-frequency updates suitable for coarse-grained power reporting.

Preliminary measurements conducted for this thesis also observed irregular update intervals on the evaluated hardware, occasionally extending into the multi-second range. While this behaviour is specific to a single system and not generalisable, it reinforces the literature’s position that Redfish telemetry exhibits meaningful vendor-dependent variability and remains unsuitable for fine-grained temporal correlation.

Overall, Redfish provides accessible, reliable whole-node power telemetry at coarse temporal resolutions, making it valuable for long-interval monitoring and for validating other measurement sources, but inappropriate for attributing energy consumption to short-lived or rapidly fluctuating containerised workloads.

\subsection{RAPL Power Domains}
\label{subsec:rapl}

Running Average Power Limit (RAPL) provides hardware-backed energy counters for several internal power domains of a processor package. Originally introduced by Intel and later adopted in a compatible form by AMD, RAPL exposes energy measurements via model-specific registers that can be accessed directly or through higher-level interfaces such as the Linux \texttt{powercap} framework or the \texttt{perf-events} subsystem \parencite{intel-sdm, raffin2024dissecting}. Raffin et~al.\ provide a detailed comparison of these access mechanisms, noting that MSR, powercap, perf-events, and eBPF differ mainly in convenience, required privileges, and robustness; all can retrieve equivalent RAPL readings when implemented correctly \parencite{raffin2024dissecting}. They recommend accessing RAPL via the powercap interface, which is easiest to implement reliably and suffers from no overhead penalties when compared with more low-level methods.

Intel platforms typically expose several well-established RAPL domains, including the processor package, the core subsystem, and (on many server architectures) a DRAM domain \parencite{hackenberg2015energy}. These domains have been validated extensively against external measurement equipment. Studies report that the combination of package and DRAM energy tracks CPU-and-memory power with good accuracy from Haswell onwards, which has led to RAPL becoming the primary fine-grained energy source in server-oriented research \parencite{hackenberg2013power, desrochers2016validation, alt2024experimental, kennes2023measuring}. More recent work on hybrid architectures such as Alder Lake confirms that RAPL continues to correlate well with external measurements under load, while precision decreases somewhat in low-power regimes \parencite{schone2024energy}. Across these studies, RAPL is generally regarded as sufficiently accurate for scientific analysis when its domain boundaries and update characteristics are considered \parencite{raffin2024dissecting}.

AMD implements a RAPL-compatible interface with a similar programming model but a reduced set of domains. Zen 1 through Zen 4 processors expose package and core domains only, without a dedicated DRAM domain \parencite{schone2021energy, raffin2024dissecting}. Sch\"one et~al.\ show that, as a consequence, memory-related energy may not be represented explicitly in AMD’s RAPL output, leading to a smaller portion of total system energy being observable through the package domain alone \parencite{schone2021energy}. This limitation primarily concerns domain completeness rather than measurement correctness: for compute-intensive workloads, package-domain values behave consistently, but workloads with significant memory activity exhibit a larger gap relative to whole-system measurements because DRAM energy is not separately reported. Raffin et~al.\ further note that, on the evaluated Zen-based server, different kernel interfaces initially exposed inconsistent domain sets; this was later corrected upstream, illustrating that AMD support is evolving and still maturing within the Linux ecosystem \parencite{raffin2024dissecting}.

Technical considerations also apply to both Intel and AMD platforms. RAPL counters have finite width and wrap after sufficiently large energy accumulation, requiring consumers to implement overflow correction \parencite{khan2018rapl, raffin2024dissecting}. The counters do not include timestamps, and empirical work shows that actual update intervals may deviate from nominal values, complicating precise temporal correlation with other telemetry \parencite{hackenberg2013power, jay2023experimental}. On some Intel platforms, security hardening measures such as energy filtering reduce temporal granularity for certain domains to mitigate side-channel risks \parencite{lipp2021platypus, intel2023, schone2024energy}. In virtualised environments, RAPL access may be trapped by the hypervisor, increasing latency and introducing small deviations from bare-metal behaviour \parencite{jay2023experimental}.

In summary, RAPL provides a widely used and comparatively fine-grained source of processor-side energy telemetry. Intel platforms typically offer multiple validated domains, including DRAM, enabling a broader view of CPU-and-memory energy. AMD platforms expose fewer domains and therefore provide a more limited perspective on total system power, particularly for memory-intensive workloads. These differences in domain coverage, measurement scope, and software integration need to be taken into account when using RAPL as a basis for energy analysis.

\subsection{GPU Telemetry}
\label{subsec:gpu_telemetry}

Unlike CPUs, where power and utilization telemetry is supported through standardised
interfaces, GPU energy visibility relies primarily on vendor-specific mechanisms.
For NVIDIA devices, two interfaces dominate this landscape: the \textit{NVIDIA Management
Library} (NVML), which has become the industry standard, and the \textit{Data Center GPU
Manager} (DCGM), a less widely used management layer that also exposes telemetry.

\subsubsection{NVML}
NVML is NVIDIA’s primary interface for device-level monitoring and underpins tools such
as \texttt{nvidia-smi}.  
It provides access to power, energy (on selected data-center GPUs), GPU utilization,
memory usage, clock frequencies, thermal state, and various health and throttle
indicators.  
Among these, power and utilization are most relevant for energy analysis.

NVML power values represent board-level estimates derived from on-device sensing
circuits and are shaped by internal averaging and architecture-dependent update
behaviour.  
Recent empirical studies across modern devices show that NVML produces fresh samples only
intermittently and applies smoothing that reduces the visibility of short-lived power
changes, while steady-state power levels remain comparatively accurate
\parencite{yang2024accurate}.  
On the Grace--Hopper GH200, these effects are pronounced: NVML reflects a coarse internal
averaging interval and therefore underrepresents short kernels and transient peaks
relative to higher-frequency system interfaces
\parencite{hernandezPreliminaryStudyFineGrained2025}.  
These findings indicate that NVML captures long-term power behaviour reliably but
inherently limits fine-grained visibility.  
Despite these constraints, existing studies consistently find that NVML provides
reasonably accurate steady-state power estimates on modern data-center GPUs and currently
represents the most reliable and widely supported mechanism for obtaining GPU power
telemetry in practical systems \parencite{hernandezPreliminaryStudyFineGrained2025}.

GPU utilization provides contextual information about device activity.  
It reports the proportion of time during which the GPU is executing any workload rather
than the fraction of computational capacity in use, making it a coarse activity
indicator rather than a detailed performance metric
\parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{DCGM}
DCGM is NVIDIA’s management and observability framework designed for data-center
deployments.  
It aggregates telemetry, performs health monitoring, exposes thermal and throttle state,
and provides detailed visibility in environments that employ Multi-Instance GPU (MIG)
partitioning.  
However, DCGM’s power and utilization metrics are derived from the same underlying
measurement sources as NVML.  
In practice, DCGM is far less commonly used for energy analysis because it does not
provide higher-fidelity power telemetry; instead, it applies additional aggregation and
is typically deployed with coarse sampling intervals, especially when used through
exporters in cluster monitoring systems.  
DCGM therefore represents an alternative access path to the same measurements rather than
a distinct source of energy-related information.  

DCGM is considerably less common in both research and operational practice, with most
GPU monitoring systems relying primarily on NVML while DCGM appears only occasionally in
cluster-level deployments \parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsubsection{Summary}
NVML and DCGM jointly define the available mechanisms for GPU telemetry in cloud
environments.  
NVML is the dominant and broadly supported interface for power and utilization
measurement, while DCGM extends it with operational metadata and management integration.
Current studies consistently show that both interfaces expose averaged, device-level
power estimates that capture long-term behaviour but are inherently limited in their
ability to represent short-duration activity or fine-grained workload structure.  
These characteristics form the scientific foundation for later discussions of temporal
behaviour and measurement methodology.

\subsection{Software-Exposed Resource Metrics}
\label{subsec:software_metrics}

In addition to hardware telemetry, Linux and Kubernetes expose a wide range of
software-level resource metrics that describe system and workload activity.
These metrics do not measure power directly but provide essential behavioural
context that complements RAPL, Redfish, and GPU telemetry.

\subsubsection{CPU and Memory Activity Metrics}

Linux provides several complementary mechanisms for tracking CPU and memory
usage.  
Global counters such as \path{/proc/stat} record cumulative CPU time since boot,
while per-task statistics in \path{/proc/<pid>} expose user-mode and kernel-mode
execution time with high granularity \parencite{kernelprocfs}.  
Control groups (cgroups) provide container-level CPU and memory accounting and
form the primary basis for utilisation metrics inside Kubernetes
\parencite{kernelcgroupv1, kernelcgroupv2}.  
Higher-level tools such as cAdvisor and metrics-server aggregate this
information via Kubelet, but at significantly lower update rates.

Event-driven approaches provide substantially finer resolution.  
eBPF allows dynamic attachment to kernel events such as context switches,
scheduling decisions, and I/O operations, enabling near-real-time capture of
per-task CPU activity with low overhead
\parencite{ciliumbpf, cassagnesRiseEBPFNonintrusive2020}.  
Hardware performance counters accessed through \code{perf} offer insight into
instruction counts, cycles, cache behaviour, and stalls
\parencite{Gregg2017CpuUtilizationWrong}.  
These sources provide detailed behavioural information but still represent
utilisation rather than energy.

\subsubsection{Storage Activity Metrics}

Storage subsystems do not expose real-time power telemetry, yet Linux provides a
rich set of activity indicators.  
Per-process statistics in \path{/proc/<pid>/io} track bytes read and written,
while cgroup I/O controllers report aggregated container-level metrics.  
Subsystem-specific tools such as \code{smartctl} and \code{nvme-cli} reveal
additional device characteristics, queue behaviour, and state transitions
\parencite{smartmontools_github, nvmecli_github}.  

In the absence of hardware power sensors, multiple works propose
workload-dependent energy models for storage devices
\parencite{choDesignTradeoffsSSDs2015, liWhichStorageDevice2014,
borbaModelingApproachEstimating2022}.  
These models can yield accurate estimates when calibrated for a specific device
but do not generalise across heterogeneous hardware due to differences in flash
controllers, firmware, and internal data paths.

\subsubsection{Network and PCIe Device Metrics}

Network interfaces provide byte and packet counters via \path{/proc/net/dev}, but
expose no dedicated power telemetry.  
Research models for NIC energy consumption exist
\parencite{sohanCharacterizing10Gbps2010, basmadjianCloudComputingIts2012,
baneshiAnalyzingPerApplicationEnergy2024}, yet all rely on device-specific idle
and active power characteristics that are not available at runtime.  
Similarly, PCIe devices support abstract power states as defined by the PCIe
specification \parencite{technotes_pci_power_2024}, but these states do not
reflect instantaneous power usage and thus offer only coarse activity signals.

\subsubsection{Secondary System Components}

Components such as fans, motherboard logic, and power delivery subsystems rarely
expose fine-grained telemetry.  
Although some BMC implementations report coarse sensor values, these readings
are inconsistent across platforms and generally unsuitable for high-resolution
analysis.  
Consequently, research commonly treats these subsystems as part of the residual
power that scales with the activity of primary components
\parencite{basmadjianCloudComputingIts2012}.

\subsubsection{Model-Based Estimation Approaches}

Because software-visible metrics capture detailed workload behaviour, many works
propose inferring energy consumption from utilisation using regression or
stochastic models
\parencite{fan2007power, hsu2011power, song2013unified,
arjonaarocaMeasurementbasedAnalysisEnergy2014}.  
While these models can be effective when fitted to a specific hardware platform,
their accuracy depends heavily on device-specific parameters, making them
unsuitable as a general mechanism for heterogeneous server environments.  
Machine-learning-based estimators share the same limitation: high accuracy when
trained for a fixed configuration, poor portability without extensive retraining.

\subsubsection{Summary}

Software-exposed metrics provide high-resolution visibility into CPU, memory,
I/O, and network activity.  
They are indispensable for correlating workload behaviour with hardware power
signals, especially for components that lack native telemetry. 
Model-based estimation remains possible but inherently platform-specific, and
therefore unsuitable as a universal foundation for fine-grained attribution in
heterogeneous environments.

\section{Temporal Behaviour of Telemetry Sources}
\label{sec:temporal_behaviour}

A comprehensive treatment of temporal characteristics can be found in 
\appchapterref{A}{vt2_Chapter2}, but the present section focuses on the empirical, 
source-specific behaviours that constrain fine-grained power and energy estimation on 
real systems. Modern server platforms expose a heterogeneous set of telemetry 
interfaces, and their timing properties vary substantially: some update at fixed 
intervals, others employ internal averaging or smoothing, several expose counters 
without timestamps, and many lack guarantees on refresh regularity. These behaviours 
shape the effective temporal resolution with which workload-induced power changes can 
be observed.

The purpose of this section is not to develop a conceptual theory of sampling or to 
explain why timing matters for attribution (both are deferred to \chapref{ch:concepts}), nor to 
introduce Tycho’s timing engine (\chapref{ch:architecture}). Rather, it establishes the empirical 
constraints imposed by the telemetry sources themselves. These include sensor refresh 
intervals, stability of consecutive updates, delays between physical behaviour and 
reported values, the presence or absence of timestamps, and the distinction between 
instantaneous versus internally averaged measurements.

The subsections that follow describe these temporal properties for each telemetry 
source individually and summarise the practical limits they impose on high-resolution 
energy analysis.

\subsection{RAPL Update Intervals and Sampling Stability}
\label{subsec:rapl_timing}

RAPL exposes energy \emph{counters} rather than instantaneous power values.
These counters accumulate energy since boot and can be read at arbitrarily high
frequency, but their usefulness is determined entirely by how often the internal
measurement logic refreshes them, a timing behaviour that is undocumented and
domain-dependent.

\paragraph{Domain-specific internal update rates}
Intel specifies the RAPL time unit as 0.976\,ms for the slowest-updating
domains, while others, notably the PP0 (core) domain, may refresh
significantly faster \parencite{schone2024energy}.  
In practice, however, these theoretical limits do not translate into usable
temporal resolution because RAPL provides no timestamps: the moment of counter
refresh is unknown to the reader.  At sub-millisecond sampling rates, the lack
of timestamps combined with irregular refresh behaviour introduces substantial
relative error, since differences between consecutive reads may reflect counter
staleness rather than actual power dynamics \parencite{khan2018rapl}.

\paragraph{Noise introduced by security-driven filtering}
To mitigate power-side channels such as Platypus, Intel optionally introduces
randomised noise through the \code{ENERGY\_FILTERING\_ENABLE} mechanism
\parencite{intel2023}.  
This filtering increases the effective minimum granularity from roughly
1\,ms to approximately 8\,ms for the PP0 domain \parencite{schone2024energy}.  
While average energy over longer intervals remains accurate, instantaneous
increments become less reliable at very short timescales.

\paragraph{Practical sampling limits}
Despite the nominal sub-millisecond timing, empirical work consistently shows
that high-frequency polling offers no practical benefit.  
Multiple studies report that sampling faster than the internal update period
only produces repeated counter values and amplifies read noise
\parencite{khan2018rapl}.  
Jay et~al.\ demonstrate that at polling rates slower than 50\,Hz, the relative
error falls below 0.5\,\% \parencite{jay2023experimental}.  
Consequently, typical measurement practice (and the limits adopted in this
thesis) treats RAPL as reliable only at tens-of-milliseconds resolution, not at
the theoretical millisecond scale suggested by its nominal time unit.

\paragraph{Summary}
Although RAPL counters can be read extremely quickly, the effective temporal
resolution is constrained by undocumented refresh intervals, absence of
timestamps, optional security filtering, and substantial measurement noise at
high polling rates.  
For practical purposes, sampling at approximately 20--50\,ms intervals yields
the most stable and accurate results, while sub-millisecond polling is
inadvisable due to high relative error and counter staleness.

\subsection{GPU Update Intervals and Sampling Freshness}
\label{subsec:gpu_timing}

GPU power telemetry is exposed primarily through NVML, with DCGM providing an alternative access path that builds on the same underlying measurement source.
Unlike CPU-side interfaces integrated into the processor package, GPU power monitoring is performed entirely by the device itself: internal sensing circuits and firmware determine how often new values are produced, how they are averaged, and when they are published to software. As a result, refresh behaviour varies substantially across architectures, and the temporal properties of the reported values depend on device-internal update cycles rather than the rate at which the host system issues queries, which limits the achievable resolution of any external sampling strategy.

\paragraph{Internal update cycles and sampling freshness}
Empirical studies consistently show that NVML publishes new power values only intermittently, even when queried at high frequency.
Yang et~al.\ report sampling availability as low as roughly twenty–twenty-five percent across more than seventy modern data-center GPUs, meaning that the majority of polls return previously published values rather than fresh measurements \parencite{yang2024accurate}.

Typical internal update periods fall on the order of tens to several hundreds of milliseconds, with architectural variation between GPU generations.
Hernandez et~al.\ report that newer architectures apply more aggressive smoothing and exhibit longer gaps between updates, reflecting slower publication cadence at the firmware level \parencite{hernandezPreliminaryStudyFineGrained2025}.
Overall, empirical evaluations show that NVML’s internal update interval may lie on the order of hundreds of milliseconds and that repeated queries do not guarantee the retrieval of a new sample at every call \parencite{yang2024accurate}.
NVML power readings do not represent instantaneous electrical measurements; they reflect firmware-level integration and smoothing over a device-internal averaging window, the duration of which varies by GPU generation and is not publicly documented..

\paragraph{Reaction delay to workload-induced power changes}
A related characteristic is NVML’s reaction delay: when GPU power changes due to workload activity, the corresponding update becomes visible only after a lag.
Multiple studies document delays in the range of approximately one to three hundred milliseconds before a new NVML value reflects the underlying power transition \parencite{yang2024accurate}.
This delay is distinct from averaging effects and arises from deferred publication of internally accumulated measurements.
On some recent architectures, the delay can be longer due to device-level smoothing layers that defer updates until sufficient internal samples have been collected \parencite{hernandezPreliminaryStudyFineGrained2025}.

\paragraph{Update regularity and jitter}
NVML update cycles are not perfectly periodic.
Even when a nominal internal cadence is observable, individual publish times exhibit modest jitter, and occasional missed or skipped updates can result in sequences of identical values.
These effects are pronounced on certain consumer-class devices and in configurations that partition the GPU, such as MIG, although they are also present to a lesser degree on data-center accelerators \parencite{yang2024accurate}.
Such irregularity introduces uncertainty regarding the true measurement time of any retrieved value, especially in the sub-second range.

\paragraph{DCGM sampling behaviour}
DCGM relies on the same underlying measurement path as NVML and therefore inherits NVML’s internal update characteristics.
In practice, DCGM is commonly accessed through its exporter, which introduces an additional periodic sampling stage (typically around one second) resulting in markedly coarser temporal behaviour than NVML’s native cadence.
As a result, DCGM-based power telemetry rarely offers sub-second resolution in operational environments \parencite{weakleyMonitoringCharacterizingGPU2025}.

\paragraph{GPU utilization update cycles}
NVML’s GPU utilization metric follows its own internal update cadence, separate from power.
It is typically refreshed more frequently (on the order of tens of milliseconds) although the exact timing remains undocumented.
While this metric does not track computational efficiency, its shorter update interval provides a comparatively more responsive indicator of device activity \parencite{weakleyMonitoringCharacterizingGPU2025}.

\subsection{Redfish Sensor Refresh Intervals and Irregularity}
\label{subsec:redfish_timing}

Redfish exposes power telemetry through the baseboard management controller (BMC) and therefore inherits the temporal behaviour of its embedded sensing hardware and firmware. In contrast to on-chip interfaces such as RAPL or NVML, Redfish is designed for management-plane observability rather than high-frequency monitoring. Prior studies consistently report that Redfish refreshes whole-node power values at coarse intervals, typically ranging from several hundred milliseconds to multiple seconds, with the exact cadence depending on vendor, BMC firmware, and underlying sensor design \parencite{wang2019empirical, aliRedfishNagiosScalableOutofBand2022}. The Redfish standard does not define a minimum update frequency, and available documentation provides little insight into internal sampling or averaging strategies.

\paragraph{Measurement semantics of Redfish power values}
Redfish does not expose instantaneous electrical measurements. Instead, the reported values originate from on-board monitoring chips connected to shunt-based sensors and are subsequently processed inside the BMC. Vendor documentation indicates that these sensors inherently integrate power over tens to hundreds of milliseconds, and that additional firmware-level smoothing may be applied before values are published through the Redfish API \parencite{aliRedfishNagiosScalableOutofBand2022}. Empirical evaluations support this interpretation: Wang et~al.\ show that Redfish exhibits reaction delays of roughly two hundred milliseconds and displays particularly stable behaviour under steady loads, consistent with block-averaged rather than instantaneous sampling \parencite{wang2019empirical}. Because neither the sensor integration window nor any BMC filtering policies are defined in the standard, the temporal semantics of published values remain implementation-dependent.

Redfish power readings include a timestamp field, but this value reflects the BMC’s observation time rather than the sampling instant of the physical power sensor. In many implementations, timestamps are rounded to seconds, which limits their utility for reconstructing sub-second dynamics and prevents reliable inference of the underlying sampling moment.

Beyond published work, empirical observations from the system used in this thesis reveal that Redfish update intervals may exhibit substantial variability. While nominal refresh periods appear regular over longer windows, individual samples occasionally show multi-second gaps, repeated values, or irregular spacing. Such behaviour is consistent with a telemetry source operating on management-plane scheduling and BMC workload constraints rather than real-time guarantees. These observations do not generalise across vendors but illustrate the degree of temporal uncertainty that can occur in practice.

Overall, Redfish provides a widely supported mechanism for obtaining whole-system power readings and is well suited for coarse-grained monitoring or validation of other telemetry sources. Its coarse refresh intervals, lack of sensor-level timestamps, and implementation-dependent irregularities, however, make it unsuitable for analysing short-duration phenomena or for use as a primary source in high-resolution energy attribution.


\subsection{Timing of Software-Exposed Metrics}
\label{subsec:sw_timing}

% MUST INCLUDE:
% - /proc and cgroup files present cumulative counters.
% - Effective resolution = polling resolution.
% - Kernel updates these counters on scheduler ticks (not constant on all systems).
% - cgroup CPU counters updated at high granularity (nanoseconds, monotonic).
% - Disk/network counters updated as part of driver paths → not periodic.
%
% eBPF:
% - Event-driven rather than periodic.
% - Timestamped on event occurrence.
% - Offers effectively “instantaneous” temporal resolution.
% - Limited only by probe overhead, not by intrinsic update cycles.
%
% perf:
% - PMCs update continuously in hardware.
% - No fixed refresh cycle: counters are read at poll-time.
% - High precision, but reading them too frequently increases overhead.
%
% MUST NOT INCLUDE:
% - No event-time model explanations (Chapter 3).
% - No Tycho scheduling concepts (Chapter 4).

Key message:
- Software counters are cumulative and quasi-continuous.
- Their temporal behaviour is dominated by polling strategy, not internal refresh.
- eBPF is the only source that is inherently event-timestamped.


\subsection{Temporal Implications of Domain Boundary Ambiguity}
\label{subsec:domain_ambiguity_temporal}

% MUST INCLUDE (Temporal Aspects Only):
% - RAPL package/core/uncore/dram domains sometimes roll up different components
%   depending on CPU generation → impacts WHEN domain aggregates change.
% - Some domains update at different refresh intervals (e.g., uncore slower).
% - AMD RAPL equivalents have different update variability.
%
% GPU/MIG:
% - MIG partitions share underlying power measurement refresh cycles.
% - Per-instance values may reflect aggregation over multiple components.
% - Only mention timing behaviour: i.e., partition boundaries DO NOT change 
%   update cycles, but influence the temporal correlation of domain-level energy.
%
% MUST NOT INCLUDE:
% - No detailed domain explanations (Chapter 3).
% - No attribution theory.
% - No Kepler/Kubewatt discussion.

Key message:
- Domain ambiguity affects temporal alignment because different domains refresh 
  at different speeds or aggregate different internal components.


\subsection{Validation Methodologies in Prior Research}
\label{subsec:validation_methods}

% MUST INCLUDE:
% - Common validation techniques:
%     - External power meters (PDU, Yokogawa WT-series, NI DAQ).
%     - Cross-domain comparison: RAPL vs NVML vs Redfish vs wall power.
%     - Step-load experiments (CPU/GPU bursts).
%     - Steady-state plateaus.
%     - Controlled microbenchmarks (stress-ng, gpu-burn, custom kernels).
%
% - Known limitations:
%     - External meters often have low sampling rates (1–10 Hz).
%     - PDUs may average over long windows.
%     - Hard to validate sub-100 ms behaviour.
%     - Internal smoothing makes “ground truth” ambiguous.
%     - Multi-device setups introduce time drift between sensors.
%
% MUST NOT INCLUDE:
% - No mention of Tycho's validation method.
% - No description of how this thesis performs validation.
% - No theoretical treatment of model error.

Key message:
- Prior work validates temporal behaviour indirectly and imperfectly.
- Lack of high-frequency external reference devices limits precision.


%====================================================================
\section{Existing Tools and Related Work}
\label{sec:related_tools}
\appchapterref{A}{vt2_Chapter4}
% Expected length: 3–4 pages.
% DO NOT discuss Tycho or Tycho architecture.
% DO summarise empirical findings from prior work (Kepler, Kubewatt, others).

%--------------------------------------------------------------------
\subsection{General Tools (Brief Overview)} ->>>>In “Existing Tools,” potentially merge “General Tools” + short mentions of other tools into a single paragraph
\label{subsec:general_tools}

% Placeholder (0.3 page):
% - Briefly summarise Scaphandre, Smartwatts, and 1–2 other tools.
% - Emphasise their measurement scope and known limitations.
% - DO NOT provide conceptual attribution explanation (Chapter 3).
% - Keep this high-level and concise.

%--------------------------------------------------------------------
\subsection{Kepler}
\label{subsec:kepler}

% Placeholder (1.5–2.0 pages):
% - Describe Kepler’s architecture based purely on literature:
%   * metric sources
%   * ratio-based attribution strategy
%   * sampling behaviour
% - Summarise empirical misattribution issues identified by research and VT2:
%   * idle power artefacts
%   * latency mismatch
%   * short-lived workload instability
%   * system-process handling issues
% - Emphasise Kepler's shift toward deployability (literature statements).
% - DO NOT mention Tycho, Tycho improvements, or motivation for Tycho.

%--------------------------------------------------------------------
\subsection{Kubewatt}
\label{subsec:kubewatt}

% Placeholder (1.0–1.5 pages):
% - Summarise Kubewatt’s findings validating Kepler:
%   * incorrect idle attribution
%   * metadata/race-condition problems
%   * visibility issues with completed pods
% - Summarise Kubewatt’s improvements:
%   * static vs dynamic separation
%   * simplified proportional models
% - Summarise Kubewatt’s limitations as reported in thesis:
%   * missing domains
%   * limited metric scope
%   * no calibration
% - DO NOT reference Tycho or compare to Tycho.

%====================================================================
\section{Research Gaps}
\label{sec:research_gaps}

% This section synthesises gaps identified throughout Chapter 2.
% ABSOLUTELY NO Tycho discussion.
% NO conceptual discussion (that belongs to Chapter 3).

%--------------------------------------------------------------------
\subsection{Gap 1: Temporal Misalignment}
\label{subsec:gap_temporal}

% Placeholder (0.5 page):
% - Integrate research showing misaligned update intervals and delays.
% - DO NOT discuss strategies for alignment (Chapter 3 or 4).

%--------------------------------------------------------------------
\subsection{Gap 2: Missing Timestamps and Averaging}
\label{subsec:gap_timestamps}

% Placeholder (0.3 page):
% - Summarise lack of timestamps and internal averaging as documented by research.

%--------------------------------------------------------------------
\subsection{Gap 3: Domain Boundary Ambiguity}
\label{subsec:gap_boundaries}

% Placeholder (0.3–0.5 page):
% - Integrate findings on poorly documented boundaries (RAPL, MIG).

%--------------------------------------------------------------------
\subsection{Gap 4: Metadata-Lifecycle Inconsistencies}
\label{subsec:gap_metadata}

% Placeholder (0.3–0.5 page):
% - Synthesise research showing cgroup/pod lifecycle inconsistencies.

%--------------------------------------------------------------------
\subsection{Gap 5: Idle Power Attribution Issues}
\label{subsec:gap_idle}

% Placeholder (0.3 page):
% - Highlight known issues in idle power handling.

%--------------------------------------------------------------------
\subsection{Gap 6: Limited Multi-Domain Integration}
\label{subsec:gap_multidomain}

% Placeholder (0.3 page):
% - Summarise research noting lack of integrated CPU–GPU–system attribution.

%--------------------------------------------------------------------
\subsection{Gap 7: Missing Calibration and Uncertainty Treatment}
\label{subsec:gap_calibration}

% Placeholder (0.3–0.5 page):
% - Summarise research calling for calibration or uncertainty-aware methods.

%====================================================================
\section{Summary}
\label{sec:background_summary}

% Placeholder (0.5 page):
% - Summarise fundamental insights: heterogeneous telemetry behaviour, 
%   incomplete domain coverage, lifecycle inconsistency, lack of calibration.
% - Prepare the reader for Chapter 3, stating explicitly that Chapter 3
%   provides the conceptual foundations needed to understand the design 
%   of an accuracy-oriented system (without yet describing Tycho).

