% \chapter{Implementation}
% \label{chap:implementation}

% \section{Purpose, Scope, and Structure}
% This chapter describes how Tycho’s architectural concepts are realised at runtime by delineating clear boundaries of responsibility between its execution-time subsystems. Its purpose is to provide a compact mental model of the system’s overall structure so that later implementation sections can be read in context. The focus is on which components exist at runtime, what each is responsible for, and which concerns are explicitly excluded from their scope. Architectural concepts are not reintroduced, and temporal mechanisms, attribution logic, and correctness arguments are deferred to subsequent chapters. The chapter is intentionally concise and contract-like: it establishes who may decide what at runtime, and on which inputs, without prescribing internal algorithms or code structure.

% \section{System Overview and Execution Boundaries}
% \label{sec:impl_system_overview}

% \subsection{Runtime Actors and Responsibilities}

% At runtime, Tycho is structured as a small set of long-lived actors that cooperate through well-defined data and responsibility boundaries. Each actor fulfils a single primary role and operates independently within that role. No actor assumes semantic responsibility beyond its own contract, and no actor implicitly compensates for the behaviour of others. This separation is fundamental to the implementation and is enforced consistently across the system.

% Collectors are responsible for acquiring raw observations from their respective domains and making them available to the rest of the system. Their responsibility ends at faithful observation and timestamped emission of samples. Collectors have no knowledge of analysis windows, attribution logic, workload identity, or downstream consumption, and they do not coordinate with each other.

% The metadata subsystem provides identity and hierarchy information required for workload attribution. It maintains a continuously refreshed view of relationships between processes, cgroups, containers, and pods. Metadata is consulted during analysis but does not participate in metric collection, temporal reasoning, or attribution decisions itself.

% Calibration mechanisms operate as a dedicated startup actor. Their responsibility is to characterize relevant system and collector behaviour before normal operation begins and to derive parameters that inform later interpretation. Calibration does not perform attribution, does not modify collected data, and does not remain active during steady-state analysis.

% The analysis engine is the sole authority responsible for interpreting collected observations. It orchestrates per-window evaluation, consumes buffered metric samples and metadata, applies attribution models, and produces window-level results. No other component is permitted to fuse domains, apply models, or enforce invariants.

% Export and downstream consumption form a strictly downstream concern. The exporter observes the outputs of the analysis engine and exposes them to external systems. Export has no influence on collection, calibration, analysis cadence, or attribution decisions, and it does not participate in correctness enforcement.

% This actor-based structure provides a stable execution model that underpins the remainder of the implementation chapter. Subsequent sections describe each actor in isolation, building on these responsibility boundaries without reintroducing global system context.

% \subsubsection{Timing Engine}

% The timing engine is responsible for coordinating when runtime actions occur, without participating in their semantic interpretation. Its primary role is to provide a centralized scheduling reference that triggers collector execution and initiates analysis cycles according to a predefined execution plan.

% The timing engine does not collect metrics and does not perform attribution. It is concerned exclusively with triggering events, not with interpreting their results. In particular, it does not inspect collected samples, reason about their content, or adapt its behaviour based on downstream outcomes.

% It is important to distinguish the timing engine from the analysis engine. The timing engine determines \emph{when} collection and analysis are invoked, while the analysis engine determines \emph{how} collected observations are interpreted and attributed. No analytical logic resides in the timing engine, and no scheduling authority resides in the analysis engine.

% By separating execution timing from analytical coordination, Tycho ensures that collection schedules and analysis semantics remain decoupled. This separation allows timing behaviour to be managed uniformly across subsystems while keeping attribution logic explicit and isolated.

% \subsubsection{Collectors}

% Collectors are independent runtime actors responsible for acquiring raw observations from individual hardware and software domains. Each collector operates autonomously and is concerned exclusively with its local observation task. Its responsibility is limited to sampling its domain, attaching a timestamp in the global execution context, and emitting a complete, self-contained sample.

% Collectors do not perform any form of interpretation. They do not infer workload identity, apply attribution logic, align samples to analysis windows, or reason about temporal relationships beyond assigning timestamps. No collector is aware of other collectors, downstream analysis, or whether a given sample will contribute to a particular attribution window.

% All collectors emit their samples into a common buffering abstraction through a uniform interface. This interface defines the execution boundary between observation and interpretation. Once a sample has been emitted, the collector relinquishes responsibility for its use, ordering, or eventual inclusion in analysis. Collectors neither coordinate emission nor adapt their behaviour based on downstream state.

% This strict scoping ensures that collectors remain simple, domain-local, and replaceable in principle. It also guarantees that all semantic interpretation of measurements is centralized outside the collection layer and addressed explicitly in later stages of the system.

% \subsubsection{Metadata Subsystem}

% The metadata subsystem provides the identity and hierarchy information required to relate raw observations to workloads. Its primary responsibility is to maintain an up-to-date view of relationships between processes, cgroups, containers, and pods as they exist at runtime. This information forms the identity substrate on which attribution decisions are based.

% Metadata is maintained as a periodically refreshed snapshot that reflects the system state at analysis time. It is designed to be consulted during attribution rather than joined with metric streams. The subsystem does not participate in metric collection, does not attach semantics to observations, and does not influence how or when samples are produced.

% The metadata subsystem is strictly separated from analysis logic. It neither performs attribution nor enforces correctness properties. Its role is limited to supplying best-effort identity mappings that enable stable workload association across analysis windows. Missing or imperfect mappings are handled downstream and do not cause execution failures within the metadata subsystem itself.

% By confining identity management to a dedicated actor, Tycho ensures that workload attribution remains explicit, traceable, and decoupled from both raw observation and interpretative logic.

% \subsubsection{Calibration Mechanisms}

% Calibration mechanisms act as a supporting runtime actor whose sole responsibility is to characterize relevant properties of the execution environment before steady-state operation begins. Their function is to measure source-specific behaviour such as update characteristics and effective delays, and to derive parameters that contextualize later interpretation of collected observations.

% Calibration executes during startup and completes prior to the first analysis window. The parameters it produces are treated as static inputs for the remainder of execution. Calibration does not participate in metric collection, does not modify collected samples, and does not remain active during steady-state analysis.

% The calibration subsystem does not perform attribution and does not enforce correctness properties. It informs the timing and analysis components by supplying measured context, but it does not make analytical decisions itself. By isolating calibration as a dedicated actor, Tycho separates environment-dependent characterization from attribution logic and avoids entangling measurement with interpretation.

% \subsubsection{Analysis Engine}

% The analysis engine is the central orchestrator of Tycho’s per-window evaluation. It is the only runtime actor responsible for interpreting collected observations and producing attribution results. All semantic reasoning, model application, and invariant enforcement are confined to this component.

% At each analysis cycle, the analysis engine consumes materialized inputs in the form of buffered metric samples, metadata snapshots, and calibration-derived parameters. It determines the ordering of analysis stages, performs cross-domain fusion, applies attribution models, and derives window-level results. No other subsystem is permitted to combine metric domains or assign energy to workloads.

% The analysis engine operates strictly as a consumer of upstream data. It does not influence collection schedules, metadata acquisition, or calibration behaviour, and it does not mutate buffered inputs. Its output consists of a single logical result per analysis window, potentially composed of multiple metrics, reflecting the attribution state that can be derived from the available inputs.

% By centralizing all interpretative logic within the analysis engine, Tycho ensures that attribution semantics remain explicit, auditable, and isolated from observation and scheduling concerns.

% \subsubsection{Export and Downstream Consumption}

% Export is a strictly downstream concern that exposes the results produced by the analysis engine to external systems. Its responsibility is limited to observing attribution outputs and making them available in a form suitable for downstream consumption.

% The export subsystem does not influence collector behaviour, timing decisions, or analysis execution. It does not participate in attribution, does not apply additional interpretation, and does not feed information back into the system. Export operates in a non-blocking manner and cannot delay or backpressure analysis.

% Exported data reflects analysis results rather than raw observations. While export mechanisms may maintain cumulative representations over time, they do not aggregate across analysis windows at the semantic level. Exported metrics are non-authoritative and serve solely as an interface between Tycho’s internal attribution logic and external monitoring or storage systems.

% By isolating export as a downstream observer, Tycho preserves a clear boundary between attribution semantics and external consumption, ensuring that analytical correctness is independent of export behaviour.

% \subsection{Execution-Time Boundaries of Responsibility}

% \subsubsection{What the Timing Engine Guarantees}

% The timing engine provides execution-time guarantees related exclusively to scheduling and triggering. It guarantees that collection and analysis triggers are issued according to the configured execution plan and that these triggers are generated consistently within the global execution context.

% The timing engine does not guarantee that triggered actions result in observable samples, nor does it guarantee alignment or completeness of collected data. It does not inspect, validate, or interpret the outcomes of triggered actions, and it does not adapt its behaviour based on downstream state or results.

% By confining its guarantees to triggering semantics only, the timing engine establishes a stable temporal coordination layer while remaining strictly separated from observation, interpretation, and attribution responsibilities.


% \subsubsection{What Collectors Guarantee}

% Collectors provide a narrow set of hard execution-time guarantees that define the boundary between observation and interpretation. Each emitted sample represents a faithful observation of the underlying source at the moment of collection. Samples are complete, self-contained, and not duplicated. Timestamps attached by collectors are monotonic and consistent within the global execution context.

% Beyond these properties, collectors make no semantic guarantees. They do not guarantee temporal alignment with other collectors, completeness of coverage within an analysis window, or any ordering relationship between samples beyond their timestamps. Collectors do not attach workload identity, do not infer higher-level meaning, and do not participate in attribution or correctness enforcement.

% In particular, collectors do not guarantee that a sample will be usable for a given analysis window, nor that all relevant activity is observed. Responsibility for handling missing data, misalignment, and partial visibility lies strictly downstream. This contract ensures that collectors remain simple and domain-local, while all interpretative responsibility is centralized elsewhere in the system.


% \subsubsection{What the Analysis Engine Guarantees}

% The analysis engine provides the system’s core semantic guarantees. It is solely responsible for fusing observations across domains, applying attribution models, and enforcing architectural invariants. In particular, conservation is treated as an absolute invariant within each analysis window: all attributed quantities are internally consistent with the observed inputs and their modeled decomposition.

% Attribution is performed on a best-effort basis under the constraints imposed by the available data. The analysis engine guarantees self-consistency of its outputs, not correspondence to ground truth. When observations are incomplete or optional domains are unavailable, the engine produces the most complete attribution that can be derived without violating invariants. Residual handling and domain-specific fallbacks are part of this responsibility.

% The analysis engine consumes materialized inputs and operates strictly read-only on upstream data. It does not correct or reinterpret observations, and it does not retroactively revise prior results. Each analysis window yields a single logical attribution result, potentially composed of multiple metrics, reflecting the maximal consistent interpretation achievable for that window.

% \subsubsection{Non-Responsibilities and Separation Constraints}

% Several responsibilities are intentionally excluded from all runtime actors to preserve explicit boundaries and avoid hidden coupling. In particular, analytical components do not control collection schedules, do not influence when observations are taken, and do not adapt upstream behaviour based on analytical outcomes. Scheduling authority remains confined to the timing engine, and observation authority remains confined to collectors.

% Conversely, collection and calibration components do not perform interpretation. They do not infer meaning from observations, apply models, or participate in attribution decisions. No component retroactively alters collected data or revises previously emitted results. Once an observation has been emitted, it is immutable from the perspective of all downstream stages.

% Tycho is strictly observational. It does not attempt to intervene in system behaviour, optimise workloads, or provide feedback for control. This separation ensures that each subsystem operates within a narrow, well-defined contract and that system behaviour remains transparent, composable, and auditable at runtime.

% \subsection{End-to-End Dataflow at Runtime}

% During steady-state operation, Tycho executes as a continuous loop of observation, interpretation, and exposure. Collectors acquire raw observations independently and emit samples into a shared buffering layer without coordination or awareness of downstream use. In parallel, the metadata subsystem maintains a periodically refreshed snapshot of workload identity and hierarchy.

% At each analysis cycle, the analysis engine retrieves materialized inputs from the buffering layer and consults the current metadata snapshot and calibration parameters. It interprets the available observations, performs cross-domain fusion and attribution, and produces a single logical result for the window. Analysis proceeds based on the data that is available at evaluation time, without assuming completeness or uniform coverage across domains.

% Once attribution completes, the resulting outputs are observed by the export subsystem and exposed to external consumers. Export operates strictly downstream and does not influence upstream execution. This dataflow remains decoupled at all stages: observation, interpretation, and exposure interact only through explicit data handoff, ensuring that asynchronous operation and independent subsystem evolution do not compromise attribution semantics.



\chapter{Implementation}
\label{chap:implementation}

\section{Purpose, Scope, and Execution-Time Structure}
\label{sec:impl_overview}

This chapter explains how Tycho’s architectural abstractions are realised at runtime under the constraints of discretization, partial observability, and asynchronous execution. Its role is to describe how responsibility boundaries defined in the architecture are enforced concretely, and how correct attribution is achieved despite imperfect and delayed inputs. Architectural concepts, models, and invariants are assumed from earlier chapters and are not reintroduced here.

At execution time, Tycho is structured as a set of long-lived subsystems with strictly separated responsibilities and unidirectional interaction. Each subsystem exercises authority over a narrow concern, and no subsystem compensates implicitly for the behaviour of others. This execution-time separation forms the foundation for correctness, auditability, and robustness throughout the implementation.

% Additional implementation detail that is not required for understanding the
% runtime structure or correctness arguments is intentionally deferred to
% \hyperref[sec:tycho_sysenv]{Appendix~C}.
% The appendix collects auxiliary material such as extended configuration
% descriptions, supporting scripts, and low-level operational notes that aid
% reproducibility and inspection without obscuring the main implementation
% narrative.

\subsection{Runtime Subsystems and Responsibilities}

Tycho’s runtime consists of the following subsystems, each of which is examined in detail later in this chapter:

\begin{itemize}
\item The \textbf{timing engine} provides execution-time coordination by triggering collection and analysis actions according to a global schedule, without participating in interpretation or attribution (\S~\ref{sec:impl_temporal}).

\item \textbf{Metric collectors} act as independent observers that acquire raw measurements from individual hardware and software domains and emit timestamped samples without coordination or semantic interpretation (\S~\ref{sec:impl_collectors}).

\item The \textbf{metadata subsystem} maintains a refreshed view of workload identity and hierarchy, supplying identity context during attribution without joining metric streams or performing analysis (\S~\ref{sec:impl_metadata}).

\item \textbf{Calibration mechanisms} derive auxiliary parameters that characterise source behaviour and contextualise interpretation, executing outside steady-state attribution and without modifying observations (\S~\ref{sec:impl_calibration}).

\item The \textbf{analysis engine} is the sole authority responsible for interpreting observations, fusing domains, applying attribution models, and enforcing architectural invariants on a per-window basis (\S~\ref{sec:impl_analysis_pipeline}).

\item \textbf{Export} observes the results of analysis and exposes them to external systems without influencing upstream execution or attribution semantics (\S~\ref{sec:impl_analysis_pipeline}).
\end{itemize}

\subsection{Execution-Time Interaction Model}

Interaction between these subsystems follows a strictly unidirectional pattern. Temporal authority originates in the timing engine, observation authority in collectors and metadata acquisition, and semantic authority exclusively in the analysis engine. Data flows forward through explicit handoff only: raw observations and identity context are materialised upstream and consumed read-only during analysis, while attribution results flow downstream to export.

This interaction model deliberately excludes feedback paths, implicit coordination, and retroactive modification of observations. Once emitted, samples are immutable; once a window is analysed, its results are final. These constraints ensure that attribution semantics remain explicit, reproducible, and independent of scheduling or export behaviour.

The remainder of this chapter elaborates on how each subsystem realises its assigned responsibility in practice, addressing temporal realisation, collection mechanics, identity handling, calibration, attribution, and robustness in turn (\S~\ref{sec:impl_temporal}–\S~\ref{sec:impl_tradeoffs}).

\section{Temporal Infrastructure and Window Realization}
\label{sec:impl_temporal}

\subsection{Architectural Context and Implementation Problem}

\S~\ref{sec:timing_engine} defines Tycho’s temporal model in abstract terms: a single monotonic time base, independently operating collectors, and fixed-duration analysis windows triggered by a timing engine. The implementation task is to realize this model under real execution constraints while preserving its guarantees, rather than restating its semantics.

Collectors are scheduled by a general-purpose operating system and are subject to jitter, preemption and variable execution latency. Polling callbacks may execute late, at uneven intervals, or out of phase with one another. Analysis must therefore not depend on execution order, callback timing, or implicit synchronization effects. Temporal correctness must derive exclusively from explicit timestamps attached to observations, not from when code happens to execute. The temporal infrastructure enforces this separation rigorously.

\subsection{Global Monotonic Time Realization}

The architectural event-time model relies on a single system-wide monotonic time base. Its implementation elevates monotonic time to a first-class dependency rather than treating it as an incidental property of the runtime environment. All collectors, the timing engine and the analysis engine obtain temporal information exclusively through a dedicated clock abstraction.

Wall-clock time is excluded from analysis-critical paths and appears only where external representation is unavoidable. The clock abstraction provides monotonic timestamps for observations and analysis boundaries, mediates conversion between real-time and monotonic representations where required, and is injected into downstream components to ensure consistent and testable temporal behavior across subsystems.

As a result, scheduling jitter becomes an explicit input to analysis rather than a hidden source of error. A collector that executes late produces a correspondingly late timestamp. No corrective action is taken at collection time; timestamps are interpreted during analysis according to the delay and freshness assumptions defined in \S~\ref{sec:timing_engine}. Monotonic timestamps thus constitute the sole temporal authority within the system.

\subsection{Timing Engine and Hierarchical Cadence Alignment}

Independent collector schedules are a central architectural principle. Realizing this independence without sacrificing determinism requires a controlled mechanism for initiating periodic actions. Tycho employs a centralized timing engine to which all periodic activities register during system initialization.

Each registration specifies a period expressed as an integer multiple of a global base quantum (default: 1\,ms). All registrations are aligned to a shared epoch defined by this quantum, establishing deterministic phasing across the system. Collector and analysis triggers are hierarchically derived from this common cadence rather than started opportunistically, ensuring that identical configurations produce identical temporal behavior across runs. Alignment does not impose a shared frequency: collectors with different periods remain independent, but their triggers occur at deterministic offsets on the global monotonic axis.

The timing engine is deliberately non-semantic. It does not inspect collected data, adapt schedules, or coordinate collectors. Its sole responsibility is to emit triggers at predetermined monotonic times. Work performed in response to a trigger is constrained to be minimal and non-blocking, typically limited to recording an observation and placing it into a buffer, preventing local execution delays from propagating into global timing behavior.

Analysis triggering is implemented using the same registration mechanism. The analysis engine registers a periodic trigger alongside collectors, making analysis execution subject to the same alignment and determinism guarantees. This design enforces single-cycle exclusivity by construction: a new analysis cycle cannot begin before the previous trigger boundary has been established, without requiring additional synchronization logic.

\subsection{Analysis Window Realization and Trigger Semantics}

Analysis windows are realized when an analysis trigger fires. At that instant, the timing engine provides a single monotonic timestamp \(t_{\text{now}}\), which defines the upper boundary of the current window. This timestamp is captured exactly once and propagated unchanged throughout the entire analysis cycle. All metrics are evaluated against windows derived from this shared boundary, and no component recomputes or refines the window definition during analysis. Consequently, all attribution decisions within a cycle refer to an identical temporal interval, independent of execution order or internal processing latency.

Window boundaries are defined by trigger times rather than sample arrival. Samples collected before \(t_{\text{now}}\) may be included or excluded according to domain-specific delay and freshness rules as defined in \S~\ref{sec:timing_engine}. Samples arriving after the trigger are attributed to subsequent windows. This separation prevents double-counting and omission even under heterogeneous collector rates.

Temporal complexity is intentionally confined to timestamping and delay interpretation. Window construction itself remains simple and predictable, providing a stable temporal substrate on which later analysis stages can reason about delay, partial observation and attribution correctness without embedding scheduling assumptions or compensating for execution artifacts.

\section{Historical Observation Retention}
\label{sec:impl_history}

Tycho retains a bounded history of raw observations in order to support downstream analysis that requires temporal context beyond a single attribution window. This retention is an explicit implementation responsibility derived from the temporal model in \S~\ref{sec:timing_engine}. Rather than operating exclusively on window-local samples, Tycho preserves historical signal to mitigate discretization effects, tolerate heterogeneous collector cadences, and enable mathematically stable downstream interpretation.

\subsubsection{Metric Observation Retention}
Historical retention is realized through per-collector observation buffers with time-based semantics. Each collector appends observations to its own buffer, which retains a fixed-duration history with a default horizon of approximately 90\,s. This horizon deliberately exceeds the nominal analysis window length and is chosen to provide substantial temporal context for downstream analysis. Buffer capacity is computed at startup from the collector’s polling interval and the configured analysis window, ensuring that retained history covers at least twice the longer of these durations, augmented by a small safety margin. Under Tycho’s default configuration for high-frequency analysis, this corresponds to retention spanning approximately 18 full analysis windows, providing substantial historical context for downstream analysis models. Retention is bounded and fixed for the lifetime of the process.

Buffered samples are append-only and immutable once written. Downstream components access buffered data strictly in a read-only manner. No guarantee is made that all collectors contribute samples to every window or that samples are temporally aligned across collectors. Partial observability and heterogeneous update patterns are therefore preserved explicitly and interpreted by the analysis engine rather than hidden by synchronization.

\subsubsection{Metadata Retention}
In addition to metric observations, Tycho maintains a bounded cache of metadata describing process, cgroup and workload identities. This cache employs a time-based retention policy aligned with the metric retention horizon to ensure that buffered observations can be joined with valid identity information during analysis. Metadata history is not used for long-term modeling and is removed once it exceeds the retention window.

\section{Metric Collection Subsystems}
\label{sec:impl_collectors}

\subsection{eBPF Collector Implementation}
\label{subsec:impl_ebpf}
This section describes how Tycho realises the event-driven CPU ownership and activity model introduced in \S~\ref{subsec:ebpf_temporal}.  
The eBPF collector implements a kernel-level acquisition path that captures execution boundaries precisely and exposes the resulting activity as bounded, per-window deltas for downstream analysis.

\subsubsection{Implementation Strategy}
The implementation follows a split-surface strategy that separates \emph{attributable} activity from \emph{non-attributable} CPU time.  
Attributable activity is accumulated per process at scheduler boundaries, together with stable identity and classification metadata.  
Non-attributable activity, including idle time and interrupt handling, is accumulated per CPU and exported independently.  
This separation reflects Tycho’s attribution requirements: process-level ownership must be preserved without ambiguity, while certain CPU time categories cannot be meaningfully assigned to workloads.  
All kernel programs operate strictly locally, without cross-CPU or cross-process aggregation; consolidation and interpretation are deferred to userspace and later analysis stages.

\subsubsection{Core Mechanisms}
Process-level accounting is driven by scheduler transitions.  
At each context switch, the outgoing execution interval is closed and its duration is accumulated into the corresponding process aggregate, together with hardware performance counters sampled at the same boundary.  
Process identity, control-group association, and kernel-thread classification are captured at these execution boundaries and stored alongside the counters.  
By aligning accumulation with actual ownership changes, the implementation preserves the architectural guarantee that execution intervals form precise attribution boundaries.

CPU-local accounting handles activity that is not attributable to individual processes.  
Each CPU maintains a local state machine that tracks the currently active context and the timestamp of the last transition.  
Idle time is detected explicitly via the scheduler’s idle task and accumulated when the CPU executes in this state.  
Hard interrupt and soft interrupt handling are measured as outermost intervals using entry and exit hooks, with durations accumulated into per-CPU bins.  
This design avoids double counting under nested interrupts while preserving a complete partition of CPU time at the node level.

Userspace collection materialises kernel-side accumulation into analysis-ready deltas.  
At a fixed polling cadence, the collector snapshots all process aggregates and resets only their counter fields while preserving identity metadata.  
This stable-key snapshot design avoids missing-entry artefacts that can occur if keys are deleted while scheduler updates are in flight.  
CPU-level bins are read and reset in the same cycle, defining a clear collection boundary for idle and interrupt time.  
The result of each collection cycle is a single tick record that represents all observed activity since the previous boundary, without overlap or double counting.

\subsubsection{Robustness and Edge Cases}
Several implementation choices ensure robustness under concurrent kernel activity.  
Process aggregates persist across collection cycles, and only delta-relevant fields are reset, preventing transient key loss under concurrent scheduler updates.  
All kernel-side state is bounded through per-CPU arrays, bounded per-process maps, and fixed-size histograms for interrupt vector enrichment.  
Reset failures or map evictions are treated as non-fatal; correctness is restored automatically in subsequent cycles.  
A fundamental observability limit remains: processes that execute entirely between two collection boundaries may not be observed.  
This limitation is inherent to discrete materialisation and is not compensated by inference.

\subsubsection{Implementation Consequences}
In practice, the eBPF collector produces a fixed-resolution utilisation and activity surface that preserves execution-boundary accuracy and stable process identity.  
Per-window deltas are exported without imposing additional timing constraints on the analysis engine, enabling proportional attribution and energy modelling in later stages.

\subsubsection{Collected Metrics}
The process-level and CPU-level metrics exported by the eBPF collector are listed in table Table~\ref{tab:ebpf-collector-metrics}.

\begin{table}[h]
    \centering
    \begin{tabular}{p{3cm} p{3.4cm} p{6.2cm}}
    \toprule
    \textbf{Metric} & \textbf{Source hook} & \textbf{Description} \\
    \midrule
    \multicolumn{3}{l}{\textit{Time-based metrics}} \\[4pt]
    Process runtime & \code{tp\_btf/sched\_switch} & Per process. Elapsed on-CPU time accumulated at context switches. \\
    Idle time & Derived from \code{sched\_switch} & Per node. Aggregated idle time across CPUs. \\
    IRQ time & \code{irq\_handler\_{\{entry,exit\}}} & Per node. Aggregated duration spent in hardware interrupt handlers. \\
    SoftIRQ time & \code{softirq\_{\{entry,exit\}}} & Per node. Aggregated duration spent in deferred kernel work. \\[4pt]
    
    \multicolumn{3}{l}{\textit{Hardware-based metrics}} \\[4pt]
    CPU cycles & PMU (\code{perf\_event\_array}) & Per process. Retired CPU cycle count during task execution. \\
    Instructions & PMU (\code{perf\_event\_array}) & Per process. Retired instruction count. \\
    Cache misses & PMU (\code{perf\_event\_array}) & Per process. Last-level cache misses; indicator of memory intensity. \\[4pt]
    
    \multicolumn{3}{l}{\textit{Classification and enrichment metrics}} \\[4pt]
    Cgroup ID & \code{sched\_switch} & Per process. Control group identifier for container attribution. \\
    Kernel thread flag & \code{sched\_switch} & Per process. Marks kernel threads executing in system context. \\
    Page cache hits & \code{mark\_page\_accessed} & Per process. Read or write access to cached pages; proxy for I/O activity. \\
    IRQ vectors & \code{softirq\_entry} & Per process. Frequency of specific soft interrupt vectors. \\
    \bottomrule
    \end{tabular}
    \caption{Metrics collected by the kernel \code{eBPF} subsystem.}
    \label{tab:ebpf-collector-metrics}
\end{table}

\subsection{RAPL Collector Implementation}
\label{sec:rapl_collector}

This section realizes the architectural model of cumulative CPU-domain energy sampling described in \S~\ref{subsec:rapl_temporal}.  
The collector’s responsibility is strictly limited to observing hardware-provided cumulative energy counters at tick boundaries and preserving their semantics.  

\subsubsection{Implementation Strategy}
To preserve domain-consistent CPU energy measurement across vendors, the collector employs a dual-backend strategy selected at runtime via CPUID.  
On Intel systems, energy is obtained through the RAPL interface exposed via the \texttt{powercap} subsystem.  
On AMD systems, the collector preferentially uses \texttt{amd\_energy} via the \texttt{hwmon} interface, which provides accurate CPU energy telemetry on these platforms.  
The powercap path is used on AMD only if no CPU-labeled hwmon energy source is present.  
This strategy ensures that the architectural RAPL domain abstraction is realized consistently, independent of vendor-specific exposure mechanisms.

\subsubsection{Core Mechanisms}
At each tick, the collector obtains a single snapshot of cumulative energy counters for all available CPU domains and sockets and records them unchanged.  
All values are stored as cumulative microjoule counters, matching the native units of the underlying sources.  
Supported domains include package and core on all platforms, with uncore (PP1) and DRAM recorded when exposed by the hardware.  
On AMD systems, \texttt{amd\_energy} publishes per-logical-core energy values; these are aggregated by summation into a single cumulative core-domain counter to preserve domain semantics.  
Domains not provided by the platform are recorded as zero to maintain a stable domain set across ticks.

\subsubsection{Robustness and Edge Cases}
Powercap-backed RAPL counters may wrap and are corrected at collection time to maintain monotonicity.  
The hwmon-backed \texttt{amd\_energy} counters do not wrap and require no correction.
Each sample is tagged exclusively with a monotonic timestamp provided by Tycho’s timing engine.  
Partial reads are permitted and result in partial ticks; no backend instability was observed in practice.

\subsubsection{Implementation Consequences}
In practice, the collector guarantees exactly one cumulative energy sample per supported domain and socket at each tick, with vendor-independent domain semantics and bounded noise.  
The resulting time series form a stable input for downstream differencing and attribution.  
The only remaining limitation is platform-dependent domain availability, which is intentionally exposed rather than approximated.

\subsubsection{Collected Metrics}
The RAPL collector exports raw cumulative energy counters once per tick.  
Table~\ref{tab:rapl-metrics} summarizes the metrics recorded per \texttt{RaplTick}.
\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.9cm} p{8.6cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Per-socket energy counters}} \\[4pt]
\code{Pkg}    & mJ & Cumulative package energy per socket\newline(RAPL \code{PKG} domain). \\
\code{Core}   & mJ & Cumulative core energy per socket\newline (RAPL \code{PP0} domain), when available. \\
\code{Uncore} & mJ & Cumulative uncore energy per socket\newline (RAPL \code{PP1} or uncore domain), when available. \\
\code{DRAM}   & mJ & Cumulative DRAM energy per socket\newline (RAPL \code{DRAM} domain), if the platform exposes it. \\[6pt]

\multicolumn{3}{l}{\textit{Metadata}} \\[4pt]
\code{Source}    & -- & Identifier of the active RAPL backend (for example \code{powercap})\\
\code{Sockets}   & -- & Map from socket identifier to the corresponding set of domain counters. \\
\code{SampleMeta.Mono} & -- & Monotonic timestamp assigned by Tycho's timing engine at the moment of collection. \\
\bottomrule
\end{tabular}
\caption{Metrics exported by the RAPL collector per \code{RaplTick}.}
\label{tab:rapl-metrics}
\end{table}

\subsection{Redfish Collector Implementation}
\label{subsec:impl_redfish}

This section describes how Tycho realizes the Redfish power source defined in \S~\ref{subsec:redfish_temporal}.  
Redfish is treated as an externally clocked, latently published observation whose update cadence and timing semantics are controlled by the Baseboard Management Controller.  
The implementation must therefore tolerate missing observations, expose temporal uncertainty explicitly, and avoid manufacturing samples while still enabling a coherent downstream power timeline.

\subsubsection{Implementation Strategy}
The Redfish collector issues at most one query per engine tick and emits a record only when a power value can be obtained reliably or when an explicit continuity fallback is required.  
The collector is permitted to emit no record for a tick.  
This choice reflects the architectural intent to avoid speculative continuity and to preserve the distinction between absence of information and persistence of state.  
Temporal alignment to Tycho’s monotonic timebase is achieved by timestamping at collection time, without attempting synchronization or correction of BMC time.

\subsubsection{Freshness Realization and Semantics}
Freshness is realized as a best-effort quality annotation computed as the difference between the local monotonic collection time and the timestamp provided by the BMC, when available.  
Given the limited and vendor-specific semantics of BMC timestamps, the collector applies no correction, filtering, or normalization.  
Freshness therefore represents observed latency and staleness rather than a validity constraint.

When continuation records are emitted, the collector reuses the most recent BMC timestamp and recomputes freshness accordingly.  
As a consequence, freshness increases during prolonged publication gaps, making temporal uncertainty explicit.  
The collector never suppresses, alters, or reclassifies samples based on freshness.  
All values are forwarded unchanged as downstream quality indicators.

\subsubsection{Heartbeat-Based Continuity as Fallback}
Irregular Redfish publication implies that fixed-cadence sampling may observe extended periods without new measurements.  
The collector therefore supports an optional heartbeat mechanism whose role is explicitly fallback-oriented.  
Heartbeat does not operate on every missed tick.  
Instead, it re-emits a specially marked continuation record only when no fresh observation has been obtained for a comparatively long interval relative to the engine cadence.

By default, this interval substantially exceeds the collection period, ensuring that short-lived access failures or transient gaps result in silence rather than artificial continuity.  
If enabled, the heartbeat threshold may be configured statically or derived adaptively from observed inter-arrival times of fresh Redfish updates, with conservative bounds to avoid pathological behavior under highly irregular BMC implementations.  
Heartbeat emission never invents new measurements.  
It explicitly signals persistence of the last known value when prolonged absence would otherwise break temporal continuity.

\subsubsection{Robustness Under Partial Observation}
Redfish access failures and missing timestamps are treated as normal operating conditions.  
If a Redfish query fails, the collector emits no record for that tick.  
No retries, backoff strategies, or suppression mechanisms influence the semantic output.  
Only when the heartbeat threshold is exceeded does the collector emit a continuation record, clearly distinguishing prolonged absence from transient failure.

Multiple chassis are handled independently.  
Freshness computation, heartbeat state, and continuation decisions are maintained per chassis and never synchronized across nodes.  
This preserves architectural assumptions about the independence of Redfish power sources in multi-node deployments.

\subsubsection{Implementation Consequences}
In practice, the collector emits at most one record per chassis per engine tick, with zero records as a valid and expected outcome.  
Continuity is preserved only when absence becomes prolonged, and even then without obscuring staleness.  
The resulting stream provides a stable, monotonic reference for total node power while exposing uncertainty rather than masking it.

This implementation anchors Tycho’s global energy view and supports later reconciliation with in-band estimates without conflating observation authority or temporal semantics.  
Its limitations are deliberate.  
Temporal resolution and accuracy are bounded by the BMC, and no component-level attribution is attempted at this stage.

\subsubsection{Collected Metrics}
The Redfish collector emits instantaneous chassis power together with identity and temporal metadata.  
Only raw observations are produced.  
Derived quantities such as energy are computed by downstream analysis stages.  
The exported fields are summarized in Table~\ref{tab:redfish-metrics}.
\begin{table}[h]
\centering
\begin{tabular}{p{3cm} p{1cm} p{9cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Primary power metric}} \\[4pt]
\code{PowerWatts} & W & Instantaneous chassis power reported by the BMC. \\[4pt]

\multicolumn{3}{l}{\textit{Temporal and identity metadata}} \\[4pt]
\code{ChassisID} & - & Identifier of the chassis or enclosure. \\
\code{Seq} & - & Server-provided sequence number indicating new measurements. \\
\code{SourceTime} & s & Timestamp provided by the BMC, if available. \\
\code{CollectorTime} & s & Local collection time of the measurement. \\
\code{FreshnessMs} & ms & Difference between \code{SourceTime} and \code{CollectorTime}. \\
\bottomrule
\end{tabular}
\caption{Metrics collected by the Redfish collector.}
\label{tab:redfish-metrics}
\end{table}

\subsection{GPU Collector Implementation}
\label{subsec:gpu_implementation}

The GPU collector realises the architecture described in
\S~\ref{subsec:gpu_architecture} and integrates accelerator telemetry into
Tycho’s unified temporal framework.
In contrast to other energy domains, GPU telemetry is published at discrete,
driver-controlled moments that are neither continuous nor externally observable.
The implementation is therefore responsible for enforcing phase-aligned,
event-driven sampling under partial observability, backend variability, and
timing jitter, while preserving strict monotonic ordering across all domains.

The central implementation invariant is that at most one \code{GpuTick} is
emitted per confirmed hardware publish, and that no tick is emitted without a
detectable device update.
All mechanisms described in this section exist to uphold this invariant in
practice, including the integration of retrospective process-level telemetry
under wall-clock semantics.

\subsubsection{Implementation Strategy}

The implementation treats GPU sampling as an inference problem rather than a
periodic measurement task.
Because the driver’s publish cadence is implicit, polling is used only as a
means to detect new hardware updates, not as a proxy for time.
Sampling effort is modulated according to the phase-aware timing model defined
in \S~\ref{subsec:gpu_phaseaware_formal}, concentrating observation near predicted
publish moments while suppressing redundant reads elsewhere.

Freshness detection and event emission are deliberately decoupled.
Polling may occur at high frequency, but a \code{GpuTick} is emitted only when a
previously unseen device update is detected and can be placed monotonically into
Tycho’s multi-domain buffer.
This separation ensures that increased polling density improves detection
latency without inflating the event stream or distorting temporal structure.

Device-level and process-level telemetry are integrated asymmetrically.
Device snapshots define the temporal anchor of each \code{GpuTick}, while
process-level records are attached retrospectively to confirmed device updates
to accommodate backend-imposed wall-clock windows.
This strategy preserves the architectural timing guarantees while enabling
multi-tenant attribution under heterogeneous backend constraints.

\subsubsection{Phase-Aware Sampling Realisation}

The phase-aware timing model defined in \S~\ref{subsec:gpu_phaseaware_formal} is
realised through a conservative observation pipeline that separates sampling
attempts from update confirmation.
Polling is driven by predicted publish moments, but observations are accepted
only when they provide evidence of a previously unseen hardware update.
This prevents both aliasing and redundant emission under irregular driver
cadence.

Freshness detection prioritises the strongest available backend signal.
When reliable cumulative energy counters are present, monotonic advancement of
these counters serves as the authoritative indicator of a new publish.
On devices lacking such counters, freshness is inferred from instantaneous power
changes exceeding a noise-tolerant threshold.
In both cases, snapshots that do not satisfy freshness criteria are discarded
without affecting estimator state or downstream timelines.

Duplicate suppression is enforced by conditioning all state updates on confirmed
freshness.
Period and phase estimators are advanced only when a new publish is detected,
ensuring that redundant polls neither bias cadence inference nor generate
spurious alignment corrections.
This guarantees that increased polling density reduces detection latency without
inflating the logical event stream.

\subsubsection{Event Construction and Emission}

When a fresh device update is confirmed, the collector constructs a
\code{GpuTick} that represents the accelerator state at a single monotonic
timestamp.
The device snapshot defines the temporal anchor of the event.
If process-level telemetry is available, the corresponding utilisation records,
aggregated over a backend-defined wall-clock window, are attached
retrospectively to the same tick.

Tick emission is strictly conditional on update confirmation.
No \code{GpuTick} is produced for redundant or ambiguous observations, and no
tick is emitted retroactively.
Each emitted tick is inserted into Tycho’s multi-domain buffer in monotonic
order, preserving causal alignment with RAPL, Redfish, and eBPF data without
interpolation or reordering.

This construction enforces a one-to-one correspondence between hardware publishes
and GPU events.
As a result, the GPU timeline reflects device behaviour rather than sampling
artefacts and provides a temporally consistent input to subsequent attribution
stages.

\subsubsection{Process Telemetry Integration}

Process-level GPU telemetry is exposed by the backend only as utilisation
aggregated over an explicit wall-clock interval.
This constraint is external to Tycho’s timing model and cannot be eliminated at
the architectural level.
The implementation therefore treats process telemetry as a retrospective signal
that must be aligned to, but not conflated with, the device-level event timeline.

To preserve temporal consistency, process queries are issued in conjunction with
device polling, but their results are attached only to confirmed device updates.
Each process record is associated with the monotonic timestamp of the
corresponding device snapshot, establishing a clear temporal anchor without
implying instantaneous semantics.
Wall-clock durations are tracked independently per device or MIG instance to
ensure that backend windows advance correctly regardless of monotonic tick
spacing.

Failure handling is deliberately non-blocking.
If a process query fails or returns incomplete data, the collector advances the
wall-clock origin to avoid repeated zero-length windows, while device-level
sampling proceeds unaffected.
This ensures that transient backend failures degrade attribution fidelity
locally without destabilising cadence inference or event emission.

\subsubsection{Robustness and Failure Modes}

GPU telemetry exhibits substantial variability across hardware generations,
driver versions, and backend capabilities.
The implementation is therefore designed to preserve architectural guarantees
under incomplete or degraded signals rather than to assume uniform availability.

Missing cumulative energy counters are handled through per-device capability
tracking.
When authoritative counters are unavailable or non-monotonic, freshness
detection falls back to power-based inference with conservative thresholds,
preventing noise-induced duplicate events at the cost of increased uncertainty.
Backend-specific differences between NVML and DCGM are treated as input
variability, not as control flow, ensuring that sampling and emission semantics
remain consistent.

Publish cadence jitter is absorbed by the
phase-aware inference mechanism.
Because estimators are updated only on confirmed publishes, short-term timing
irregularities do not propagate into spurious alignment corrections or event
duplication.
At worst, detection latency increases temporarily, while the one-tick-per-publish
invariant remains intact.

MIG instances are handled uniformly as independent telemetry sources during
collection.
No additional analytical assumptions are introduced at this stage, and MIG
metadata is propagated without special treatment.
This conservative stance avoids overstating attribution guarantees in
configurations where downstream analysis does not explicitly model MIG
topologies.

\subsubsection{Implementation Consequences}

The GPU collector implementation enforces the architectural timing guarantees in
the presence of implicit publish cadences, heterogeneous backend capabilities,
and partial observability.
In practice, this ensures that the GPU event stream is free of redundant samples,
causally ordered with respect to all other measurement domains, and aligned to
genuine hardware updates rather than to polling artefacts.
The one-to-one correspondence between confirmed device publishes and emitted
\code{GpuTick} events is preserved even under jitter, missing counters, or
transient backend failures.

At the same time, the implementation inherits unavoidable limitations from the
telemetry ecosystem.
Publish cadence inference is necessarily approximate, and process-level
utilisation remains aggregated over backend-defined wall-clock windows.
These constraints bound the temporal precision of attribution but do not violate
the correctness or ordering guarantees of the GPU timeline.

By producing a temporally consistent, event-driven GPU measurement stream, the
collector enables downstream analysis stages to correlate accelerator activity
with CPU, memory, and platform power without resampling or heuristic alignment.
This integration is a prerequisite for accurate cross-domain attribution and
allows later stages to reason about GPU energy consumption under the same
invariants that govern all other Tycho subsystems.

\subsubsection{Collected Metrics}

The GPU collector reports both device-level and process-level telemetry for each
emitted \code{GpuTick}.
Device metrics capture the instantaneous operational state of the accelerator at
the time of a confirmed publish, while process metrics describe aggregated
utilisation over the corresponding backend window.
Tables~\ref{tab:gpu-device-metrics} and~\ref{tab:gpu-process-metrics} summarise the
metrics collected at each level.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.7cm} p{8.8cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Utilisation metrics}} \\[4pt]
\code{SMUtilPct}      & \%   & Streaming multiprocessor (SM) utilisation. \\
\code{MemUtilPct}     & \%   & Memory controller utilisation. \\
\code{EncUtilPct}     & \%   & Hardware video encoder utilisation. \\
\code{DecUtilPct}     & \%   & Hardware video decoder utilisation. \\[4pt]

\multicolumn{3}{l}{\textit{Energy and thermal metrics}} \\[4pt]
\code{PowerMilliW}        & mW & Instantaneous power via NVML/DCGM (1s average). \\
\code{InstantPowerMilliW} & mW & High-frequency instantaneous power from NVIDIA field APIs. \\
\code{CumEnergyMilliJ}    & mJ & Cumulative energy counter (preferred freshness signal). \\
\code{TempC}              & °C & GPU temperature. \\[4pt]

\multicolumn{3}{l}{\textit{Memory and frequency metrics}} \\[4pt]
\code{MemUsedBytes}   & bytes & Allocated framebuffer memory. \\
\code{MemTotalBytes}  & bytes & Total framebuffer memory. \\
\code{SMClockMHz}     & MHz   & SM clock frequency. \\
\code{MemClockMHz}    & MHz   & Memory clock frequency. \\[4pt]

\multicolumn{3}{l}{\textit{Topology and metadata}} \\[4pt]
\code{DeviceIndex}    & --    & Numeric device identifier. \\
\code{UUID}           & --    & Stable device UUID. \\
\code{PCIBusID}       & --    & PCI bus identifier. \\
\code{IsMIG}          & --    & Indicates a MIG instance. \\
\code{MIGParentID}    & --    & Parent device index for MIG instances. \\
\code{Backend}        & --    & Backend type (\code{NVML} or \code{DCGM}). \\
\bottomrule
\end{tabular}
\caption{Device- and MIG-level metrics collected by the GPU subsystem.}
\label{tab:gpu-device-metrics}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.7cm} p{8.8cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\code{Pid}              & --   & Process identifier. \\
\code{ComputeUtil}      & \%   & Per-process SM utilisation aggregated over the query window. \\
\code{MemUtil}          & \%   & Per-process memory controller utilisation. \\
\code{EncUtil}          & \%   & Per-process encoder utilisation. \\
\code{DecUtil}          & \%   & Per-process decoder utilisation. \\[4pt]

\code{GpuIndex}         & --   & Device or MIG instance to which the sample belongs. \\
\code{GpuUUID}          & --   & Corresponding device UUID. \\
\code{TimeStampUS}      & µs   & Backend timestamp associated with the utilisation record. \\[4pt]

\multicolumn{3}{l}{\textit{MIG metadata (when applicable)}} \\[4pt]
\code{GpuInstanceID}     & --  & MIG GPU instance identifier. \\
\code{ComputeInstanceID} & --  & MIG compute-instance identifier. \\
\bottomrule
\end{tabular}
\caption{Process-level metrics collected over a backend-defined time window.}
\label{tab:gpu-process-metrics}
\end{table}


\section{Metadata and Identity Infrastructure}
\label{sec:impl_metadata}
% Implementation of the architectural metadata subsystem.
% Provides stable identities and hierarchy required for attribution.
% No interpretation of metrics or attribution logic is performed here.

\subsection{Hierarchy Modeling}

\subsubsection{Node, Workload, Pod, Container Identities}
% Describe how identities at different hierarchy levels are represented in practice.
% Emphasize consistency with the architectural attribution hierarchy.
% Clarify that identities are treated as labels for joining, not as attribution decisions.

\subsubsection{Join Keys and Referential Stability}
% Explain which identifiers are used to join metrics to identities.
% Describe how referential stability is ensured within attribution windows.
% Emphasize avoidance of ambiguous or time-unstable joins.

\subsection{Identity Lifetime Management}

\subsubsection{Stability Within Attribution Windows}
% Explain how identity changes are prevented or controlled within a window.
% Emphasize window-local consistency guarantees required for correct attribution.

\subsubsection{Controlled Evolution Across Windows}
% Describe how identity creation, deletion, and changes are handled across windows.
% Emphasize explicit transitions rather than silent reinterpretation.

\subsection{Degradation Under Metadata Incompleteness}

\subsubsection{Missing Joins and Fallback Semantics}
% Describe behavior when identity information is missing or incomplete.
% Explain fallback attribution behavior and its limitations.
% Emphasize explicit degradation rather than incorrect attribution.

\section{Calibration Mechanisms}
\label{sec:impl_calibration}
% Realization of architectural calibration concepts.
% Calibration supports correctness of temporal alignment and metric construction.
% Calibration does not perform attribution or interpretation.

\subsection{Polling-Frequency Calibration}

\subsubsection{Motivation and Correctness Role}
% Explain why effective polling frequency matters for windowed analysis.
% Describe how polling characteristics influence temporal coverage and bias.
% Emphasize calibration as a correctness prerequisite, not an optimization.

\subsubsection{Application to Collector Scheduling}
% Describe how calibrated polling information is applied in practice.
% Clarify that collectors remain independently scheduled.
% Emphasize that calibration informs analysis assumptions, not collector control.

\subsection{Delay Calibration}

\subsubsection{Delay Estimation}
% Describe how source-specific observation delays are estimated.
% Explain the role of calibration runs or measurements.
% Emphasize bounded, approximate delay characterization.

\subsubsection{Use of Calibrated Delays in Analysis}
% Describe how calibrated delays are applied during event-time alignment.
% Clarify that delays are used to shift or bound observations, not to reorder causality.

\subsection{Calibration Failure Modes}

\subsubsection{Stale Calibration and Safety Constraints}
% Describe behavior when calibration data becomes stale or unavailable.
% Explain safety constraints and conservative fallback behavior.
% Emphasize explicit degradation rather than silent misuse of invalid calibration.


\section{Analysis and Attribution Pipeline}
\label{sec:impl_analysis_pipeline}

\subsection{Pipeline Orchestration and Stage Execution}
\subsubsection{Stage Ordering and Dependencies}
\subsubsection{Per-Window Execution Contract}

\subsection{Stage 1: Component Metric Construction}
% This is where *_corrected metrics belong (implementation of architectural Stage 1).

\subsubsection{Aligned Per-Window Inputs}
% How raw observations are selected per window and aligned consistently.

\subsubsection{eBPF Utilization Metrics (Totals and Aggregates)}
% Construction of utilization totals and workload aggregates used downstream.

\subsubsection{RAPL Domain Energy Metrics}
% Construction of per-window domain energies from raw RAPL observations.

\subsubsection{Redfish-Corrected System Energy Metric}
% Implementation of cross-domain fusion producing redfish_corrected.
% Treated as ground truth for subsequent stages.

\subsubsection{GPU-Corrected Energy Metric}
% Implementation of intra-domain GPU fusion producing gpu_corrected.

\subsection{Stage 2: System-Level Energy Model and Residual}
% Realization of the architectural system-level energy model.
% This stage establishes a conserved energy budget per window and derives the residual.

\subsubsection{Global Energy Decomposition Realization}
% Describe how per-window component energies are combined.
% Explain realization of the global decomposition:
% RAPL domains + GPU energy + residual = redfish_corrected system energy.
% Emphasize per-node scope and window-local consistency.

\subsubsection{Residual Computation}
% Describe how residual energy is computed as the unassigned remainder.
% Explain ordering dependencies on Stage 1 outputs.
% Clarify that residual is a first-class output, not an error term.

\subsubsection{Handling of Negative Residuals in Practice}
% Describe how temporary negative residuals can arise due to delayed system response.
% Explain why negative residuals are tolerated and not clamped aggressively.
% Emphasize recovery over subsequent windows.

\subsubsection{Conservation and Consistency Checks}
% Describe checks enforcing conservation and internal consistency.
% Explain how violations are detected and handled.
% Emphasize invariant preservation over local precision.

\subsection{Stage 3: Idle and Dynamic Energy Semantics}
% Realization of architectural idle and dynamic energy definitions.
% This stage decomposes per-component energy without workload attribution.

\subsubsection{RAPL Idle and Dynamic Realization}
% Describe separation of RAPL domain energy into idle and dynamic parts.
% Explain reliance on utilization signals and window semantics.
% Emphasize consistency with architectural definitions.

\subsubsection{Redfish-Corrected Idle and Dynamic Realization}
% Describe idle and dynamic separation at the system level.
% Explain how redfish_corrected is decomposed without workload semantics.
% Clarify relationship to residual energy.

\subsubsection{GPU Idle and Dynamic Realization}
% Describe separation of GPU energy into idle and dynamic components.
% Emphasize that GPU idle is not attributed to workloads.
% State that idle GPU energy is assigned to the system.
\subsection{Stage 4: Workload Attribution and Aggregation}
% Realization of architectural workload attribution.
% This stage assigns dynamic and idle energy to workloads and aggregates results hierarchically.

\subsubsection{Attribution Identity Join and Join Failure Handling}
% Describe how component metrics are joined with workload identities.
% Explain join assumptions and required consistency within a window.
% Describe explicit handling of join failures and missing identities.

\subsubsection{CPU Dynamic Attribution}
% Describe proportional attribution of dynamic CPU energy to workloads.
% Explain reliance on utilization-derived weights.
% Emphasize window-local correctness and conservation.

\subsubsection{CPU Idle Allocation}
% Describe allocation of CPU idle energy to workloads.
% Explain architectural rationale for idle distribution.
% Emphasize consistency with idle+dynamic conservation.

\subsubsection{GPU Dynamic Attribution}
% Describe attribution of dynamic GPU energy to workloads.
% Explain handling of multiple GPUs and concurrent workloads.
% Emphasize separation from GPU idle handling.

\subsubsection{GPU Idle Handling (\texttt{\_\_system\_\_})}
% State explicitly that GPU idle energy is not attributed to workloads.
% Describe assignment of GPU idle energy to the system identity.
% Clarify implications for workload-level totals.

\subsubsection{Workload-Level and Hierarchical Aggregation}
% Describe aggregation of attributed energy across hierarchy levels.
% Explain construction of workload, pod, and node-level totals.
% Emphasize preservation of conservation across aggregations.

\section{Correctness, Robustness, and Degradation Behavior}
\label{sec:impl_robustness}
% Cross-cutting implementation concerns.
% This section explains how architectural guarantees are preserved under non-ideal conditions.

\subsection{Architectural Invariant Enforcement}

\subsubsection{Conservation Enforcement Strategy}
% Describe how energy conservation is enforced at each analysis stage.
% Explain detection and handling of conservation violations.
% Emphasize window-local enforcement and recovery behavior.

\subsubsection{Idle+Dynamic Consistency Enforcement}
% Describe enforcement of idle+dynamic=total relationships.
% Explain how inconsistencies are detected and bounded.
% Emphasize consistency across domains and hierarchy levels.

\subsection{Partial Observability and Missing Data}

\subsubsection{Missing Source Samples}
% Describe behavior when one or more metric sources are missing for a window.
% Explain conservative handling to avoid incorrect attribution.
% Emphasize explicit marking of reduced validity.

\subsubsection{Missing Metadata Joins}
% Describe behavior when workload identities cannot be resolved.
% Explain fallback attribution semantics.
% Emphasize avoidance of silent misattribution.

\subsection{Transient Pathologies}

\subsubsection{Temporary Drops in Idle Signals}
% Describe observed transient drops in idle metrics.
% Explain why these are tolerated and non-permanent.
% Emphasize recovery over subsequent windows.

\subsubsection{Residual Negativity and Recovery}
% Describe transient negative residual behavior.
% Explain why this does not violate architectural correctness.
% Emphasize convergence and bounded impact.

\subsection{Graceful Degradation Paths}
% Summarize explicit degradation behaviors under sustained invalid conditions.
% Emphasize predictable and transparent fallback rather than failure.

\section{Implementation Trade-Offs and Design Decisions}
\label{sec:impl_tradeoffs}
% Reflection on major implementation choices that are not obvious from the architecture alone.
% Focus on trade-offs required to realize correctness under practical constraints.

\subsection{Accuracy vs Complexity}
% Discuss trade-offs between model fidelity and implementation complexity.
% Explain why Tycho favors accuracy-first designs even at higher complexity.
% Clarify which simplifications were intentionally avoided.

\subsection{Timing Precision vs System Overhead}
% Discuss trade-offs between temporal precision and runtime overhead.
% Explain why certain timing resolutions or calibration strategies were chosen.
% Emphasize bounded imprecision over uncontrolled overhead.

\subsection{Alternatives Considered}
% Briefly summarize alternative implementation strategies that were evaluated.
% Explain why they were rejected in favor of the chosen design.
% Keep discussion high-level and Tycho-specific.

\section{Summary}
% Summarize how the implementation realizes the architectural concepts.
% Reinforce preservation of correctness and invariants.
% Prepare the reader for the evaluation and results chapters.

