\chapter{Implementation}
\label{chap:implementation}

\section{Purpose, Scope, and Execution-Time Structure}
\label{sec:impl_overview}

This chapter explains how Tycho’s architectural abstractions are realised at runtime under the constraints of discretization, partial observability, and asynchronous execution. Its role is to describe how responsibility boundaries defined in the architecture are enforced concretely, and how correct attribution is achieved despite imperfect and delayed inputs. Architectural concepts, models, and invariants are assumed from earlier chapters and are not reintroduced here.

At execution time, Tycho is structured as a set of long-lived subsystems with strictly separated responsibilities and unidirectional interaction. Each subsystem exercises authority over a narrow concern, and no subsystem compensates implicitly for the behaviour of others. This execution-time separation forms the foundation for correctness, auditability, and robustness throughout the implementation.

Additional implementation detail that is not required for understanding the
runtime structure or correctness arguments is intentionally deferred to
\hyperref[sec:tycho_sysenv]{Appendix~C}.
The appendix collects auxiliary material such as extended configuration
descriptions, supporting scripts, and low-level operational notes that aid
reproducibility and inspection without obscuring the main implementation
narrative.

\subsection{Runtime Subsystems and Responsibilities}

Tycho’s runtime consists of the following subsystems, each of which is examined in detail later in this chapter:

\begin{itemize}
\item The \textbf{timing engine} provides execution-time coordination by triggering collection and analysis actions according to a global schedule, without participating in interpretation or attribution (\S~\ref{sec:impl_temporal}).

\item \textbf{Metric collectors} act as independent observers that acquire raw measurements from individual hardware and software domains and emit timestamped samples without coordination or semantic interpretation (\S~\ref{sec:impl_collectors}).

\item The \textbf{metadata subsystem} maintains a refreshed view of workload identity and hierarchy, supplying identity context during attribution without joining metric streams or performing analysis (\S~\ref{sec:impl_metadata}).

\item \textbf{Calibration mechanisms} derive auxiliary parameters that characterise source behaviour and contextualise interpretation, executing outside steady-state attribution and without modifying observations (\S~\ref{sec:impl_calibration}).

\item The \textbf{analysis engine} is the sole authority responsible for interpreting observations, fusing domains, applying attribution models, and enforcing architectural invariants on a per-window basis.

\item \textbf{Export} observes the results of analysis and exposes them to external systems without influencing upstream execution or attribution semantics
\end{itemize}

\subsection{Execution-Time Interaction Model}

Interaction between these subsystems follows a strictly unidirectional pattern. Temporal authority originates in the timing engine, observation authority in collectors and metadata acquisition, and semantic authority exclusively in the analysis engine. Data flows forward through explicit handoff only: raw observations and identity context are materialised upstream and consumed read-only during analysis, while attribution results flow downstream to export.

This interaction model deliberately excludes feedback paths, implicit coordination, and retroactive modification of observations. Once emitted, samples are immutable; once a window is analysed, its results are final. These constraints ensure that attribution semantics remain explicit, reproducible, and independent of scheduling or export behaviour.

The remainder of this chapter elaborates on how each subsystem realises its assigned responsibility in practice, addressing temporal realisation, collection mechanics, identity handling, calibration, attribution, and robustness in turn.

\section{Temporal Infrastructure and Window Realization}
\label{sec:impl_temporal}

\subsection{Architectural Context and Implementation Problem}

\S~\ref{sec:timing_engine} defines Tycho’s temporal model in abstract terms: a single monotonic time base, independently operating collectors, and fixed-duration analysis windows triggered by a timing engine. The implementation task is to realize this model under real execution constraints while preserving its guarantees, rather than restating its semantics.

Collectors are scheduled by a general-purpose operating system and are subject to jitter, preemption and variable execution latency. Polling callbacks may execute late, at uneven intervals, or out of phase with one another. Analysis must therefore not depend on execution order, callback timing, or implicit synchronization effects. Temporal correctness must derive exclusively from explicit timestamps attached to observations, not from when code happens to execute. The temporal infrastructure enforces this separation rigorously.

\subsection{Global Monotonic Time Realization}

The architectural event-time model relies on a single system-wide monotonic time base. Its implementation elevates monotonic time to a first-class dependency rather than treating it as an incidental property of the runtime environment. All collectors, the timing engine and the analysis engine obtain temporal information exclusively through a dedicated clock abstraction.

Wall-clock time is excluded from analysis-critical paths and appears only where external representation is unavoidable. The clock abstraction provides monotonic timestamps for observations and analysis boundaries, mediates conversion between real-time and monotonic representations where required, and is injected into downstream components to ensure consistent and testable temporal behavior across subsystems.

As a result, scheduling jitter becomes an explicit input to analysis rather than a hidden source of error. A collector that executes late produces a correspondingly late timestamp. No corrective action is taken at collection time; timestamps are interpreted during analysis according to the delay and freshness assumptions defined in \S~\ref{sec:timing_engine}. Monotonic timestamps thus constitute the sole temporal authority within the system.

\subsection{Timing Engine and Hierarchical Cadence Alignment}

Independent collector schedules are a central architectural principle. Realizing this independence without sacrificing determinism requires a controlled mechanism for initiating periodic actions. Tycho employs a centralized timing engine to which all periodic activities register during system initialization.

Each registration specifies a period expressed as an integer multiple of a global base quantum (default: 1\,ms). All registrations are aligned to a shared epoch defined by this quantum, establishing deterministic phasing across the system. Collector and analysis triggers are hierarchically derived from this common cadence rather than started opportunistically, ensuring that identical configurations produce identical temporal behavior across runs. Alignment does not impose a shared frequency: collectors with different periods remain independent, but their triggers occur at deterministic offsets on the global monotonic axis.

The timing engine is deliberately non-semantic. It does not inspect collected data, adapt schedules, or coordinate collectors. Its sole responsibility is to emit triggers at predetermined monotonic times. Work performed in response to a trigger is constrained to be minimal and non-blocking, typically limited to recording an observation and placing it into a buffer, preventing local execution delays from propagating into global timing behavior.

Analysis triggering is implemented using the same registration mechanism. The analysis engine registers a periodic trigger alongside collectors, making analysis execution subject to the same alignment and determinism guarantees. This design enforces single-cycle exclusivity by construction: a new analysis cycle cannot begin before the previous trigger boundary has been established, without requiring additional synchronization logic.

\subsection{Analysis Window Realization and Trigger Semantics}

Analysis windows are realized when an analysis trigger fires. At that instant, the timing engine provides a single monotonic timestamp \(t_{\text{now}}\), which defines the upper boundary of the current window. This timestamp is captured exactly once and propagated unchanged throughout the entire analysis cycle. All metrics are evaluated against windows derived from this shared boundary, and no component recomputes or refines the window definition during analysis. Consequently, all attribution decisions within a cycle refer to an identical temporal interval, independent of execution order or internal processing latency.

Window boundaries are defined by trigger times rather than sample arrival. Samples collected before \(t_{\text{now}}\) may be included or excluded according to domain-specific delay and freshness rules as defined in \S~\ref{sec:timing_engine}. Samples arriving after the trigger are attributed to subsequent windows. This separation prevents double-counting and omission even under heterogeneous collector rates.

Temporal complexity is intentionally confined to timestamping and delay interpretation. Window construction itself remains simple and predictable, providing a stable temporal substrate on which later analysis stages can reason about delay, partial observation and attribution correctness without embedding scheduling assumptions or compensating for execution artifacts.

\section{Historical Observation Retention}
\label{sec:impl_history}

Tycho retains a bounded history of raw observations in order to support downstream analysis that requires temporal context beyond a single attribution window. This retention is an explicit implementation responsibility derived from the temporal model in \S~\ref{sec:timing_engine}. Rather than operating exclusively on window-local samples, Tycho preserves historical signal to mitigate discretization effects, tolerate heterogeneous collector cadences, and enable mathematically stable downstream interpretation.

\subsubsection{Metric Observation Retention}
Historical retention is realized through per-collector observation buffers with time-based semantics. Each collector appends observations to its own buffer, which retains a fixed-duration history with a default horizon of approximately 90\,s. This horizon deliberately exceeds the nominal analysis window length and is chosen to provide substantial temporal context for downstream analysis. Buffer capacity is computed at startup from the collector’s polling interval and the configured analysis window, ensuring that retained history covers at least twice the longer of these durations, augmented by a small safety margin. Under Tycho’s default configuration for high-frequency analysis, this corresponds to retention spanning approximately 18 full analysis windows, providing substantial historical context for downstream analysis models. Retention is bounded and fixed for the lifetime of the process.

Buffered samples are append-only and immutable once written. Downstream components access buffered data strictly in a read-only manner. No guarantee is made that all collectors contribute samples to every window or that samples are temporally aligned across collectors. Partial observability and heterogeneous update patterns are therefore preserved explicitly and interpreted by the analysis engine rather than hidden by synchronization.

\subsubsection{Metadata Retention}
In addition to metric observations, Tycho maintains a bounded cache of metadata describing process, cgroup and workload identities. This cache employs a time-based retention policy aligned with the metric retention horizon to ensure that buffered observations can be joined with valid identity information during analysis. Metadata history is not used for long-term modeling and is removed once it exceeds the retention window.

\section{Metric Collection Subsystems}
\label{sec:impl_collectors}

\subsection{eBPF Collector Implementation}
\label{subsec:impl_ebpf}
This section describes how Tycho realises the event-driven CPU ownership and activity model introduced in \S~\ref{subsec:ebpf_temporal}.  
The eBPF collector implements a kernel-level acquisition path that captures execution boundaries precisely and exposes the resulting activity as bounded, per-window deltas for downstream analysis.

\subsubsection{Implementation Strategy}
The implementation follows a split-surface strategy that separates \emph{attributable} activity from \emph{non-attributable} CPU time.  
Attributable activity is accumulated per process at scheduler boundaries, together with stable identity and classification metadata.  
Non-attributable activity, including idle time and interrupt handling, is accumulated per CPU and exported independently.  
This separation reflects Tycho’s attribution requirements: process-level ownership must be preserved without ambiguity, while certain CPU time categories cannot be meaningfully assigned to workloads.  
All kernel programs operate strictly locally, without cross-CPU or cross-process aggregation; consolidation and interpretation are deferred to userspace and later analysis stages.

\subsubsection{Core Mechanisms}
Process-level accounting is driven by scheduler transitions.  
At each context switch, the outgoing execution interval is closed and its duration is accumulated into the corresponding process aggregate, together with hardware performance counters sampled at the same boundary.  
Process identity, control-group association, and kernel-thread classification are captured at these execution boundaries and stored alongside the counters.  
By aligning accumulation with actual ownership changes, the implementation preserves the architectural guarantee that execution intervals form precise attribution boundaries.

CPU-local accounting handles activity that is not attributable to individual processes.  
Each CPU maintains a local state machine that tracks the currently active context and the timestamp of the last transition.  
Idle time is detected explicitly via the scheduler’s idle task and accumulated when the CPU executes in this state.  
Hard interrupt and soft interrupt handling are measured as outermost intervals using entry and exit hooks, with durations accumulated into per-CPU bins.  
This design avoids double counting under nested interrupts while preserving a complete partition of CPU time at the node level.

Userspace collection materialises kernel-side accumulation into analysis-ready deltas.  
At a fixed polling cadence, the collector snapshots all process aggregates and resets only their counter fields while preserving identity metadata.  
This stable-key snapshot design avoids missing-entry artefacts that can occur if keys are deleted while scheduler updates are in flight.  
CPU-level bins are read and reset in the same cycle, defining a clear collection boundary for idle and interrupt time.  
The result of each collection cycle is a single tick record that represents all observed activity since the previous boundary, without overlap or double counting.

\subsubsection{Robustness and Edge Cases}
Several implementation choices ensure robustness under concurrent kernel activity.  
Process aggregates persist across collection cycles, and only delta-relevant fields are reset, preventing transient key loss under concurrent scheduler updates.  
All kernel-side state is bounded through per-CPU arrays, bounded per-process maps, and fixed-size histograms for interrupt vector enrichment.  
Reset failures or map evictions are treated as non-fatal; correctness is restored automatically in subsequent cycles.  
A fundamental observability limit remains: processes that execute entirely between two collection boundaries may not be observed.  
This limitation is inherent to discrete materialisation and is not compensated by inference.

\subsubsection{Implementation Consequences}
In practice, the eBPF collector produces a fixed-resolution utilisation and activity surface that preserves execution-boundary accuracy and stable process identity.  
Per-window deltas are exported without imposing additional timing constraints on the analysis engine, enabling proportional attribution and energy modelling in later stages.

\subsubsection{Collected Metrics}
The process-level and CPU-level metrics exported by the eBPF collector are listed in table Table~\ref{tab:ebpf-collector-metrics}.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{p{2.8cm} p{3.8cm} p{6.0cm}}
    \toprule
    \textbf{Metric} & \textbf{Source hook} & \textbf{Description} \\
    \midrule
    \multicolumn{3}{l}{\textit{Time-based metrics}} \\[4pt]
    Process runtime & \code{tp\_btf/sched\_switch} & Per process. Elapsed on-CPU time accumulated at context switches. \\
    Idle time & Derived from\newline\code{sched\_switch} & Per node. Aggregated idle time across CPUs. \\
    IRQ time & \code{irq\_handler\_{\{entry,exit\}}} & Per node. Aggregated duration spent in hardware interrupt handlers. \\
    SoftIRQ time & \code{softirq\_{\{entry,exit\}}} & Per node. Aggregated duration spent in deferred kernel work. \\[4pt]
    
    \addlinespace[8pt]
    \multicolumn{3}{l}{\textit{Hardware-based metrics}} \\[4pt]
    CPU cycles & PMU\newline(\code{perf\_event\_array}) & Per process. Retired CPU cycle count during task execution. \\
    Instructions & PMU\newline(\code{perf\_event\_array}) & Per process. Retired instruction count. \\
    Cache misses & PMU\newline(\code{perf\_event\_array}) & Per process. Last-level cache misses; indicator of memory intensity. \\[4pt]
    
    \addlinespace[8pt]
    \multicolumn{3}{l}{\textit{Classification and enrichment metrics}} \\[4pt]
    Cgroup ID & \code{sched\_switch} & Per process. Control group identifier for container attribution. \\
    Kernel thread flag & \code{sched\_switch} & Per process. Marks kernel threads executing in system context. \\
    Page cache hits & \code{mark\_page\_accessed} & Per process. Read or write access to cached pages; proxy for I/O activity. \\
    IRQ vectors & \code{softirq\_entry} & Per process. Frequency of specific soft interrupt vectors. \\
    \bottomrule
    \end{tabular}
    \caption{Metrics collected by the kernel \code{eBPF} subsystem.}
    \label{tab:ebpf-collector-metrics}
\end{table}

\subsection{RAPL Collector Implementation}
\label{sec:rapl_collector}

This section realizes the architectural model of cumulative CPU-domain energy sampling described in \S~\ref{subsec:rapl_temporal}.  
The collector’s responsibility is strictly limited to observing hardware-provided cumulative energy counters at tick boundaries and preserving their semantics.  

\subsubsection{Implementation Strategy}
To preserve domain-consistent CPU energy measurement across vendors, the collector employs a dual-backend strategy selected at runtime via CPUID.  
On Intel systems, energy is obtained through the RAPL interface exposed via the \code{powercap} subsystem.  
On AMD systems, the collector preferentially uses \code{amd\_energy} via the \code{hwmon} interface, which provides accurate CPU energy telemetry on these platforms.  
The powercap path is used on AMD only if no CPU-labeled hwmon energy source is present.  
This strategy ensures that the architectural RAPL domain abstraction is realized consistently, independent of vendor-specific exposure mechanisms.

\subsubsection{Core Mechanisms}
At each tick, the collector obtains a single snapshot of cumulative energy counters for all available CPU domains and sockets and records them unchanged.  
All values are stored as cumulative microjoule counters, matching the native units of the underlying sources.  
Supported domains include package and core on all platforms, with uncore (PP1) and DRAM recorded when exposed by the hardware.  
On AMD systems, \code{amd\_energy} publishes per-logical-core energy values; these are aggregated by summation into a single cumulative core-domain counter to preserve domain semantics.  
Domains not provided by the platform are recorded as zero to maintain a stable domain set across ticks.

\subsubsection{Robustness and Edge Cases}
Powercap-backed RAPL counters may wrap and are corrected at collection time to maintain monotonicity.  
The hwmon-backed \code{amd\_energy} counters do not wrap and require no correction.
Each sample is tagged exclusively with a monotonic timestamp provided by Tycho’s timing engine.  
Partial reads are permitted and result in partial ticks; no backend instability was observed in practice.

\subsubsection{Implementation Consequences}
In practice, the collector guarantees exactly one cumulative energy sample per supported domain and socket at each tick, with vendor-independent domain semantics and bounded noise.  
The resulting time series form a stable input for downstream differencing and attribution.  
The only remaining limitation is platform-dependent domain availability, which is intentionally exposed rather than approximated.

\subsubsection{Collected Metrics}
The RAPL collector exports raw cumulative energy counters once per tick.  
Table~\ref{tab:rapl-metrics} summarizes the metrics recorded per \code{RaplTick}.
\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.9cm} p{8.6cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Per-socket energy counters}} \\[4pt]
\code{Pkg}    & mJ & Cumulative package energy per socket\newline(RAPL \code{PKG} domain). \\
\code{Core}   & mJ & Cumulative core energy per socket\newline (RAPL \code{PP0} domain), when available. \\
\code{Uncore} & mJ & Cumulative uncore energy per socket\newline (RAPL \code{PP1} or uncore domain), when available. \\
\code{DRAM}   & mJ & Cumulative DRAM energy per socket\newline (RAPL \code{DRAM} domain), if the platform exposes it. \\[6pt]

\multicolumn{3}{l}{\textit{Metadata}} \\[4pt]
\code{Source}    & -- & Identifier of the active RAPL backend (for example \code{powercap})\\
\code{Sockets}   & -- & Map from socket identifier to the corresponding set of domain counters. \\
\code{SampleMeta.Mono} & -- & Monotonic timestamp assigned by Tycho's timing engine at the moment of collection. \\
\bottomrule
\end{tabular}
\caption{Metrics exported by the RAPL collector per \code{RaplTick}.}
\label{tab:rapl-metrics}
\end{table}

\subsection{Redfish Collector Implementation}
\label{subsec:impl_redfish}

This section describes how Tycho realizes the Redfish power source defined in \S~\ref{subsec:redfish_temporal}.  
Redfish is treated as an externally clocked, latently published observation whose update cadence and timing semantics are controlled by the Baseboard Management Controller.  
The implementation must therefore tolerate missing observations, expose temporal uncertainty explicitly, and avoid manufacturing samples while still enabling a coherent downstream power timeline.

\subsubsection{Implementation Strategy}
The Redfish collector issues at most one query per engine tick and emits a record only when a power value can be obtained reliably or when an explicit continuity fallback is required.  
The collector is permitted to emit no record for a tick.  
This choice reflects the architectural intent to avoid speculative continuity and to preserve the distinction between absence of information and persistence of state.  
Temporal alignment to Tycho’s monotonic timebase is achieved by timestamping at collection time, without attempting synchronization or correction of BMC time.

\subsubsection{Freshness Realization and Semantics}
Freshness is realized as a best-effort quality annotation computed as the difference between the local monotonic collection time and the timestamp provided by the BMC, when available.  
Given the limited and vendor-specific semantics of BMC timestamps, the collector applies no correction, filtering, or normalization.  
Freshness therefore represents observed latency and staleness rather than a validity constraint.

When continuation records are emitted, the collector reuses the most recent BMC timestamp and recomputes freshness accordingly.  
As a consequence, freshness increases during prolonged publication gaps, making temporal uncertainty explicit.  
The collector never suppresses, alters, or reclassifies samples based on freshness.  
All values are forwarded unchanged as downstream quality indicators.

\subsubsection{Heartbeat-Based Continuity as Fallback}
Irregular Redfish publication implies that fixed-cadence sampling may observe extended periods without new measurements.  
The collector therefore supports an optional heartbeat mechanism whose role is explicitly fallback-oriented.  
Heartbeat does not operate on every missed tick.  
Instead, it re-emits a specially marked continuation record only when no fresh observation has been obtained for a comparatively long interval relative to the engine cadence.

By default, this interval substantially exceeds the collection period, ensuring that short-lived access failures or transient gaps result in silence rather than artificial continuity.  
If enabled, the heartbeat threshold may be configured statically or derived adaptively from observed inter-arrival times of fresh Redfish updates, with conservative bounds to avoid pathological behavior under highly irregular BMC implementations.  
Heartbeat emission never invents new measurements.  
It explicitly signals persistence of the last known value when prolonged absence would otherwise break temporal continuity.

\subsubsection{Robustness Under Partial Observation}
Redfish access failures and missing timestamps are treated as normal operating conditions.  
If a Redfish query fails, the collector emits no record for that tick.  
No retries, backoff strategies, or suppression mechanisms influence the semantic output.  
Only when the heartbeat threshold is exceeded does the collector emit a continuation record, clearly distinguishing prolonged absence from transient failure.

Multiple chassis are handled independently.  
Freshness computation, heartbeat state, and continuation decisions are maintained per chassis and never synchronized across nodes.  
This preserves architectural assumptions about the independence of Redfish power sources in multi-node deployments.

\subsubsection{Implementation Consequences}
In practice, the collector emits at most one record per chassis per engine tick, with zero records as a valid and expected outcome.  
Continuity is preserved only when absence becomes prolonged, and even then without obscuring staleness.  
The resulting stream provides a stable, monotonic reference for total node power while exposing uncertainty rather than masking it.

This implementation anchors Tycho’s global energy view and supports later reconciliation with in-band estimates without conflating observation authority or temporal semantics.  
Its limitations are deliberate.  
Temporal resolution and accuracy are bounded by the BMC, and no component-level attribution is attempted at this stage.

\subsubsection{Collected Metrics}
The Redfish collector emits instantaneous chassis power together with identity and temporal metadata.  
Only raw observations are produced.  
Derived quantities such as energy are computed by downstream analysis stages.  
The exported fields are summarized in Table~\ref{tab:redfish-metrics}.
\begin{table}[H]
\centering
\begin{tabular}{p{3cm} p{1cm} p{9cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Primary power metric}} \\[4pt]
\code{PowerWatts} & W & Instantaneous chassis power reported by the BMC. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Temporal and identity metadata}} \\[4pt]
\code{ChassisID} & - & Identifier of the chassis or enclosure. \\
\code{Seq} & - & Server-provided sequence number indicating new measurements. \\
\code{SourceTime} & s & Timestamp provided by the BMC, if available. \\
\code{CollectorTime} & s & Local collection time of the measurement. \\
\code{FreshnessMs} & ms & Difference between \code{SourceTime} and\newline\code{CollectorTime}. \\
\bottomrule
\end{tabular}
\caption{Metrics collected by the Redfish collector.}
\label{tab:redfish-metrics}
\end{table}

\subsection{GPU Collector Implementation}
\label{subsec:gpu_implementation}

The GPU collector realises the architecture described in
\S~\ref{subsec:gpu_architecture} and integrates accelerator telemetry into
Tycho’s unified temporal framework.
In contrast to other energy domains, GPU telemetry is published at discrete,
driver-controlled moments that are neither continuous nor externally observable.
The implementation is therefore responsible for enforcing phase-aligned,
event-driven sampling under partial observability, backend variability, and
timing jitter, while preserving strict monotonic ordering across all domains.

The central implementation invariant is that at most one \code{GpuTick} is
emitted per confirmed hardware publish, and that no tick is emitted without a
detectable device update.
All mechanisms described in this section exist to uphold this invariant in
practice, including the integration of retrospective process-level telemetry
under wall-clock semantics.

\subsubsection{Implementation Strategy}

The implementation treats GPU sampling as an inference problem rather than a
periodic measurement task.
Because the driver’s publish cadence is implicit, polling is used only as a
means to detect new hardware updates, not as a proxy for time.
Sampling effort is modulated according to the phase-aware timing model defined
in \S~\ref{subsec:gpu_phaseaware_formal}, concentrating observation near predicted
publish moments while suppressing redundant reads elsewhere.

Freshness detection and event emission are deliberately decoupled.
Polling may occur at high frequency, but a \code{GpuTick} is emitted only when a
previously unseen device update is detected and can be placed monotonically into
Tycho’s multi-domain buffer.
This separation ensures that increased polling density improves detection
latency without inflating the event stream or distorting temporal structure.

Device-level and process-level telemetry are integrated asymmetrically.
Device snapshots define the temporal anchor of each \code{GpuTick}, while
process-level records are attached retrospectively to confirmed device updates
to accommodate backend-imposed wall-clock windows.
This strategy preserves the architectural timing guarantees while enabling
multi-tenant attribution under heterogeneous backend constraints.

\subsubsection{Phase-Aware Sampling Realisation}

The phase-aware timing model defined in \S~\ref{subsec:gpu_phaseaware_formal} is
realised through a conservative observation pipeline that separates sampling
attempts from update confirmation.
Polling is driven by predicted publish moments, but observations are accepted
only when they provide evidence of a previously unseen hardware update.
This prevents both aliasing and redundant emission under irregular driver
cadence.

Freshness detection prioritises the strongest available backend signal.
When reliable cumulative energy counters are present, monotonic advancement of
these counters serves as the authoritative indicator of a new publish.
On devices lacking such counters, freshness is inferred from instantaneous power
changes exceeding a noise-tolerant threshold.
In both cases, snapshots that do not satisfy freshness criteria are discarded
without affecting estimator state or downstream timelines.

Duplicate suppression is enforced by conditioning all state updates on confirmed
freshness.
Period and phase estimators are advanced only when a new publish is detected,
ensuring that redundant polls neither bias cadence inference nor generate
spurious alignment corrections.
This guarantees that increased polling density reduces detection latency without
inflating the logical event stream.

\subsubsection{Event Construction and Emission}

When a fresh device update is confirmed, the collector constructs a
\code{GpuTick} that represents the accelerator state at a single monotonic
timestamp.
The device snapshot defines the temporal anchor of the event.
If process-level telemetry is available, the corresponding utilisation records,
aggregated over a backend-defined wall-clock window, are attached
retrospectively to the same tick.

Tick emission is strictly conditional on update confirmation.
No \code{GpuTick} is produced for redundant or ambiguous observations, and no
tick is emitted retroactively.
Each emitted tick is inserted into Tycho’s multi-domain buffer in monotonic
order, preserving causal alignment with RAPL, Redfish, and eBPF data without
interpolation or reordering.

This construction enforces a one-to-one correspondence between hardware publishes
and GPU events.
As a result, the GPU timeline reflects device behaviour rather than sampling
artefacts and provides a temporally consistent input to subsequent attribution
stages.

\subsubsection{Process Telemetry Integration}

Process-level GPU telemetry is exposed by the backend only as utilisation
aggregated over an explicit wall-clock interval.
This constraint is external to Tycho’s timing model and cannot be eliminated at
the architectural level.
The implementation therefore treats process telemetry as a retrospective signal
that must be aligned to, but not conflated with, the device-level event timeline.

To preserve temporal consistency, process queries are issued in conjunction with
device polling, but their results are attached only to confirmed device updates.
Each process record is associated with the monotonic timestamp of the
corresponding device snapshot, establishing a clear temporal anchor without
implying instantaneous semantics.
Wall-clock durations are tracked independently per device or MIG instance to
ensure that backend windows advance correctly regardless of monotonic tick
spacing.

Failure handling is deliberately non-blocking.
If a process query fails or returns incomplete data, the collector advances the
wall-clock origin to avoid repeated zero-length windows, while device-level
sampling proceeds unaffected.
This ensures that transient backend failures degrade attribution fidelity
locally without destabilising cadence inference or event emission.

\subsubsection{Robustness and Failure Modes}

GPU telemetry exhibits substantial variability across hardware generations,
driver versions, and backend capabilities.
The implementation is therefore designed to preserve architectural guarantees
under incomplete or degraded signals rather than to assume uniform availability.

Missing cumulative energy counters are handled through per-device capability
tracking.
When authoritative counters are unavailable or non-monotonic, freshness
detection falls back to power-based inference with conservative thresholds,
preventing noise-induced duplicate events at the cost of increased uncertainty.
Backend-specific differences between NVML and DCGM are treated as input
variability, not as control flow, ensuring that sampling and emission semantics
remain consistent.

Publish cadence jitter is absorbed by the
phase-aware inference mechanism.
Because estimators are updated only on confirmed publishes, short-term timing
irregularities do not propagate into spurious alignment corrections or event
duplication.
At worst, detection latency increases temporarily, while the one-tick-per-publish
invariant remains intact.

MIG instances are handled uniformly as independent telemetry sources during
collection.
No additional analytical assumptions are introduced at this stage, and MIG
metadata is propagated without special treatment.
This conservative stance avoids overstating attribution guarantees in
configurations where downstream analysis does not explicitly model MIG
topologies.

\subsubsection{Implementation Consequences}

The GPU collector implementation enforces the architectural timing guarantees in
the presence of implicit publish cadences, heterogeneous backend capabilities,
and partial observability.
In practice, this ensures that the GPU event stream is free of redundant samples,
causally ordered with respect to all other measurement domains, and aligned to
genuine hardware updates rather than to polling artefacts.
The one-to-one correspondence between confirmed device publishes and emitted
\code{GpuTick} events is preserved even under jitter, missing counters, or
transient backend failures.

At the same time, the implementation inherits unavoidable limitations from the
telemetry ecosystem.
Publish cadence inference is necessarily approximate, and process-level
utilisation remains aggregated over backend-defined wall-clock windows.
These constraints bound the temporal precision of attribution but do not violate
the correctness or ordering guarantees of the GPU timeline.

By producing a temporally consistent, event-driven GPU measurement stream, the
collector enables downstream analysis stages to correlate accelerator activity
with CPU, memory, and platform power without resampling or heuristic alignment.
This integration is a prerequisite for accurate cross-domain attribution and
allows later stages to reason about GPU energy consumption under the same
invariants that govern all other Tycho subsystems.

\subsubsection{Collected Metrics}

The GPU collector reports both device-level and process-level telemetry for each
emitted \code{GpuTick}.
Device metrics capture the instantaneous operational state of the accelerator at
the time of a confirmed publish, while process metrics describe aggregated
utilisation over the corresponding backend window.
Tables~\ref{tab:gpu-device-metrics} and~\ref{tab:gpu-process-metrics} summarise the
metrics collected at each level.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.7cm} p{8.8cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Utilisation metrics}} \\[4pt]
\code{SMUtilPct}      & \%   & Streaming multiprocessor (SM) utilisation. \\
\code{MemUtilPct}     & \%   & Memory controller utilisation. \\
\code{EncUtilPct}     & \%   & Hardware video encoder utilisation. \\
\code{DecUtilPct}     & \%   & Hardware video decoder utilisation. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Energy and thermal metrics}} \\[4pt]
\code{PowerMilliW}        & mW & Instantaneous power via NVML/DCGM (1s average). \\
\code{InstantPowerMilliW} & mW & High-frequency instantaneous power from NVIDIA field APIs. \\
\code{CumEnergyMilliJ}    & mJ & Cumulative energy counter (preferred freshness signal). \\
\code{TempC}              & °C & GPU temperature. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Memory and frequency metrics}} \\[4pt]
\code{MemUsedBytes}   & bytes & Allocated framebuffer memory. \\
\code{MemTotalBytes}  & bytes & Total framebuffer memory. \\
\code{SMClockMHz}     & MHz   & SM clock frequency. \\
\code{MemClockMHz}    & MHz   & Memory clock frequency. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Topology and metadata}} \\[4pt]
\code{DeviceIndex}    & --    & Numeric device identifier. \\
\code{UUID}           & --    & Stable device UUID. \\
\code{PCIBusID}       & --    & PCI bus identifier. \\
\code{IsMIG}          & --    & Indicates a MIG instance. \\
\code{MIGParentID}    & --    & Parent device index for MIG instances. \\
\code{Backend}        & --    & Backend type (\code{NVML} or \code{DCGM}). \\
\bottomrule
\end{tabular}
\captionsetup{width=12.6cm}
\caption{Device-level metrics collected by the GPU subsystem.}
\label{tab:gpu-device-metrics}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{p{3.5cm} p{0.7cm} p{8.8cm}}
\toprule
\textbf{Metric} & \textbf{Unit} & \textbf{Description} \\
\midrule

\multicolumn{3}{l}{\textit{Per-process utilisation metrics}} \\[4pt]
\code{Pid}         & -- & Process identifier. \\
\code{ComputeUtil} & \%  & Per-process SM utilisation aggregated over the query window. \\
\code{MemUtil}     & \%  & Per-process memory controller utilisation. \\
\code{EncUtil}     & \%  & Per-process encoder utilisation. \\
\code{DecUtil}     & \%  & Per-process decoder utilisation. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Device and timing metadata}} \\[4pt]
\code{GpuIndex}    & -- & Device or MIG instance to which the sample belongs. \\
\code{GpuUUID}     & -- & Corresponding device UUID. \\
\code{TimeStampUS} & µs & Backend timestamp associated with the utilisation record. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{MIG metadata (when applicable)}} \\[4pt]
\code{GpuInstanceID}     & -- & MIG GPU instance identifier. \\
\code{ComputeInstanceID} & -- & MIG compute-instance identifier. \\

\bottomrule
\end{tabular}
\captionsetup{width=12.6cm}
\caption{Process-level metrics collected over a backend-defined time window.}
\label{tab:gpu-process-metrics}
\end{table}

\section{Metadata and Identity Infrastructure}
\label{sec:impl_metadata}

\subsection{Architectural Context}
This section realises the metadata subsystem defined in \S~\ref{sec:arch_metadata_collection} as a refresh-driven, bounded cache that supplies joinable workload identity to the analysis engine.
Metadata does not form a time series and is not evaluated over analysis windows.
Its function is to provide sufficiently fresh identity state at analysis boundaries while remaining robust under partial observability, workload churn, and asynchronous sources.

\subsection{Controller-Orchestrated Refresh and Lifetime Enforcement}
All metadata mutation is centralized in a metadata controller, which constitutes the sole authority over state updates and lifecycle management.
Collectors never modify analysis-visible state directly and never delete entries.
They submit observations to the controller, which serializes updates and enforces freshness and retention rules.
This separation is required to ensure that identity state is maximally fresh at analysis boundaries while remaining bounded and deterministic under missing or partial updates.

At the start of every analysis cycle, the analysis engine triggers exactly one metadata refresh.
This refresh dominates all other scheduling and ensures that identity state reflects the most recent observable system structure at the moment the analysis window is evaluated.
Additional refreshes within the same cycle are unnecessary, as the window is closed at the cycle boundary and later identity changes cannot affect its evaluation.

To bound metadata age when analysis cycles are long or sparse, the controller may execute collectors periodically.
Each collector has an independent freshness target, with defaults of $1\,\mathrm{s}$ for the proc collector and $3\,\mathrm{s}$ for the kubelet collector.
Background execution is suppressed when an analysis-triggered refresh is imminent, specifically when it lies within $250\,\mathrm{ms}$, ensuring that the cycle-start refresh dominates and that redundant collection near analysis boundaries is avoided.

Metadata collection is explicitly best-effort.
Collectors do not define a notion of success and may submit partial updates.
The controller accepts all updates without requiring a complete snapshot, and cache convergence is achieved through repeated refreshes.
Missing observations are handled exclusively through horizon-based expiry rather than through collection-time interpretation, preserving a strict separation between identity acquisition and attribution logic.

\subsection{Metadata Store, Keys, and Temporal Alignment}
Metadata is stored in an in-memory cache partitioned by entity type.
Entries represent the most recent known identity state and are overwritten in place on update; historical versions are not retained.
Pod entries are keyed by pod UID, container entries by normalized runtime container ID, and process entries by PID with the process start token (\code{StartJiffies}) retained for disambiguation.

PID reuse is handled by treating the pair (PID, \code{StartJiffies}) as the effective process identity.
When a PID is reused, a new entry is created rather than overwriting the prior instance, preventing stale process metadata from being joined with unrelated activity during overlapping analysis windows.

All metadata updates within a refresh are timestamped once using Tycho’s global monotonic timebase.
The same timestamp is applied uniformly to all entries updated in that refresh, ensuring consistent temporal alignment with metric observations without introducing enumeration-induced skew.
An auxiliary wall-clock timestamp is recorded for diagnostics but is not used in attribution logic.

Garbage collection is executed periodically by the controller and independently of refresh scheduling.
Entries whose last-seen timestamp falls outside the same retention horizon used for metric buffers are removed.
This alignment guarantees that any retained metric observation remains joinable with identity metadata while providing deterministic memory bounds and natural cache drainage under missing updates.

\subsection{Proc Collector}
The proc collector provides the operating-system view required to associate process-level activity with container identity.
It enumerates processes via the proc filesystem and records a deliberately minimal attribute set consisting of process identifiers, command name, and cgroup membership.
This minimalism avoids unstable or expensive enrichment at collection time while retaining all information required for later joins.

Process-to-container association is derived from cgroup membership and normalized into a runtime container identifier.
Both cgroup version~1 and version~2 layouts are supported, and normalization targets containerd and CRI-O runtimes.
Processes that cannot be mapped to a Kubernetes container are labeled with a sentinel container identifier stored directly in the process entry, avoiding synthetic container objects and keeping system activity explicit at analysis time.

Process enumeration is inherently racy.
Processes may disappear during traversal and individual reads may fail due to lifecycle races.
Such failures are treated as normal; only successfully read entries are updated, and stale state is removed by horizon-based expiry.

\subsection{Kubelet Collector}
The kubelet collector supplies the authoritative Kubernetes node-local view required for correct pod and container attribution.
It periodically retrieves the kubelet \code{/pods} endpoint and persists only identity information that cannot be reliably reconstructed after termination.

Container identifiers are normalized at collection time, and only the normalized form is stored.
Init containers and ephemeral containers are represented uniformly as container entries with lifecycle-dependent status categories.
Termination state and exit codes are recorded when available, allowing analysis to avoid attributing energy to completed workloads without relying on event histories.

The kubelet view is the sole stable source of resource specifications once workloads terminate.
Tycho therefore stores CPU and memory requests and limits for both containers and aggregated pods.
If the kubelet is temporarily unreachable, no updates are applied for that refresh.
There is no explicit freshness gating in analysis; degradation manifests through missing or stale joins bounded by the retention horizon.

\subsection{Metadata Contract and Join Surface}
The metadata subsystem exposes a fixed set of identity fields that define the complete join surface available to the analysis engine.
Tables~\ref{tab:process-metadata-collector-metrics}, \ref{tab:kubelet-metadata-pod}, and \ref{tab:kubelet-metadata-container} summarize the fields collected by the proc and kubelet collectors.
This inventory constitutes an implementation contract: attribution feasibility and correctness depend directly on the presence and semantics of these fields, and no additional identity information is assumed downstream.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{3cm} p{3.4cm} p{6.6cm}}
\toprule
\textbf{Field} & \textbf{Source} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Process identity}} \\[4pt]
PID & \code{/proc} & Numeric process identifier; unique at any moment but reused over time. \\
StartJiffies & \code{/proc/<pid>/stat} & Kernel start time of the process in clock ticks (jiffies), used to detect PID reuse. \\[4pt]
\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Container and system classification}} \\[4pt]
Container ID & Kepler cgroup\newline resolver & Normalized container identifier for pod processes; \code{system\_processes} for host and kernel processes. \\
Command & \code{/proc/<pid>/comm} & Short command name for debugging and manual inspection. \\[4pt]
\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Timestamps}} \\[4pt]
LastSeenMono & Monotonic timebase & Timestamp aligned with metric collectors. \\
LastSeenWall & Controller timestamp & Wall-clock timestamp for GC. \\
\bottomrule
\end{tabular}
\caption{Process metadata collected by the process collector}
\label{tab:process-metadata-collector-metrics}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{p{2.6cm} p{3.8cm} p{6.6cm}}
\toprule
\textbf{Field} & \textbf{Source} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Pod identity}} \\[4pt]
PodUID & Kubelet PodList & Stable pod identifier for correlation\newline and container grouping. \\
PodName,\newline Namespace & Kubelet PodList & Human-readable pod identity\newline and namespace. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Lifecycle and scheduling context}} \\[4pt]
Phase & PodStatus & Coarse pod state\newline (Pending, Running, Succeeded, Failed). \\
QoSClass & PodStatus & Kubernetes QoS classification\newline (Guaranteed, Burstable, BestEffort). \\
OwnerKind /\newline OwnerName & Pod metadata & Controller reference\newline (e.g.\ ReplicaSet, DaemonSet). \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Resource specifications}} \\[4pt]
Requests\newline (CPU, Memory) & \code{pod.spec.containers} & Aggregate pod-level requests following Kubernetes scheduling semantics. \\
Limits\newline (CPU, Memory) & \code{pod.spec.containers} & Aggregate pod-level limits following\newline Kubernetes scheduling semantics. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Timestamps}} \\[4pt]
LastSeenMono & Monotonic timebase & Timestamp aligned with metric collectors. \\
LastSeenWall & Controller timestamp & Wall-clock timestamp for GC.\\
\bottomrule
\end{tabular}
\caption{Pod metadata collected by the kubelet collector}
\label{tab:kubelet-metadata-pod}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{p{2.6cm} p{3.8cm} p{6.6cm}}
\toprule
\textbf{Field} & \textbf{Source} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Container identity}} \\[4pt]
ContainerID & PodStatus & Normalized container identifier. \\
ContainerName & PodStatus & Declared container name within pod. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Lifecycle state}} \\[4pt]
State & ContainerStatus & Fine-grained state\newline (Running, Waiting, Terminated). \\
ExitCode & ContainerStatus & Termination exit code when available. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Resource specifications}} \\[4pt]
Requests\newline (CPU, Memory) & \code{pod.spec.containers} & Container-level resource requests;\newline preserved for terminated containers. \\
Limits\newline (CPU, Memory) & \code{pod.spec.containers} & Container-level resource limits. \\[4pt]

\addlinespace[8pt]
\multicolumn{3}{l}{\textit{Timestamps}} \\[4pt]
LastSeenMono & Monotonic timebase & Timestamp aligned with metric collectors. \\
LastSeenWall & Controller timestamp & Wall-clock timestamp for GC. \\
\bottomrule
\end{tabular}
\caption{Container metadata collected by the kubelet collector}
\label{tab:kubelet-metadata-container}
\end{table}

\subsection{Design Consequences and Exclusions}
By centralizing lifecycle enforcement and treating sources as independently valid, the subsystem provides maximally fresh identity at analysis boundaries while remaining robust to partial observability and transient failures.
Correctness relies on bounded freshness and explicit joins rather than on point-in-time snapshots or event histories.

\section{Calibration}
\label{sec:impl_calibration}

This section describes how the calibration architecture introduced in
\S~\ref{sec:calibration} is realized in Tycho’s implementation.
Calibration is implemented as a bounded, startup-time procedure whose sole role
is to reduce temporal uncertainty in hardware-controlled metric publication
before steady-state collection begins.
It exists to improve the correctness of downstream timing and attribution under
partial observability, without introducing runtime adaptation or feedback.

Calibration is optional and can be disabled via configuration.
When enabled, it is executed at most once per Tycho process lifetime and is not
re-entered or repeated.
All calibration results are node-local and apply only to the process instance
that performed them.

\subsection{Architectural Context}

The calibration architecture distinguishes two orthogonal concerns:
polling-frequency calibration, which bounds the minimum safe polling period for
hardware-controlled metric sources, and delay calibration, which bounds the
reaction latency between workload transitions and observable metric changes.
Only polling-frequency calibration is integrated into Tycho’s startup path.
Delay calibration is treated as an external preparatory step and is consumed
purely as static configuration.

Calibration parameters are consumed by the timing and analysis subsystems.
They do not influence runtime attribution logic directly and do not observe live
workloads.

\subsection{Startup Strategy and Collector Gating}

Polling-frequency calibration is executed during Tycho startup, prior to enabling
the affected collectors.
Collectors whose polling cadence depends on calibrated bounds are held inactive
until calibration completes or is bypassed.
As a consequence, no metrics from these collectors are emitted before a polling
interval has been selected.

Calibration does not block system startup indefinitely.
If insufficient observations are obtained or calibration fails for any reason,
Tycho falls back to user-configured default polling intervals.
This fallback is silent at the semantic level and does not alter runtime behavior,
ensuring that metric availability is not contingent on successful calibration.
Empirically derived bounds are preferred when available, but correctness does not
depend on their presence.

When multiple devices contribute to a single collector on a node, calibration
results are aggregated conservatively.
The most restrictive bound across all observed devices is selected and applied
uniformly, ensuring that no device-level publication is undersampled due to
intra-node variability.

\subsection{Polling-Frequency Calibration Mechanism}

Polling-frequency calibration is realized as a short-lived hyperpolling phase.
During this phase, Tycho queries the relevant hardware interface at a
conservatively high rate and passively observes the arrival of distinct metric
updates.
From these observations, it derives a conservative bound on the minimum safe
polling period that avoids undersampling under nominal conditions.

For GPU power metrics, calibration is performed independently for each device
using NVML.
Per-device observations are aggregated at node level, and the most restrictive
bound is selected.
The resulting polling period is then applied uniformly to all GPU collectors on
that node, ensuring consistent temporal coverage across heterogeneous devices.

For Redfish power metrics, calibration operates at the level of the BMC.
Observed updates across all exposed chassis contribute to the inferred cadence,
allowing calibration to remain valid in multi-chassis configurations.
When Redfish \textit{heartbeat} is enabled, the calibrated polling period is additionally
constrained by a hard cap derived from the heartbeat interval.
Calibration therefore establishes a lower bound on safe polling, while heartbeat
logic enforces upper limits on staleness during steady-state operation.
The two mechanisms are strictly layered and do not overlap in responsibility.

No polling-frequency calibration is performed for RAPL or eBPF.
RAPL counters update quasi-continuously relative to Tycho’s temporal resolution,
making undersampling architecturally irrelevant.
eBPF metrics are event-driven and decoupled from device-side publication cadence,
rendering polling-frequency discovery unnecessary.

\subsection{Delay Calibration Integration}

Delay calibration is not performed within Tycho.
Estimating the latency between workload transitions and observable metric
reactions requires controlled, high-intensity workload generation that is
incompatible with Tycho’s non-intrusive monitoring constraints.

Instead, delay calibration is carried out offline using external tooling that
executes directly on the target system.
These measurements derive conservative, device-specific delay bounds for GPU
power metrics.
The resulting bounds are supplied to Tycho exclusively via configuration.
At runtime, Tycho treats these bounds as static parameters and applies them during
analysis to prevent premature attribution and to align metric data with workload
phases.

When multiple GPU devices are present, Tycho selects the smallest configured
delay bound across devices and applies it uniformly.
This choice favors temporal responsiveness while remaining consistent with the
best-effort nature of delay estimation.

No delay calibration is applied to Redfish.
Irregular publication intervals, variable network latency, and opaque
BMC-internal behavior preclude stable delay estimation.
Redfish metrics are therefore treated as coarse signals whose temporal coherence
is enforced through freshness tracking and scheduling constraints rather than
delay correction.

\subsection{Implementation Consequences}

Calibration improves attribution correctness by reducing avoidable temporal
uncertainty in hardware-controlled metric sources.
It provides best-effort bounds that constrain polling and alignment decisions
without introducing runtime adaptation or control coupling.
By resolving these uncertainties ahead of steady-state operation, Tycho preserves
deterministic execution, bounded timing behavior, and strict separation between
observation and analysis.

\section{Analysis and Attribution Infrastructure}

\subsection{Analysis Engine Responsibilities and Cycle Lifecycle}
\label{sec:impl_analysis_engine_lifecycle}

This section describes how the orchestration model defined in \S~\ref{sec:arch_analysis_orchestration} is enforced at runtime.
The analysis engine is a minimal orchestration authority whose sole responsibility is to execute window-scoped attribution deterministically and without hidden coupling.
It constructs analysis cycles, selects attribution windows, enforces a fixed execution order, and establishes a materialization boundary for derived results.
The engine does not participate in metric semantics, dependency inference, modeling decisions, result interpretation, or exporter interaction.

Each invocation of the engine corresponds to exactly one analysis cycle.
Cycles are triggered externally by the timing engine; the analysis engine does not self-schedule, maintain timers, or perform background work.
Invocation cadence and jitter are therefore treated as external concerns and do not affect correctness, provided that invocations are serialized and the monotonic timebase is strictly increasing.

At cycle entry, the engine atomically constructs a fresh execution context that fully determines the behavior of the cycle.
This context includes the selected attribution window, admissibility constraints for reading observations, read-only access to upstream buffers, access to node-local metadata, access to shared cross-window state, and a cycle-local materialization boundary.
No additional execution context is injected after construction, and no global mutable analysis state is consulted during execution.

Once constructed, the cycle is executed exactly once.
The engine invokes all metrics sequentially according to a fixed execution plan.
There is no re-entry into a partially executed cycle, no mid-cycle rescheduling, and no orchestration-level retry or backtracking.
After execution completes, all cycle-local structures are discarded, including the materialization store, and no derived quantities persist implicitly into subsequent cycles.

Failures are isolated to the cycle in which they occur.
If an individual metric fails to produce a result, the failure is logged and execution continues for remaining metrics.
Such failures affect only the completeness of results for the current window and do not abort the cycle.
Each subsequent invocation constructs a new cycle with a fresh execution context, independent of prior errors or partial results.

By enforcing atomic cycle construction, single-pass execution, and explicit teardown, the engine guarantees that each cycle yields at most one coherent set of window-scoped results.
There is no opportunity for partial publication, retroactive correction, or cross-cycle interference.
This execution discipline provides the structural foundation on which staging, materialization, and cross-window modeling are implemented while preserving determinism, auditability, and non-retrospective semantics.

\subsection{Attribution Window Selection and Temporal Safety}
\label{sec:impl_analysis_window_selection}

This section describes how the attribution window defined in \S~\ref{sec:arch_analysis_orchestration_windows} is selected and enforced at runtime.
Window selection is performed exactly once at cycle entry and is derived exclusively from the global monotonic timebase, which serves as the sole temporal authority.
Wall-clock time, collector timestamps, and exporter behavior are not consulted, ensuring a strictly ordered and unambiguous temporal reference.

The window end \(t_k\) is selected with a fixed intentional lag relative to real-time execution.
This lag corresponds to the maximum admissible metric delay plus a safety margin obtained from configuration and optional calibration.
By construction, each window is therefore placed in a region of the past where all participating metrics are guaranteed to have observed the samples required to interpret their contributions under their declared delay semantics.
The lag is constant across cycles, yielding attribution windows with fixed duration and uniform semantics in steady state.

During startup, when insufficient observation history exists to populate the intended window fully, the window start may be clamped to the beginning of the monotonic timeline.
This behavior is explicit, deterministic, and confined to the initial phase of execution.
No other deviations from the steady-state window definition are permitted.

Window selection is deliberately decoupled from collector sampling behavior.
Collectors may operate at different frequencies, with irregular timing or transient gaps, without influencing window boundaries.
The engine does not impose an alignment grid or attempt to synchronize with sampling schedules.
Instead, windows are defined purely in terms of monotonic ticks, and metrics interpret the contents of upstream buffers over the selected interval using interval semantics and metric-local delay correction.

Once selected, the attribution window is immutable for the duration of the cycle.
All metrics observe the same base window, and any effective windows derived through delay correction are computed deterministically from this reference.
The engine does not revise window boundaries mid-cycle and does not reopen or reinterpret windows after execution.

By combining a monotonic timebase, a fixed safety lag, and immutable window selection, the implementation enforces temporal admissibility by construction.
Attribution correctness is insulated from collector jitter and delayed observations, speculative window closure is avoided, causality is preserved, and each cycle yields a single, final temporal interpretation of the available evidence.

\subsection{Staged Pipeline Execution and Dependency Discipline}
\label{sec:impl_analysis_staging}

This section explains how the dependency discipline defined in \S~\ref{sec:arch_analysis_orchestration_stages} is enforced at runtime.
Dependency correctness is not inferred dynamically and is not validated during execution.
Instead, it is achieved by construction through a fixed execution order that reflects the semantic structure of the analysis pipeline.

For each analysis cycle, the engine constructs a static execution plan consisting of an ordered list of metric invocations.
This plan is invariant across cycles for a given build and configuration.
Metrics are registered into the analysis registry in an order that encodes their semantic dependencies, and the engine preserves this order exactly during execution.
As a result, metrics executed later in the plan may depend on outputs produced earlier in the same cycle, while reverse dependencies are structurally impossible.

Stages are not represented as explicit runtime entities.
Instead, stage boundaries are implicit and correspond to contiguous segments of the execution plan.
Each segment groups metrics that operate at the same semantic level, such as aggregation, decomposition, or attribution.
Downstream segments assume that all metrics in earlier segments have either materialized their window-scoped outputs or abstained from doing so due to missing or inadmissible inputs.

The implementation deliberately avoids constructing dependency graphs, performing topological sorting, or validating dependency satisfaction at runtime.
There are no per-metric dependency declarations and no control flow conditioned on the availability of upstream results.
If a metric observes an undefined input, it degrades according to its semantics rather than triggering reordering, backtracking, or failure.

This design treats dependency correctness as an architectural obligation rather than an algorithmic problem.
The runtime enforces execution order faithfully but does not attempt to infer semantic validity.
By resolving dependencies explicitly at composition time, the implementation preserves determinism, avoids hidden coupling between metrics, and ensures that degradation under partial observability propagates monotonically through the pipeline without compromising internal consistency.

\subsection{Metric Materialization and Intra-Cycle Visibility}
\label{sec:impl_analysis_materialization}

This section describes how the materialization model is enforced at runtime.
At the beginning of each analysis cycle, the engine establishes a cycle-local materialization boundary that serves as the sole authoritative representation of all derived results for the current attribution window.
All metric emissions during the cycle are routed through this boundary, ensuring that materialization is strictly window-scoped and isolated from other cycles.

Derived results are recorded as immutable, window-scoped facts.
Within a cycle, each metric is expected to materialize at most one final value per metric identity and label set.
Multiple emissions for the same identity deterministically overwrite earlier ones, but such behavior is not relied upon for correctness.
The effective semantic contract is therefore exactly-once materialization per window.

All materialized results are conceptually visible to metrics executed later in the same cycle.
There is no notion of stage-local visibility or scoped access control.
Dependency correctness relies entirely on execution order rather than on restricting access to intermediate results.
Metrics that consume undefined inputs must degrade according to their semantics rather than delaying execution or attempting recovery.

Materialized results never persist implicitly across cycles.
When a cycle completes, the materialization boundary is discarded in its entirety, and all derived quantities cease to exist from the perspective of subsequent cycles.
Any dependence on prior windows must be mediated through explicit cross-window state owned and managed by the consuming metric.

By enforcing a per-cycle materialization boundary with global intra-cycle visibility and strict teardown semantics, the implementation guarantees that each cycle yields a single, final, and internally consistent set of derived quantities for its attribution window.
This prevents hidden cross-window coupling, eliminates incremental refinement of results, and allows later attribution stages to operate on materialized facts rather than raw observations while preserving determinism and auditability.

\subsection{Cross-Window State and Explicit Memory}
\label{sec:impl_analysis_cross_window_state}

This section describes how limited cross-window memory is supported without violating the window-scoped and non-retrospective execution model defined in \S~\ref{sec:analysis_attribution_arch}.
While derived results never persist across cycles, certain attribution models require controlled statefulness in order to converge or adapt over time.
The implementation accommodates such models through an explicit and narrowly scoped state mechanism.

Cross-window state is provided through a shared state store that persists for the lifetime of the analysis process and is made available to each cycle at construction time.
The state store is passive and provides no execution logic or semantic interpretation.
The analysis engine does not read from, write to, or reason about its contents; its sole responsibility is to supply access to metrics during cycle execution.

All cross-window state is strictly metric-owned.
Metrics that require memory across windows must explicitly retrieve, initialize, and update their own state entries.
Metrics that do not access the state store remain purely window-scoped and stateless by construction.
State entries are keyed by metric identity and labels, making ownership and scope explicit and preventing implicit sharing between unrelated metrics.

Use of cross-window memory is an explicit opt-in.
Any temporal coupling introduced by stateful behavior is therefore visible at the point of use and auditable in isolation.
State is updated only after a cycle completes and influences only subsequent cycles.
Previously materialized results are never revised, and no stateful mechanism can retroactively alter the interpretation of earlier windows.

By confining cross-window memory to explicit, metric-owned state and maintaining strict cycle boundaries, the implementation supports stateful attribution models where required, while preserving determinism, isolation, and the architectural guarantee that each cycle yields a single, final interpretation of its attribution window.

\subsection{Output Commit and Sink Boundary}
\label{sec:impl_analysis_sink_boundary}

This section describes how analysis results are committed and published without allowing downstream mechanisms to influence attribution semantics.
At cycle construction time, the engine establishes a collecting sink that serves as the sole emission boundary for all metrics executed during the cycle.
All derived results emitted by metrics are routed through this boundary and are treated as belonging to the same attribution window.

Result commitment occurs at the moment of materialization.
Once a derived quantity is recorded within the cycle-local materialization boundary, it is final for the current window.
Publication to downstream sinks is strictly observational and occurs after materialization without providing feedback into the analysis process.
The engine never reads from sinks, waits for acknowledgements, or conditions execution on publication success.

Sinks are therefore non-authoritative by design.
Exporter behavior may delay, drop, aggregate, or reorder published results, but such behavior does not alter the meaning or validity of the computed window-scoped quantities.
Attribution correctness is defined entirely within the analysis layer and is independent of downstream reliability or performance characteristics.

All results produced during a cycle are committed as a logical batch.
There is no partial publication of intermediate results and no distinction between provisional and final outputs.
Once cycle execution completes, the set of materialized results represents the maximal, internally consistent interpretation achievable for the attribution window.

By strictly separating analysis from publication, the implementation preserves reproducibility and robustness.
Failures at the export layer degrade only the visibility of results, not their correctness, and cannot introduce hidden coupling or timing dependencies into the attribution process.
This boundary ensures that attribution semantics remain stable, auditable, and invariant under changes to exporter implementation or behavior.

\subsection{Implementation Consequences and Guarantees}
\label{sec:impl_analysis_consequences}

The implementation described in this chapter enforces the architectural contract of the analysis layer directly through execution structure rather than dynamic control logic.
Each analysis cycle yields a single, deterministic, window-scoped interpretation of the available evidence, constructed under fixed temporal assumptions and executed in a strictly ordered, non-retrospective manner.
All derived results are materialized explicitly, remain immutable for the duration of the cycle, and are isolated from both prior and subsequent cycles.

Temporal correctness is ensured by construction.
Attribution windows are selected once per cycle from a global monotonic timebase using a fixed safety lag, are immutable during execution, and are interpreted without reliance on collector sampling grids or exporter timing.
As a result, causality is preserved and attribution semantics are insulated from collection jitter, delayed observations, and downstream publication behavior.

Dependency correctness is enforced structurally.
Metric execution order is fixed and reproducible, stages are implicit in the execution plan, and no runtime dependency inference or reordering occurs.
Under partial observability, results degrade monotonically through omission or explicitly defined fallback semantics, and previously materialized interpretations are never revised.

Stateful attribution models are supported exclusively through explicit, metric-owned cross-window state.
No derived quantities persist implicitly across cycles, and any temporal coupling introduced by memory is localized, visible, and auditable.
This preserves isolation between cycles while allowing convergence or adaptation where required.

Equally important are the properties the implementation does not guarantee.
The analysis layer does not ensure completeness of results for every window, does not backfill or reinterpret prior windows, does not validate semantic correctness of pipeline composition at runtime, and does not provide reliability guarantees for result publication.
These non-guarantees are deliberate and reflect the prioritization of correctness, determinism, and transparency over forced coverage or convenience.

Together, these consequences establish a stable execution contract for attribution.
They enable later analysis stages to operate on materialized, window-scoped quantities with well-defined temporal and dependency semantics, while ensuring that increasing analytical sophistication does not compromise determinism, auditability, or adherence to the architectural model.

\subsection{Stage 1: Component Metric Construction}
% This is where *_corrected metrics belong (implementation of architectural Stage 1).

\subsubsection{Component-Level eBPF Utilization Metrics (Totals)}
\label{sec:impl_ebpf_util_metrics}

\paragraph{Architectural Context.}
This subsubsection realizes the utilization metrics defined in \S~\ref{sec:arch_ebpf_util_metrics}.
It implements the window-aligned construction of node-level CPU time-share ratios and cumulative kernel and hardware counters from discrete eBPF observations, without redefining architectural semantics or introducing additional modeling assumptions.

\paragraph{Implementation Strategy.}
Discrete eBPF observations are consumed as per-tick deltas.
For each analysis window, deltas are integrated over an effective window interval to approximate the continuous quantities defined architecturally.
CPU execution time is normalized against node capacity to obtain ratios, while all other eBPF-derived signals are aggregated across processes and accumulated into persistent node-level counters.

\paragraph{Core Mechanisms.}
Per-tick deltas for CPU idle, hardware interrupt, and software interrupt execution are integrated independently over the window.
Normalization uses the product of window duration and logical CPU count to obtain dimensionless time-share ratios.
Active CPU utilization is derived as the complement of the integrated non-active components, enforcing exact conservation of schedulable CPU capacity.
For event-based signals, per-process deltas are summed at each tick, integrated over the window, and added to cumulative counters that advance monotonically across analysis cycles.

\paragraph{Robustness and Edge Cases.}
Window boundaries may intersect eBPF tick intervals.
Integration therefore accounts for partial overlap between ticks and windows to preserve conservation and avoid bias at window edges.
Internal accumulation admits fractional contributions to accommodate overlap, while exported counters are materialized as integer-valued quantities.
If insufficient eBPF samples are available to conservatively integrate a window, no metrics are emitted for that window.

\paragraph{Implementation Consequences.}
The implementation enforces the architectural guarantees of strict monotonicity for counters and exact partitioning of CPU capacity across utilization classes.
All cumulative metrics are scoped to the lifetime of the Tycho process and may reset on restart.
These semantics ensure stable interpretation under sampling variability and allow downstream stages to rely on consistent utilization signals.

\paragraph{Exported Metrics.}
The metrics exported by this implementation are summarized in the table~\ref{tab:ebpf-util-exported-metrics}.
Only fully materialized utilization metrics intended for external consumption are listed.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.2cm} p{1.8cm} p{1.6cm} p{2.0cm}}
\toprule
\textbf{Metric} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{CPU time-share ratios}} \\[4pt]
\code{bpf\_cpu\_idle\_ratio}    & Gauge  & ratio & \code{source} \\
\code{bpf\_cpu\_irq\_ratio}     & Gauge  & ratio & \code{source} \\
\code{bpf\_cpu\_softirq\_ratio} & Gauge  & ratio & \code{source} \\
\code{bpf\_cpu\_active\_ratio}  & Gauge  & ratio & \code{source} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Aggregated cumulative counters}} \\[4pt]
\code{bpf\_cpu\_instructions}    & Counter & count & \code{source} \\
\code{bpf\_cpu\_cycles}          & Counter & count & \code{source} \\
\code{bpf\_cache\_misses}        & Counter & count & \code{source} \\
\code{bpf\_page\_cache\_hits}    & Counter & count & \code{source} \\
\code{bpf\_irq\_net\_tx}         & Counter & count & \code{source} \\
\code{bpf\_irq\_net\_rx}         & Counter & count & \code{source} \\
\code{bpf\_irq\_block}           & Counter & count & \code{source} \\
\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{source}=\code{bpf}$.
\end{minipage}
\captionsetup{width=12.6cm}
\caption{Exported utilization metrics derived from \code{eBPF} observations.}
\label{tab:ebpf-util-exported-metrics}
\end{table}


\subsubsection{RAPL Component Metrics (Totals)}

\paragraph{Architectural Context.}
This section realises the cumulative RAPL energy model defined in the corresponding architecture subsubsection, producing zero-based, monotonic energy counters per RAPL domain and an auxiliary, window-averaged power signal.
Only total energy and total power are constructed here; no decomposition or attribution is performed at this stage.

\paragraph{Implementation Strategy.}
The implementation consumes time-ordered RAPL counter samples within the effective analysis window and aggregates them across sockets for each domain.
Energy counters are constructed directly from native cumulative readings, while power is derived secondarily from in-window energy differences.
All logic is structured to ensure that exported energy counters remain monotonic, zero-based, and independent of window boundaries.

\paragraph{Core Mechanisms.}
For each domain, the implementation identifies the first and last available cumulative RAPL counters within the effective window.
The last counter represents the native cumulative energy at window end.
To eliminate hardware-defined initial offsets, the first observed native value per domain is stored once and subtracted from all subsequent exports, yielding a zero-based cumulative energy counter.

Window-local energy increments are computed by differencing the first and last counters within the window.
Because raw RAPL inputs are already corrected for wraparound upstream, no additional wraparound handling is required at this stage.
Average power is then derived by dividing the in-window energy increment by the window duration.
This power signal is emitted alongside the energy counter but is not used for any downstream computation.

\paragraph{Robustness and Edge Cases.}
If fewer than two valid RAPL samples are available within a window, no metrics are emitted, avoiding partial or misleading updates.
Non-positive or ill-defined window durations suppress emission entirely.
All aggregation is performed per domain and summed across sockets conservatively, ensuring that missing socket data cannot inflate reported energy.
Since cumulative energy is always derived from native counters and offset subtraction is monotonic, exported energy values never decrease.

\paragraph{Implementation Consequences.}
The implementation guarantees that exported RAPL energy metrics are stable cumulative counters suitable as authoritative inputs for later stages.
Auxiliary power metrics are provided solely for inspection and convenience and are explicitly excluded from attribution logic.
Optional quality or diagnostic metadata may be emitted for debugging purposes, but it does not affect metric semantics.

\paragraph{Exported Metrics.}
The metrics exported by this implementation are summarized in the table~\ref{tab:rapl-exported-metrics}.
Only cumulative energy counters intended as authoritative inputs and their auxiliary, user-facing power counterparts are listed.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{4.0cm} p{1.8cm} p{1.6cm} p{5.2cm}}
\toprule
\textbf{Metric} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{RAPL component totals (per domain)}} \\[4pt]
\code{rapl\_energy\_mj} & Counter & mJ & \code{domain, kind, source} \\
\code{rapl\_power\_mw}  & Gauge   & mW & \code{domain, kind, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{domain}\in\{\text{pkg},\text{core},\text{uncore},\text{dram}\}$,
$\code{kind}=\text{total}$,
$\code{source}=\code{rapl}$.
\end{minipage}

\caption{Exported RAPL component energy and power metrics.}
\label{tab:rapl-exported-metrics}
\end{table}


\subsubsection{GPU Component Metrics (Totals)}

\paragraph{Architectural Context.}
This section realizes the architectural definition of GPU power and energy reconstruction as a history-aware, constraint-based process.
Unlike collectors that derive window-local quantities directly from raw samples, the GPU metric maintains a corrected power signal over a retained corrected-time horizon and derives windowed quantities as projections of that maintained signal.
Historical state is therefore treated as a correctness mechanism rather than an optimization, enabling reconciliation of delayed, averaged, and cumulative GPU observations into a single authoritative timeline.
All execution-time mechanisms described below exist to uphold this contract under discretization, partial observability, and bounded computation.

\paragraph{Implementation Strategy.}
GPU reconstruction is implemented by maintaining a per-device corrected power series on a uniform corrected-time grid that persists across analysis cycles.
Rather than recomputing power independently per window, the implementation incrementally extends and updates this series using newly available observations while preserving previously reconstructed history.
To bound computation, reconstruction is confined to a moving tail region near the current analysis window, while constraints are derived from observations spanning a longer retained history.
Windowed GPU energy and power are obtained by projecting the maintained series onto the analysis window, ensuring temporal coherence across windows and alignment with the accuracy-first architectural objective.

\paragraph{Constraint Realization.}
Raw GPU telemetry is realized as weighted constraints on the corrected power series.
Instantaneous power samples contribute point-wise soft constraints, while one-second averaged power samples are realized as boxcar-mean constraints over the preceding approximately one-second interval on the uniform grid.
Cumulative energy counters, when present and validated, contribute dominant consistency constraints derived from positive, monotonic energy deltas mapped to the corresponding grid intervals.
Constraint families use fixed internal relative weights reflecting expected reliability, with cumulative energy dominating when enabled.
All constraints act jointly on the reconstructed series in corrected time, avoiding sequential correction or signal prioritization.

\paragraph{Reconstruction and Update Semantics.}
Reconstruction is performed on a moving tail of the corrected power history covering the most recent portion of the retained horizon.
Only this tail segment is re-solved each cycle, after which it overwrites the corresponding region of the maintained history on the uniform grid.
This moving-horizon approach bounds computational cost while allowing historical observations to influence the present reconstruction.
If reconstruction cannot be performed due to insufficient or inconsistent observations, the previously retained history is preserved unchanged, ensuring continuity of the corrected signal across cycles.

\paragraph{Windowed Energy and Power Derivation.}
Windowed GPU energy is derived by projecting the corrected power history onto the current analysis window.
When validated cumulative energy counters are available and a prior baseline exists, the window energy increment is obtained directly from the counter delta to preserve absolute energy consistency.
Otherwise, window energy is computed by integrating the corrected power series over the window interval.
Windowed GPU power is derived from the corresponding window energy and duration and represents the same underlying quantity as a gauge.
Per-window energy values are treated as auxiliary quantities, while the maintained reconstructed history remains authoritative.

\paragraph{Robustness and Edge Cases.}
The implementation explicitly tolerates missing, delayed, and partially invalid GPU telemetry.
Cumulative energy constraints are enabled only when counters are present, non-zero, strictly increasing, and yield valid deltas; otherwise they are disabled automatically.
Non-negativity is enforced via post-solve projection to prevent physically implausible power estimates.
If projection affects a substantial fraction of bins, a conservative second reconstruction pass is attempted with cumulative energy constraints disabled, and the better-conditioned solution is retained.
Numerical conditioning is ensured through a minimal diagonal stabilization term applied purely for solver robustness and without modeling semantics.
Transient reconstruction failures preserve the previously retained history, preventing discontinuities in downstream metrics.

\paragraph{Implementation Consequences.}
By maintaining a corrected GPU power history across analysis cycles, the implementation achieves temporal coherence and energy consistency that window-local estimation cannot provide.
Historical context enables delayed, averaged, and cumulative observations to jointly constrain the reconstructed signal, improving stability and effective temporal resolution under realistic telemetry conditions.
When cumulative energy counters are available, absolute energy consistency is preserved across windows; otherwise the system degrades gracefully to history-informed integration without violating physical plausibility.
While no claim is made to recover ground-truth GPU power, all exported GPU energy and power metrics are guaranteed to derive from a single, internally consistent reconstructed signal that maximally exploits available information.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:gpu-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{4.0cm} p{1.8cm} p{1.6cm} p{5.2cm}}
\toprule
\textbf{Metric} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Total GPU metrics}} \\[4pt]
\code{gpu\_energy\_mj} & Counter & mJ & \code{gpu\_uuid, kind, source} \\
\code{gpu\_power\_mw}  & Gauge   & mW & \code{gpu\_uuid, kind, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}=\text{total}$,
$\code{source}=\code{nvml\_corrected}$.
\end{minipage}

\caption{Exported GPU metrics derived from the corrected reconstruction.}
\label{tab:gpu-exported-metrics}
\end{table}


\subsubsection{Redfish Component Metrics (Totals)}
\label{sec:metric_redfish_impl}

\paragraph{Architectural Context.}
This section realises the Redfish-based system power construction defined in \S~\ref{sec:metric_redfish_arch}.
The architecture specifies two complementary system-level series: a raw integration of Redfish telemetry and a corrected reconstruction that treats Redfish as an anchoring signal for a higher-rate proxy-based model.
The implementation must therefore support delay-aware observation handling, horizon-spanning state, adaptive alignment, and a controlled transition from raw to corrected emission without violating counter continuity.

\paragraph{Implementation Strategy.}
The implementation is split into three cooperating responsibilities.
First, a raw producer integrates sparse Redfish samples over the effective analysis window and maintains the canonical system counter during warmup.
Second, a fusion substrate maintains a fixed-grid horizon cache populated from component proxy metrics and refreshes Redfish observations under a selected effective delay.
Third, a corrected producer fits reconstruction parameters against the cached observations, reconstructs per-bin system power, and integrates the reconstruction over the current window.
A readiness flag stored in analysis state governs takeover: raw emission remains authoritative until corrected reconstruction is feasible for the selected chassis.

\paragraph{Core Mechanisms.}
Raw system metrics are produced by selecting all Redfish samples overlapping the delay-shifted effective window, including at most one predecessor sample to enable zero-order-hold integration.
Window energy is obtained by integrating held power values over the window extent, and the exported cumulative counter is advanced monotonically in persistent state.

Corrected reconstruction operates on a fixed fusion grid with configurable bin width and horizon length.
A horizon cache stores per-bin proxy features derived from CPU package energy, DRAM energy, GPU energy, and CPU instruction counts.
Only newly required bins are populated each cycle, while overlapping cache content is preserved by shifting the horizon forward.

Redfish observations are projected into the corrected time domain by subtracting a candidate delay.
An adaptive delay is selected by searching a bounded candidate set and scoring each candidate by the extent to which proxy-derived parts power exceeds Redfish power over a recent horizon segment, subject to an explicit noise margin.
Delay updates are rate-limited between cycles, with a higher allowable slew when a step change is detected in the proxy power series.
The selected delay is persisted in state and reused as a fallback when the search fails.

Model fitting is performed over the horizon using weighted least squares in scaled space.
The reconstruction combines proxy features into per-bin system power using non-negative coefficients for physically interpretable terms and an unconstrained instruction-rate coefficient.
When non-negativity constraints become active, a constrained refit is performed to reduce bias from post-fit truncation.
Reconstructed bin power is integrated over the overlap with the current analysis window to obtain window energy and average power.

To preserve counter continuity at takeover, the corrected cumulative counter is represented as the sum of a local corrected accumulator and a one-time offset.
The offset is seeded from the last available raw exported counter when corrected emission becomes active, ensuring that the canonical counter remains continuous across the transition.

\paragraph{Robustness and Edge Cases.}
Corrected emission is suppressed until a sufficient number of usable Redfish observations exist within the horizon.
Observations whose corrected timestamps would underflow are discarded to prevent degenerate kernel projections and unbounded bin scans.
If proxy features are unavailable or the delay-selection search fails, the implementation conservatively retains the previously selected delay or falls back to the configured default.

Delay adaptation is explicitly rate-limited to avoid oscillation under noise, while step detection enables faster convergence after large workload transitions.
All fitted coefficients are checked for finiteness before use; if fitting fails, the implementation falls back to the previously valid parameter vector or a conservative default.
Corrected emission is withheld entirely if these safeguards cannot be satisfied.

\paragraph{Implementation Consequences.}
During warmup, the raw Redfish integration remains the sole canonical system series.
Once corrected reconstruction becomes ready, the implementation deletes the raw canonical series from the sink and emits only corrected values thereafter, preventing ambiguous double-publication under identical metric identifiers.
The corrected counter does not reset at takeover due to the offset mechanism, but it remains a best-effort cumulative quantity that may restart on process restart.
Overall, the implementation prioritises temporal consistency and conservatism: when alignment or anchoring is unreliable, raw integration remains authoritative.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:redfish-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{4.0cm} p{1.8cm} p{1.6cm} p{5.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{System metrics}} \\[4pt]
\code{system\_power\_mw}  & Gauge   & mW & \code{chassis, source, kind} \\
\code{system\_energy\_mj} & Counter & mJ & \code{chassis, source, kind} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}=\text{total}$,
$\code{source}\in\{\code{redfish\_raw},\code{redfish\_corrected}\}$.
\end{minipage}

\caption{Exported Redfish-derived system metrics.}
\label{tab:redfish-exported-metrics}
\end{table}

\subsection{Stage 2: System-Level Energy Model and Residual}
\label{subsec:impl_stage2_residual}

\paragraph{Architectural Context.}
This stage realizes the system-level energy balance defined in \S~\ref{subsec:stage2_residual}.
Its role is to operationalize residual energy as a conservative, node-local quantity derived from system and component observations, while tolerating asynchronous and delayed inputs.
No workload attribution or semantic decomposition is performed at this stage.

\paragraph{Implementation Strategy.}
For each analysis window, the implementation derives window energy from window-averaged power and window duration for both system-level and component-level signals.
Residual energy is computed as a window-local difference and accumulated into a monotonic counter.
Power-level residual metrics are treated as auxiliary observables, whereas cumulative energy counters are considered authoritative.
An explicit window validity signal is emitted to indicate when residual interpretation is temporally reliable.

\paragraph{Core Mechanisms.}
Residual energy accumulation follows the architectural definition of $E_{\text{res}}$ from \S~\ref{subsec:stage2_residual}.
Window energy increments are clamped to non-negative values before accumulation to preserve monotonicity of the exported counter.
This clamping is applied strictly at the accumulation boundary and does not alter the conceptual definition of residual energy.
Residual power is derived from the same window-local quantities but is not used as a basis for accumulation.

\paragraph{Robustness and Edge Cases.}
Temporal misalignment between system-level and component-level signals may lead to transient inconsistencies at the power level.
These effects are contained by decoupling instantaneous power from cumulative energy accounting.
Rather than masking such artifacts, the implementation exposes an explicit window usability signal, allowing downstream stages to reason about residual validity without compromising conservation.
Warmup behavior is handled by maintaining continuity between raw and corrected system sources while preserving counter monotonicity.

\paragraph{Implementation Consequences.}
The implementation guarantees strict energy conservation at both window and cumulative levels.
Residual energy counters remain monotonic and conservative under all operating conditions.
Transient timing artifacts originating from system-level measurement latency may appear in auxiliary power metrics but do not affect energy correctness.
Residual energy thus forms a stable and auditable constraint for downstream decomposition and attribution stages.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:residual-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.0cm} p{1.3cm} p{1.1cm} p{5.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Residual energy and power}} \\[4pt]
\code{residual\_energy\_mj} & Counter & mJ & \code{chassis, source, kind} \\
\code{residual\_power\_mw}  & Gauge   & mW & \code{chassis, source, kind} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Residual window validity}} \\[4pt]
\code{residual\_window\_usable} & Gauge & bool & \code{chassis, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}=\text{total}$.
\end{minipage}

\caption{Exported residual metrics.}
\label{tab:residual-exported-metrics}
\end{table}

\subsection{Stage 3: Idle and Dynamic Energy Semantics}
% Realization of architectural idle and dynamic energy definitions.
% This stage decomposes per-component energy without workload attribution.
\subsubsection{RAPL Idle and Dynamic Decomposition}
\label{sec:impl_rapl_idle_dynamic}

\paragraph{Architectural Context.}
This section realises the architectural idle and dynamic split defined in \S~\ref{sec:arch_rapl_idle_dynamic} for RAPL domains by estimating a conservative idle baseline from utilization-conditioned observations and materialising a per-window energy decomposition that preserves strict conservation.

\paragraph{Implementation Strategy.}
The implementation derives idle power from the already exported total RAPL energy by differencing cumulative counters per window and estimating a baseline power $\beta_d$ from low-utilization operating points.
Utilization proxies are obtained directly from raw eBPF process ticks within the effective window to avoid dependencies on intermediate metric availability.
Idle estimation and smoothing are maintained as explicit cross-window state, while the per-window split is computed deterministically from the current baseline and total energy increment.

\paragraph{Core Mechanisms.}
For each domain, the cumulative total energy is converted to a window increment by differencing against the last observed value and clamping negative deltas to zero.
A utilization proxy $u_d$ is computed by normalizing the window rate of eBPF-derived activity against a rolling p95 amplitude, yielding a bounded proxy in $[0,1]$.
Pairs $(u_d, P^{\mathrm{tot}}_d)$ are fed into a scalar idle model that performs bucketed regression over low-utilization samples and returns the intercept as the candidate idle baseline.
Baseline updates are asymmetrically smoothed in time, permitting faster decreases than increases and suppressing upward adaptation until the p95 normalization is deemed stable.
The resulting idle power is clamped to the observed total power for the same window, converted to an idle energy increment, and subtracted from the total increment to obtain the dynamic remainder.
Both increments are accumulated into monotonic per-domain counters and exposed together with window-average powers.

\paragraph{Robustness and Edge Cases.}
If total energy is unavailable or observed for the first time, the cycle is skipped to avoid poisoning state.
Idle power is constrained to be non-negative and never exceed total power, ensuring that the split cannot generate energy even under transient noise or model error.
When utilization normalization is not yet stable, baseline increases are disabled to prevent premature upward bias, while decreases remain bounded to resist single-sample outliers.
All state required for differencing, normalization, modeling, and smoothing is scoped per domain, preventing cross-domain interference.

\paragraph{Implementation Consequences.}
The realised split guarantees per-window energy conservation and produces a conservative idle estimate that converges under sustained low activity while remaining safe on permanently loaded systems.
Dynamic energy is defined purely as the residual, ensuring that subsequent attribution stages operate on a well-bounded remainder without requiring access to the idle model internals.
The approach tolerates missing or delayed observations and degrades to zero-idle attribution rather than emitting inconsistent metrics.

While the CPU attribution path is comparatively compact, this reflects the availability of continuous, architecturally exposed counters and execution-derived proxies; GPU attribution necessarily incurs additional complexity due to backend-controlled publication semantics, implicit sampling phases, and heterogeneous telemetry guarantees.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:rapl-idle-dynamic-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{4.6cm} p{1.5cm} p{1.3cm} p{5.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Idle and dynamic RAPL metrics}} \\[4pt]
\code{rapl\_energy\_mj} & Counter & mJ & \code{domain, kind, source} \\
\code{rapl\_power\_mw}  & Gauge   & mW & \code{domain, kind, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{13.6cm}
\footnotesize\emph{Label domain:}
$\code{domain}\in\{\text{pkg},\text{core},\text{uncore},\text{dram}\}$,
$\code{kind}\in\{\text{idle},\text{dynamic}\}$,
$\code{source}=\code{rapl}$.
\end{minipage}

\caption{Exported RAPL idle and dynamic metrics.}
\label{tab:rapl-idle-dynamic-exported-metrics}
\end{table}

\subsubsection{Residual Idle and Dynamic Decomposition}

\paragraph{Architectural Context.}
This subsubsection realises the residual idle and dynamic decomposition defined in the architecture, which introduces a persistent residual idle baseline and a conditional interpretation contract based on window usability.
The implementation enforces exact conservation on the clamped residual budget while ensuring that learning and interpretation are explicitly gated under temporal misalignment.

\paragraph{Implementation Strategy.}
Residual idle and dynamic power are constructed from a non-negative residual budget derived per analysis window.
A persistent idle baseline is maintained as explicit cross-window state and is reused whenever learning is not permitted.
Learning of this baseline is strictly conditional on corrected system input, window usability, and a minimum residual magnitude.
Metric emission remains continuous and conservative in all windows, while baseline updates are selectively enabled.

\paragraph{Core Mechanisms.}
For each window, residual total power is obtained by subtracting the sum of aligned component power from system power and clamping the result to a non-negative budget.
Temporal misalignment is detected using conservative window-local indicators, including sustained lag of system power behind accounted components and sharp increases in component power coinciding with near-zero or clamped residuals.
A short hold horizon is applied to transient classification to prevent flapping.
Window usability is defined as the conjunction of non-transient classification and the absence of residual clamping.
The residual idle baseline is seeded on the first window that satisfies all learning preconditions.
Subsequent updates follow a candidate-then-commit mechanism that tracks sustained low residual observations and applies bounded downward adjustments only after confirmation.
Idle and dynamic residual power are materialized conservatively by capping idle at the clamped residual budget and assigning the remainder to dynamic, ensuring exact per-window conservation.
Energy counters are obtained by integrating window-local power and accumulated monotonically using the clamped budget, with explicit handling of the raw-to-corrected system metric transition.

\paragraph{Robustness and Edge Cases.}
Unusable windows do not trigger idle baseline learning and cannot decrease the baseline, preventing collapse during temporal misalignment.
Learning is forbidden during warmup to avoid contamination from raw system metrics.
Residual budgets below a minimum magnitude are excluded from learning to prevent noise fitting.
Downward baseline adaptation is hardened by requiring persistence across multiple consecutive learnable windows and by rate-limiting the maximum permanent drop per commit.
The idle component is always capped by the current clamped residual budget, preventing negative dynamic residuals even when the baseline temporarily exceeds the budget.
Once corrected system metrics become active, raw residual series are explicitly removed to avoid overlapping series with incompatible semantics.

\paragraph{Implementation Consequences.}
Residual idle and dynamic metrics are conservative by construction and conserve the clamped residual budget exactly in every window.
Persistent idle state remains stable under aggressive workload changes because temporally invalid windows are excluded from learning and the last committed baseline is held constant.
The usability signal establishes a hard contract for downstream consumers: residual idle and dynamic values may be present in all windows, but are only interpretable and learnable when usability is true.
The exported idle baseline is explicit model state and must not be interpreted as a physical idle quantity.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:residual-idle-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.4cm} p{1.2cm} p{1.0cm} p{5cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Residual idle and dynamic metrics}} \\[4pt]
\code{residual\_power\_mw}  & Gauge   & mW & \code{chassis, source, kind} \\
\code{residual\_energy\_mj} & Counter & mJ & \code{chassis, source, kind} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Residual idle model state}} \\[4pt]
\code{residual\_idle\_baseline\_mw} & Gauge & mW & \code{chassis, source} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Residual window validity}} \\[4pt]
\code{residual\_window\_usable} & Gauge & bool & \code{chassis, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{13.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}\in\{\text{idle},\text{dynamic}\}$,
$\code{source}\in\{\code{redfish\_raw},\code{redfish\_corrected}\}$.
\end{minipage}

\caption{Exported residual idle and dynamic metrics.}
\label{tab:residual-idle-exported-metrics}
\end{table}

\subsubsection{GPU Idle and Dynamic Decomposition}
\label{sec:impl_gpu_idle_dynamic}

\paragraph{Architectural Context.}
This section realizes the architectural GPU idle and dynamic decomposition defined in \S~\ref{sec:arch_gpu_idle_dynamic}, operating exclusively on the corrected per-device total power and window-consistent duration.

\paragraph{Implementation Strategy.}
For each GPU device, the implementation maintains a per-device idle estimator whose input is the corrected total power together with a compact utilization signal.
Idle estimation is updated opportunistically under stable conditions and otherwise held constant, ensuring that transient load changes do not perturb the baseline.
Idle and dynamic components are derived per window from the same total power observation and duration, then accumulated into monotonic energy counters.

\paragraph{Core Mechanisms.}
The implementation retrieves the corrected total power gauge and the corresponding window duration, and maps the most recent device utilization observation into the corrected window.
An idle estimator is instantiated per device and observes tuples $(u_{\mathrm{sm}}, u_{\mathrm{mem}}, P_{\mathrm{total}})$.
A new idle estimate is accepted only when utilization remains approximately constant and within a low-utilization region, yielding a stable baseline $\beta_{\mathit{uuid}}$.
For each window, idle power is obtained by clamping $\beta_{\mathit{uuid}}$ to the current total power, dynamic power is computed as the non-negative residual, and both are converted to window energy increments using the same duration as the total.
All three cumulative energies (total, idle, dynamic) are updated atomically in per-device state.

\paragraph{Robustness and Edge Cases.}
If total power is unavailable for a device in a window, no idle or dynamic update is performed to avoid inconsistent state.
Idle estimates are bounded to the interval $[0, P_{\mathrm{total}}]$ to prevent negative or superlinear components.
Stability gating prevents sudden utilization changes from contaminating the idle baseline, causing such transients to be attributed entirely to dynamic power.
All intermediate values are checked for non-finite results and clipped to preserve monotonicity of cumulative energy counters.

\paragraph{Implementation Consequences.}
The implementation enforces a conservative decomposition in which idle power evolves slowly and only under stationary conditions, while dynamic power absorbs short-term fluctuations.
Energy conservation holds per window by construction, and cumulative counters remain monotonic over the lifetime of the system.
The per-device state isolation allows heterogeneous GPUs to be handled independently without cross-coupling effects.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:gpu-idle-dynamic-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{%
  p{4.6cm}  % Metric name
  p{1.6cm}  % Type
  p{1.4cm}  % Unit
  p{5.0cm}  % Labels
}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{GPU idle and dynamic decomposition metrics}} \\[4pt]
\code{gpu\_power\_mw}
  & Gauge
  & mW
  & \code{gpu\_uuid, kind, source} \\
\code{gpu\_energy\_mj}
  & Counter
  & mJ
  & \code{gpu\_uuid, kind, source} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}\in\{\text{idle},\text{dynamic}\}$,
$\code{source}=\code{nvml\_corrected}$.
\end{minipage}

\caption{Exported GPU idle and dynamic decomposition metrics.}
\label{tab:gpu-idle-dynamic-exported-metrics}
\end{table}

\subsection{Stage 4: Workload Attribution and Aggregation}
% Realization of architectural workload attribution.
% This stage assigns dynamic and idle energy to workloads and aggregates results hierarchically.

\subsubsection{Workload Attribution of eBPF Utilization Counters}

\paragraph{Architectural Context.}
This section realizes the workload-level aggregation model, mapping per-process eBPF utilization deltas to workload-attributed monotonic counters while enforcing strict conservation and residual \code{\_\_system\_\_} semantics.

\paragraph{Implementation Strategy.}
Attribution is implemented as a window-local aggregation over the effective eBPF observation window.
All raw per-process deltas observed in the window are treated as authoritative inputs.
Resolution to workload identity is attempted per process using the standard Stage~4 resolver, but aggregation into non-system workloads is conditional.
Unresolved activity is not dropped or heuristically reassigned and is instead captured explicitly via residual construction.

\paragraph{Core Mechanisms.}
For each effective window, per-process deltas are integrated over the window interval using fractional overlap factors to account for partial boundary intersections.
Two accumulations are performed in parallel.
First, signal-wise totals are computed by summing all observed per-process deltas, independent of resolution.
Second, resolved per-process deltas are aggregated into per-workload buckets.
After aggregation, the \code{\_\_system\_\_} workload is materialized as the residual between the total activity and the sum of all non-system workloads, with negative residuals clamped to zero to preserve non-negativity.
All workload aggregates are accumulated into state-backed monotonic counters, preserving counter semantics across windows.

\paragraph{Robustness and Edge Cases.}
Partial observability affects only resolution, not conservation.
If process identity or workload metadata is unavailable, unstable, or inconsistent, the corresponding deltas contribute solely to the total and therefore increase the \code{\_\_system\_\_} residual.
Window boundary truncation is handled via fractional scaling, with accumulators maintained in floating-point state to avoid systematic loss under repeated truncation.
Emitted counter values are monotonically non-decreasing by construction, and defensive checks prevent regression under any execution order.
Workload series are garbage-collected after a bounded inactivity period, while the \code{\_\_system\_\_} series is explicitly exempt from deletion.

\paragraph{Implementation Consequences.}
The implementation guarantees strict conservation between system-level eBPF counters and the sum of workload-attributed counters over any analysis horizon.
Resolution failure degrades monotonically into \code{\_\_system\_\_} without redistributing activity among resolved workloads.
The resulting counters are suitable as first-class workload attribution outputs and as explanatory inputs for subsequent energy attribution and validation stages.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:ebpf-workload-util-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{7.5cm} p{1.2cm} p{1.0cm} p{2.9cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{CPU execution activity}} \\[4pt]
\code{workload\_bpf\_cpu\_instructions\_total} & Counter & count & \textit{common (see below)} \\
\code{workload\_bpf\_cpu\_cycles\_total}       & Counter & count & \textit{common (see below)} \\
\code{workload\_bpf\_cache\_misses\_total}     & Counter & count & \textit{common (see below)} \\
\code{workload\_bpf\_page\_cache\_hits\_total} & Counter & count & \textit{common (see below)} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Interrupt activity}} \\[4pt]
\code{workload\_bpf\_irq\_net\_tx\_total} & Counter & count & \textit{common (see below)} \\
\code{workload\_bpf\_irq\_net\_rx\_total} & Counter & count & \textit{common (see below)} \\
\code{workload\_bpf\_irq\_block\_total}   & Counter & count & \textit{common (see below)} \\[4pt]

\addlinespace[8pt]
\multicolumn{4}{l}{\textit{Process runtime}} \\[4pt]
\code{workload\_bpf\_process\_run\_us\_total} & Counter & $\mu$s & \textit{common (see below)} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{13.5cm}
\footnotesize\emph{Label domain:}
$\code{source}=\code{bpf}$;
labels \code{namespace}, \code{pod}, and \code{container} identify the workload.
\end{minipage}

\caption{Exported workload-attributed eBPF utilization counters.}
\label{tab:ebpf-workload-util-exported-metrics}
\end{table}

\subsubsection{Workload Resolution and Identity Enforcement}

\paragraph{Architectural Context.}
This section realizes the workload resolution abstraction introduced in the Stage~4 architecture.
The resolver provides a conditional mapping from execution-level observations to Kubernetes workload identities and enforces the semantics of the distinguished \code{\_\_system\_\_} class.
It is an auxiliary component of the attribution pipeline: attribution logic consumes its outputs but does not embed or compensate for resolution failures.
All guarantees are enforced here under discretization, metadata delay, and partial observability.

\paragraph{Implementation Strategy.}
Workload resolution is implemented as a best-effort, cycle-scoped join between execution identities and the metadata store associated with the current analysis cycle.
Resolution is intentionally asymmetric: positive identification requires multiple consistency checks, while failure at any point causes immediate fallback to an unresolved state.
The resolver never invents workload identities, never extrapolates beyond available metadata, and never retries resolution retroactively across cycles.
This strategy ensures conservative behavior and monotone degradation toward \code{\_\_system\_\_} under uncertainty.

\paragraph{Core Mechanisms.}
Resolution proceeds in three ordered stages.
First, a process-based path attempts to resolve a stable process identity using a guarded \code{(PID, StartJiffies)} pair to harden against PID reuse.
Second, when process identity is unavailable or unsafe, a cgroup-based fallback is attempted using only explicitly attributable cgroup identifiers.
Third, a resolved container identifier is mapped to Kubernetes namespace, pod, and container labels via the metadata store.
Only when all required identity components are present is a workload key materialized; otherwise resolution fails.
The resolver itself returns either a fully specified workload key or an unresolved outcome, with no intermediate states.

\paragraph{Robustness and Edge Cases.}
All resolution steps are guarded against known failure modes.
PID reuse is explicitly checked and causes conservative rejection rather than reassignment.
Sentinel or root cgroup identifiers are excluded to prevent poisoning of the mapping chain.
Missing, stale, or incomplete metadata entries immediately invalidate resolution.
Importantly, resolution failure does not propagate partial identity information: unresolved observations are not weakly labeled but are handled uniformly by the attribution layer as \code{\_\_system\_\_}.
This guarantees that loss of metadata cannot redistribute energy between resolved workloads.

\paragraph{Implementation Consequences.}
The resolver enforces a strict correctness boundary for Stage~4 attribution.
When identity information is reliable, workload attribution is precise and reproducible within the limits of the chosen fairness basis.
When identity information degrades, attribution degrades conservatively and explicitly without violating conservation or introducing hidden assumptions.
This behavior ensures that workload-resolved metrics remain interpretable across runs with varying metadata quality and system churn.

\paragraph{Exported Metrics.}
The workload resolver does not export metrics directly.
Its effects are observable only through the workload-labeled metrics produced by subsequent attribution stages, including the explicit appearance of the \code{\_\_system\_\_} workload class when resolution is not possible.

\subsubsection{CPU Dynamic Energy Attribution}

\paragraph{Architectural Context.}
This section realizes the bin-level CPU dynamic attribution model defined in the corresponding architecture section.
For each RAPL CPU domain, a per-window dynamic energy budget is distributed across workloads using execution-derived activity proxies, while enforcing conservation, non-negativity, completeness, and monotone degradation toward \code{\_\_system\_\_} under partial observability.

\paragraph{Implementation Strategy.}
The implementation treats each RAPL domain uniformly and executes attribution independently per domain.
The authoritative input is the per-window delta of the cumulative dynamic energy counter for the domain.
Workload attribution is computed at native eBPF tick resolution and only aggregated to window scope after all bin-level decisions have been made.
All attribution state is explicit and window-scoped, with no cross-window reinterpretation.

\paragraph{Core Mechanisms.}
Per-bin activity weights are constructed from eBPF process counters by differencing against state-managed baselines.
Baselines are keyed by a stable process identity when available and fall back conservatively when not.
For each bin, total activity mass and per-workload activity mass are accumulated according to the domain-specific proxy.
The window-level dynamic energy budget is then allocated across bins proportionally to their activity mass, and each bin’s energy share is further allocated across workloads proportionally to their per-bin activity.
Exact conservation in integer millijoules is enforced using a largest-remainder finalization step, with any rounding remainder routed explicitly to \code{\_\_system\_\_}.

\paragraph{Robustness and Edge Cases.}
Missing or zero activity mass is handled conservatively.
If no activity is observed for a domain across the entire window, the full dynamic energy budget is assigned to \code{\_\_system\_\_}.
If activity is absent in an individual bin, that bin’s energy share is assigned to \code{\_\_system\_\_}.
For the \code{dram} domain, a window-scoped fallback replaces cache-miss activity with CPU cycle activity when cache-miss mass is zero but cycle mass is non-zero.
Counter resets, PID reuse, and partial process visibility are handled by baseline rebasing and by skipping unsafe deltas, which can only increase the \code{\_\_system\_\_} share and never redistribute energy among resolved workloads.

\paragraph{Implementation Consequences.}
The implementation guarantees conservative, monotonic, and temporally causal workload attribution of CPU dynamic energy.
Integer rounding is resolved locally at both allocation stages using a largest-remainder scheme, ensuring exact conservation without cross-bin interference.
Under reduced observability, attribution degrades only by increasing the \code{\_\_system\_\_} share, without redistributing energy among resolved workloads.
The explicit bin-level realization preserves the architectural contract and enables later extensions such as idle allocation and residual energy handling without modifying the dynamic attribution core.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:cpu-dyn-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.0cm} p{1.8cm} p{1.6cm} p{4.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{CPU dynamic workload energy}} \\[4pt]
\code{workload\_rapl\_energy\_mj}
  & Counter
  & mJ
  & \textit{common (see below)} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{domain}\in\{\text{pkg},\text{core},\text{uncore},\text{dram}\}$,
$\code{kind}=\text{dynamic}$,
$\code{source}=\code{rapl}$;
labels \code{namespace}, \code{pod}, and \code{container} identify the attributed workload.
\end{minipage}

\caption{Exported CPU dynamic workload energy metrics.}
\label{tab:cpu-dyn-exported-metrics}
\end{table}


\subsubsection{CPU Idle Energy Attribution}

\paragraph{Architectural Context.}
This section realizes the CPU idle attribution model defined in the corresponding architecture subsubsection, enforcing a two-pool fairness policy under discretized, window-based execution.
The implementation adheres to the Stage~4 invariants and mirrors the workload identity and lifecycle semantics used for CPU dynamic attribution.

\paragraph{Implementation Strategy.}
Per-window idle energy budgets are derived by differencing cumulative RAPL idle energy counters for each supported domain.
Attribution is performed independently per domain, using domain-specific reservation signals while reusing the window-sticky activity weights produced by CPU dynamic attribution.
All allocation decisions are local to the current window and do not depend on future information.

\paragraph{Core Mechanisms.}
For each domain, the idle budget is split into a reserved and an opportunistic pool using the domain-specific reservation fraction $\beta$.
Reservation weights are obtained from container-level CPU requests for \code{pkg}, \code{core}, and \code{uncore}, and from container-level memory requests for \code{dram}.
Activity weights are taken from the window-aggregated dynamic allocation maps.

The eligible workload set is defined as the union of workloads observed in dynamic attribution, currently running containers, and the mandatory \code{\_\_system\_\_} workload.
The reserved pool is allocated proportionally among workloads with strictly positive requests.
The opportunistic pool is allocated proportionally among workloads without requests and the \code{\_\_system\_\_} workload, using activity weights.
Per-workload allocations from both pools are combined and accumulated into monotonic workload-level idle energy counters.

\paragraph{Robustness and Edge Cases.}
Missing or negative idle deltas result in zero budget for the affected window.
If no valid requests exist, the reserved pool collapses to zero without redistributing energy.
If no activity weights are available, the opportunistic pool collapses and the full idle budget is attributed to \code{\_\_system\_\_}.
All rounding discrepancies are resolved deterministically, with any remainder routed to \code{\_\_system\_\_}, ensuring exact per-window conservation.
Short-lived or metadata-late workloads may receive idle energy only via the opportunistic pool.
Inactive workload series are garbage-collected using the same TTL-based mechanism as dynamic CPU attribution, without retroactive reinterpretation.

\paragraph{Implementation Consequences.}
The implementation guarantees strict conservation, non-negativity, and completeness for CPU idle energy attribution across all supported RAPL domains.
Idle attribution degrades conservatively under partial observability and remains interpretable as a fairness policy rather than a causal signal.
The resulting workload-level idle energy counters are directly comparable to dynamic CPU energy outputs and suitable for downstream aggregation and analysis.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:cpu-idle-attrib-exported-metrics}.

\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.0cm} p{1.8cm} p{1.6cm} p{4.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Workload-attributed CPU idle energy}} \\[4pt]
\code{workload\_rapl\_energy\_mj}
  & Counter
  & mJ
  & \textit{common (see below)} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{domain}\in\{\text{pkg},\text{core},\text{uncore},\text{dram}\}$,
$\code{kind}=\text{idle}$,
$\code{source}=\code{rapl}$;
labels \code{namespace}, \code{pod}, and \code{container} identify the attributed workload.
\end{minipage}

\caption{Exported CPU idle workload energy metrics.}
\label{tab:cpu-idle-attrib-exported-metrics}
\end{table}


\subsubsection{GPU Dynamic Energy Attribution}

\paragraph{Architectural Context.}
This subsection realises the Stage 4 GPU dynamic attribution rule by allocating corrected-series energy over process-sample intervals and aggregating by resolved workload identity, while routing unsupported fractions to \code{\_\_system\_\_}.

\paragraph{Implementation Strategy.}
For each GPU UUID, the implementation combines (i) a corrected energy series that can be integrated over arbitrary sub-intervals, (ii) a dynamic window budget, and (iii) a timestamped stream of per-process \code{ComputeUtil} snapshots.
Attribution is performed by iterating sub-intervals induced by the snapshot times and applying a hold-last policy for utilization between snapshots.

\paragraph{Core Mechanisms.}
The corrected energy series is integrated over sub-intervals using zero-order hold on the fixed grid to obtain $\Delta E_{\mathrm{gpu}}(I,\mathrm{uuid})$.
A per-window scale factor $f(W,\mathrm{uuid})$ is computed from the ratio of the dynamic window budget to total corrected window energy and is clipped to $[0,1]$.
For each sub-interval, utilization mass is computed from the held PID map.
If mass is zero, the entire sub-interval dynamic energy is accumulated to \code{\_\_system\_\_}.
Otherwise, PID shares are computed proportionally and resolved to workloads using the existing resolver chain; unresolved shares are accumulated to \code{\_\_system\_\_}.
Any floating-point remainder between interval budget and assigned shares is routed to \code{\_\_system\_\_} to preserve per-interval conservation.

\paragraph{Robustness and Edge Cases.}
Snapshot ingestion clamps \code{ComputeUtil} into $[0,100]$ and treats invalid values as zero.
If corrected-series integration fails for a sub-interval, no energy is allocated for that sub-interval, which preserves non-negativity and avoids inventing energy.
If the corrected window energy is non-positive, or the corrected series is unavailable, the dynamic budget is conservatively routed to \code{\_\_system\_\_}.
Resolver failure for a PID share does not affect other shares and degrades monotonically by redirecting only the unresolved share to \code{\_\_system\_\_}.

\paragraph{Implementation Consequences.}
Dynamic attribution preserves conservation against the constructed dynamic budget per GPU UUID and window, up to numerical roundoff which is absorbed by \code{\_\_system\_\_}.
Temporal allocation follows the observed snapshot cadence, preventing the coarse artifacts of reducing utilization to a single window statistic, while keeping the exported outputs window-scoped.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:gpu-dyn-attrib-exported-metrics}.
\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.0cm} p{1.8cm} p{1.6cm} p{4.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Workload-attributed GPU dynamic energy}} \\[4pt]
\code{workload\_gpu\_energy\_mj}
  & Counter
  & mJ
  & \textit{common (see below)} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}=\text{dynamic}$;
labels \code{gpu\_uuid}, \code{namespace}, \code{pod}, and \code{container} identify the attributed workload.
\end{minipage}

\caption{Exported metrics for GPU dynamic workload attribution.}
\label{tab:gpu-dyn-attrib-exported-metrics}
\end{table}


\subsubsection{GPU Idle Energy Attribution}

\paragraph{Architectural Context.}
This subsection realises the Stage 4 rule that GPU idle energy is not distributed to workloads and is instead emitted exclusively as \code{\_\_system\_\_}.

\paragraph{Implementation Strategy.}
The implementation mirrors the corrected idle GPU energy counter into the workload GPU energy metric for the \code{\_\_system\_\_} workload key, per GPU UUID.

\paragraph{Core Mechanisms.}
For each GPU UUID, the absolute corrected idle energy counter value is loaded and written into the corresponding \code{workload\_gpu\_energy\_mj} series with \code{kind="idle"} and workload labels set to \code{\_\_system\_\_}.
The write is monotonic-guarded to prevent counter regression if upstream inputs transiently decrease.

\paragraph{Robustness and Edge Cases.}
Invalid or negative idle counter values are clamped to zero.
If the idle counter is missing for a device, no idle workload point is emitted for that device in the current cycle.
Monotonic guarding ensures that transient upstream regressions do not violate the counter contract at the exporter boundary.

\paragraph{Implementation Consequences.}
Idle workload energy is complete by construction and exactly consistent with corrected idle energy, while avoiding speculative distribution.
Together with dynamic attribution, the workload GPU energy metric admits a complete accounting view in which idle energy is explicitly represented as \code{\_\_system\_\_}.

\paragraph{Exported Metrics.}
The exported metrics are summarized in Table~\ref{tab:gpu-idle-attrib-exported-metrics}.
\begin{table}[H]
\centering
\small
\begin{tabular}{p{5.0cm} p{1.8cm} p{1.6cm} p{4.2cm}}
\toprule
\textbf{Metric name} & \textbf{Type} & \textbf{Unit} & \textbf{Labels} \\
\midrule

\multicolumn{4}{l}{\textit{Workload-attributed GPU idle energy}} \\[4pt]
\code{workload\_gpu\_energy\_mj}
  & Counter
  & mJ
  & \textit{common (see below)} \\

\bottomrule
\end{tabular}

\smallskip
\begin{minipage}{12.6cm}
\footnotesize\emph{Label domain:}
$\code{kind}=\text{idle}$;
labels \code{gpu\_uuid}, \code{namespace}, \code{pod}, and \code{container} identify the attributed workload.
\end{minipage}

\caption{Exported metrics for GPU idle workload attribution.}
\label{tab:gpu-idle-attrib-exported-metrics}
\end{table}

\subsection{Prometheus Exporter Implementation}

\paragraph{Architectural Context.}
This implementation realizes the passive exposition model defined above by translating committed analysis points into a Prometheus-compatible pull interface.

\paragraph{Implementation Strategy.}
Analysis points are pushed into the exporter as immutable values and retained only as the most recent sample per metric series.
Metric families are created lazily on first observation, with a fixed label schema determined at discovery time.

\paragraph{Core Mechanisms.}
Each analysis metric identifier is mapped to a sanitized Prometheus metric name with a mandatory "\code{tycho\_}" prefix.
Label keys are fixed on first sight and reused for all subsequent emissions.
At scrape time, the exporter exposes the latest committed value for each active series without additional buffering or recomputation.

\paragraph{Robustness and Edge Cases.}
Schema instability is handled conservatively by ignoring newly appearing labels after first observation.
Exporter startup is decoupled from analysis initialization, ensuring availability even while upstream components perform long-running calibration or setup.
Multiple sinks may coexist, allowing Prometheus export and auxiliary sinks such as logging to operate concurrently.

\paragraph{Implementation Consequences.}
The exporter preserves analysis correctness by construction, as it neither modifies nor reinterprets metric values.
All temporal guarantees are inherited directly from the analysis engine, while exposition remains strictly observational and replaceable.
By remaining strictly passive and state-free with respect to attribution, the exporter preserves the analysis invariants of determinism, non-retrospective execution, and window-scoped finality established throughout the implementation.

\section{Summary}

This chapter described how Tycho’s architectural abstractions are realized as a
deterministic, execution-time system under discretization, partial observability,
and asynchronous measurement.
The implementation enforces correctness properties through execution structure
rather than adaptive control, ensuring that attribution semantics are explicit,
auditable, and reproducible.

At runtime, Tycho is organized as a set of long-lived subsystems with strictly
separated responsibilities.
Temporal coordination, observation, identity acquisition, calibration, analysis,
and export are decoupled by construction, and no subsystem compensates implicitly
for the behavior of another.
A global monotonic timebase, explicit analysis windows, and single-pass execution
ensure that each analysis cycle yields a final, window-scoped interpretation of
the available evidence.

Metric construction and attribution are realized as a staged pipeline.
Early stages construct component-level energy and utilization signals while
preserving monotonicity and conservation.
Subsequent stages decompose these signals into idle and dynamic components and
allocate dynamic energy to workloads using execution-derived proxies.
Workload resolution is conservative and explicitly bounded, with unresolved
activity degrading monotonically into the distinguished \code{\_\_system\_\_} class
rather than being redistributed.

Across all stages, the implementation prioritizes conservative behavior under
uncertainty.
Missing or delayed observations do not trigger backfilling or reinterpretation of
prior windows.
Instead, degradation is explicit, localized, and bounded, preserving the
interpretability of all emitted metrics.
Cumulative energy counters remain monotonic, per-window conservation is enforced
exactly, and all stateful behavior is confined to explicit, metric-owned memory.

Together, these implementation choices realize the architectural model faithfully
while remaining robust under real execution conditions.
They establish a stable and well-defined foundation for the evaluation of
measurement accuracy, attribution behavior, and system overhead in the following
chapters.